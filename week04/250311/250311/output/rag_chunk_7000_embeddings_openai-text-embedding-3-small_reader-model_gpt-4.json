[{"question": "What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n", "true_answer": "x86_64-unknown-linux-musl", "source_doc": "huggingface/tokenizers/blob/main/bindings/node/npm/linux-x64-musl/README.md", "generated_answer": "The `tokenizers-linux-x64-musl` binary is designed for the **x86_64-unknown-linux-musl** architecture. [\ubb38\uc11c 0]", "retrieved_docs": ["`tokenizers-linux-x64-musl`\n\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`", "`tokenizers-linux-arm64-musl`\n\nThis is the **aarch64-unknown-linux-musl** binary for `tokenizers`", "`tokenizers-linux-arm64-gnu`\n\nThis is the **aarch64-unknown-linux-gnu** binary for `tokenizers`"], "test_settings": "chunk_7000_embeddings_openai-text-embedding-3-small_reader-model_gpt-4", "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response accurately identifies the architecture as x86_64-unknown-linux-musl, which is correct. The additional \"[\ubb38\uc11c 0]\" is extraneous but does not detract from the correctness of the answer."}, {"question": "What is the purpose of the BLIP-Diffusion model?\n", "true_answer": "The BLIP-Diffusion model is designed for controllable text-to-image generation and editing.", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/pipelines/blip_diffusion.md", "generated_answer": "The purpose of the BLIP-Diffusion model is to enable zero-shot subject-driven generation and control-guided zero-shot generation for text-to-image tasks. It supports multimodal control by consuming inputs of subject images and text prompts, and is designed to overcome limitations of existing models, such as lengthy fine-tuning and difficulties in preserving subject fidelity. BLIP-Diffusion introduces a multimodal encoder pre-trained to provide subject representation, allowing for efficient fine-tuning and flexible integration with existing techniques for novel subject-driven generation and editing applications [0].", "retrieved_docs": ["!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# BLIP-Diffusion\n\nBLIP-Diffusion was proposed in [BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing](https://arxiv.org/abs/2305.14720). It enables zero-shot subject-driven generation and control-guided zero-shot generation. \n\n\nThe abstract from the paper is:\n\n*Subject-driven text-to-image generation models create novel renditions of an input subject based on text prompts. Existing models suffer from lengthy fine-tuning and difficulties preserving the subject fidelity. To overcome these limitations, we introduce BLIP-Diffusion, a new subject-driven image generation model that supports multimodal control which consumes inputs of subject images and text prompts. Unlike other subject-driven generation models, BLIP-Diffusion introduces a new multimodal encoder which is pre-trained to provide subject representation. We first pre-train the multimodal encoder following BLIP-2 to produce visual representation aligned with the text. Then we design a subject representation learning task which enables a diffusion model to leverage such visual representation and generates new subject renditions. Compared with previous methods such as DreamBooth, our model enables zero-shot subject-driven generation, and efficient fine-tuning for customized subject with up to 20x speedup. We also demonstrate that BLIP-Diffusion can be flexibly combined with existing techniques such as ControlNet and prompt-to-prompt to enable novel subject-driven generation and editing applications. Project page at [this https URL](https://dxli94.github.io/BLIP-Diffusion-website/).*\n\nThe original codebase can be found at [salesforce/LAVIS](https://github.com/salesforce/LAVIS/tree/main/projects/blip-diffusion). You can find the official BLIP-Diffusion checkpoints under the [hf.co/SalesForce](https://hf.co/SalesForce) organization.\n\n`BlipDiffusionPipeline` and `BlipDiffusionControlNetPipeline` were contributed by [`ayushtues`](https://github.com/ayushtues/).\n\n<Tip>\n\nMake sure to check out the Schedulers [guide](../../using-diffusers/schedulers) to learn how to explore the tradeoff between scheduler speed and quality, and see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines) section to learn how to efficiently load the same components into multiple pipelines.\n\n</Tip>\n\n\n## BlipDiffusionPipeline\n[[autodoc]] BlipDiffusionPipeline\n    - all\n    - __call__\n\n## BlipDiffusionControlNetPipeline\n[[autodoc]] BlipDiffusionControlNetPipeline\n    - all\n    - __call__", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Philosophy\n\n\ud83e\udde8 Diffusers provides **state-of-the-art** pretrained diffusion models across multiple modalities.\nIts purpose is to serve as a **modular toolbox** for both inference and training.\n\nWe aim at building a library that stands the test of time and therefore take API design very seriously.\n\nIn a nutshell, Diffusers is built to be a natural extension of PyTorch. Therefore, most of our design choices are based on [PyTorch's Design Principles](https://pytorch.org/docs/stable/community/design.html#pytorch-design-philosophy). Let's go over the most important ones:\n\n## Usability over Performance\n\n- While Diffusers has many built-in performance-enhancing features (see [Memory and Speed](https://huggingface.co/docs/diffusers/optimization/fp16)), models are always loaded with the highest precision and lowest optimization. Therefore, by default diffusion pipelines are always instantiated on CPU with float32 precision if not otherwise defined by the user. This ensures usability across different platforms and accelerators and means that no complex installations are required to run the library.\n- Diffusers aims to be a **light-weight** package and therefore has very few required dependencies, but many soft dependencies that can improve performance (such as `accelerate`, `safetensors`, `onnx`, etc...). We strive to keep the library as lightweight as possible so that it can be added without much concern as a dependency on other packages.\n- Diffusers prefers simple, self-explainable code over condensed, magic code. This means that short-hand code syntaxes such as lambda functions, and advanced PyTorch operators are often not desired.\n\n## Simple over easy\n\nAs PyTorch states, **explicit is better than implicit** and **simple is better than complex**. This design philosophy is reflected in multiple parts of the library:\n- We follow PyTorch's API with methods like [`DiffusionPipeline.to`](https://huggingface.co/docs/diffusers/main/en/api/diffusion_pipeline#diffusers.DiffusionPipeline.to) to let the user handle device management.\n- Raising concise error messages is preferred to silently correct erroneous input. Diffusers aims at teaching the user, rather than making the library as easy to use as possible.\n- Complex model vs. scheduler logic is exposed instead of magically handled inside. Schedulers/Samplers are separated from diffusion models with minimal dependencies on each other. This forces the user to write the unrolled denoising loop. However, the separation allows for easier debugging and gives the user more control over adapting the denoising process or switching out diffusion models or schedulers.\n- Separately trained components of the diffusion pipeline, *e.g.* the text encoder, the unet, and the variational autoencoder, each have their own model class. This forces the user to handle the interaction between the different model components, and the serialization format separates the model components into different files. However, this allows for easier debugging and customization. DreamBooth or Textual Inversion training\nis very simple thanks to Diffusers' ability to separate single components of the diffusion pipeline.\n\n## Tweakable, contributor-friendly over abstraction\n\nFor large parts of the library, Diffusers adopts an important design principle of the [Transformers library](https://github.com/huggingface/transformers), which is to prefer copy-pasted code over hasty abstractions. This design principle is very opinionated and stands in stark contrast to popular design principles such as [Don't repeat yourself (DRY)](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself).\nIn short, just like Transformers does for modeling files, Diffusers prefers to keep an extremely low level of abstraction and very self-contained code for pipelines and schedulers.\nFunctions, long code blocks, and even classes can be copied across multiple files which at first can look like a bad, sloppy design choice that makes the library unmaintainable.\n**However**, this design has proven to be extremely successful for Transformers and makes a lot of sense for community-driven, open-source machine learning libraries because:\n- Machine Learning is an extremely fast-moving field in which paradigms, model architectures, and algorithms are changing rapidly, which therefore makes it very difficult to define long-lasting code abstractions.\n- Machine Learning practition...", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Philosophy\n\n\ud83e\udde8 Diffusers provides **state-of-the-art** pretrained diffusion models across multiple modalities.\nIts purpose is to serve as a **modular toolbox** for both inference and training.\n\nWe aim at building a library that stands the test of time and therefore take API design very seriously.\n\nIn a nutshell, Diffusers is built to be a natural extension of PyTorch. Therefore, most of our design choices are based on [PyTorch's Design Principles](https://pytorch.org/docs/stable/community/design.html#pytorch-design-philosophy). Let's go over the most important ones:\n\n## Usability over Performance\n\n- While Diffusers has many built-in performance-enhancing features (see [Memory and Speed](https://huggingface.co/docs/diffusers/optimization/fp16)), models are always loaded with the highest precision and lowest optimization. Therefore, by default diffusion pipelines are always instantiated on CPU with float32 precision if not otherwise defined by the user. This ensures usability across different platforms and accelerators and means that no complex installations are required to run the library.\n- Diffusers aims to be a **light-weight** package and therefore has very few required dependencies, but many soft dependencies that can improve performance (such as `accelerate`, `safetensors`, `onnx`, etc...). We strive to keep the library as lightweight as possible so that it can be added without much concern as a dependency on other packages.\n- Diffusers prefers simple, self-explainable code over condensed, magic code. This means that short-hand code syntaxes such as lambda functions, and advanced PyTorch operators are often not desired.\n\n## Simple over easy\n\nAs PyTorch states, **explicit is better than implicit** and **simple is better than complex**. This design philosophy is reflected in multiple parts of the library:\n- We follow PyTorch's API with methods like [`DiffusionPipeline.to`](https://huggingface.co/docs/diffusers/main/en/api/diffusion_pipeline#diffusers.DiffusionPipeline.to) to let the user handle device management.\n- Raising concise error messages is preferred to silently correct erroneous input. Diffusers aims at teaching the user, rather than making the library as easy to use as possible.\n- Complex model vs. scheduler logic is exposed instead of magically handled inside. Schedulers/Samplers are separated from diffusion models with minimal dependencies on each other. This forces the user to write the unrolled denoising loop. However, the separation allows for easier debugging and gives the user more control over adapting the denoising process or switching out diffusion models or schedulers.\n- Separately trained components of the diffusion pipeline, *e.g.* the text encoder, the UNet, and the variational autoencoder, each has their own model class. This forces the user to handle the interaction between the different model components, and the serialization format separates the model components into different files. However, this allows for easier debugging and customization. DreamBooth or Textual Inversion training\nis very simple thanks to Diffusers' ability to separate single components of the diffusion pipeline.\n\n## Tweakable, contributor-friendly over abstraction\n\nFor large parts of the library, Diffusers adopts an important design principle of the [Transformers library](https://github.com/huggingface/transformers), which is to prefer copy-pasted code over hasty abstractions. This design principle is very opinionated and stands in stark contrast to popular design principles such as [Don't repeat yourself (DRY)](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself).\nIn short, just like Transformers does for modeling files, Diffusers prefers to keep an extremely low level of abstraction and very self-contained code for pipelines and schedulers.\nFunctions, long code blocks, and even classes can be copied across multiple files which at first can look like a bad, sloppy design choice that makes the library unmaintainable.\n**However**, this design has proven to be extremely successful for Transformers and makes a lot of sense for community-driven, open-source machine learning libraries because:\n- Machine Learning is an extremely fast-moving field in which paradigms, model architectures, and algorithms are changing rapidly, which therefore makes it very difficult to define long-lasting code abstractions.\n- Machine Learning practitione..."], "test_settings": "chunk_7000_embeddings_openai-text-embedding-3-small_reader-model_gpt-4", "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response provides a detailed explanation of the purpose of the BLIP-Diffusion model, highlighting its capabilities in zero-shot subject-driven generation and control-guided zero-shot generation for text-to-image tasks. It accurately describes the model's support for multimodal control, its ability to overcome limitations of existing models, and the introduction of a multimodal encoder for efficient fine-tuning and flexible integration. While the response is more detailed than the concise 5-point answer, it remains accurate and factually correct, aligning well with the purpose of the model. Therefore, it meets the criteria for a high score, as it is mostly correct and factual."}, {"question": "How can a user claim authorship of a paper on the Hugging Face Hub?\n", "true_answer": "By clicking their name on the corresponding Paper page and clicking \"claim authorship\", then confirming the request in paper settings for admin team validation.", "source_doc": "huggingface/hub-docs/blob/main/docs/hub/paper-pages.md", "generated_answer": "A user can claim authorship of a paper on the Hugging Face Hub by clicking on their name on the corresponding Paper page and then clicking \"claim authorship.\" This action will redirect them to the paper settings, where they can confirm the request. The admin team will validate the request, and once confirmed, the Paper page will show as verified [\ubb38\uc11c 0].", "retrieved_docs": ["Paper Pages\n\nPaper pages allow people to find artifacts related to a paper such as models, datasets and apps/demos (Spaces). Paper pages also enable the community to discuss about the paper.\n\n<div class=\"flex justify-center\">\n<img class=\"block dark:hidden\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-discussions.png\"/>\n<img class=\"hidden dark:block\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-discussions-dark.png\"/>\n</div>\n\n## Linking a Paper to a model, dataset or Space\n\nIf the repository card (`README.md`) includes a link to a paper on arXiv, the Hugging Face Hub will extract the arXiv ID and include it in the repository's tags. Clicking on the arxiv tag will let you:\n\n* Visit the Paper page.\n* Filter for other models or datasets on the Hub that cite the same paper.\n\n<div class=\"flex justify-center\">\n<img class=\"block dark:hidden\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv.png\"/>\n<img class=\"hidden dark:block\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv-dark.png\"/>\n</div>\n\n## Claiming authorship to a Paper\n\nThe Hub will attempt to automatically match paper to users based on their email. \n\n<div class=\"flex justify-center\">\n<img class=\"block dark:hidden\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-authors.png\"/>\n<img class=\"hidden dark:block\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-authors-dark.png\"/>\n</div>\n\nIf your paper is not linked to your account, you can click in your name in the corresponding Paper page and click \"claim authorship\". This will automatically re-direct to your paper settings where you can confirm the request. The admin team will validate your request soon. Once confirmed, the Paper page will show as verified.\n\n<div class=\"flex justify-center\">\n<img class=\"block dark:hidden\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-settings.png\"/>\n<img class=\"hidden dark:block\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-settings-dark.png\"/>\n</div>\n\n\n## Frequently Asked Questions\n\n### Can I control which Paper pages show in my profile?\n\nYes! You can visit your Papers in [settings](https://huggingface.co/settings/papers), where you will see a list of verified papers. There, you can click the \"Show on profile\" checkbox to hide/show it in your profile. \n\n### Do you support ACL anthology?\n\nWe're starting with Arxiv as it accounts for 95% of the paper URLs Hugging Face users have linked in their repos organically. We'll check how this evolve and potentially extend to other paper hosts in the future.\n\n### Can I have a Paper page even if I have no model/dataset/Space?\n\nYes. You can go to [the main Papers page](https://huggingface.co/papers), click search and write the name of the paper or the full Arxiv id. If the paper does not exist, you will get an option to index it. You can also just visit the page `hf.co/papers/xxxx.yyyyy` replacing with the arxiv id of the paper you wish to index.", "p align=\"center\">\n  <br/>\n    <img alt=\"huggingface_hub library logo\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/huggingface_hub.svg\" width=\"376\" height=\"59\" style=\"max-width: 100%;\">\n  <br/>\n</p>\n\n<p align=\"center\">\n    <i>The official Python client for the Huggingface Hub.</i>\n</p>\n\n<p align=\"center\">\n    <a href=\"https://huggingface.co/docs/huggingface_hub/en/index\"><img alt=\"Documentation\" src=\"https://img.shields.io/website/http/huggingface.co/docs/huggingface_hub/index.svg?down_color=red&down_message=offline&up_message=online&label=doc\"></a>\n    <a href=\"https://github.com/huggingface/huggingface_hub/releases\"><img alt=\"GitHub release\" src=\"https://img.shields.io/github/release/huggingface/huggingface_hub.svg\"></a>\n    <a href=\"https://github.com/huggingface/huggingface_hub\"><img alt=\"PyPi version\" src=\"https://img.shields.io/pypi/pyversions/huggingface_hub.svg\"></a>\n    <a href=\"https://pypi.org/project/huggingface-hub\"><img alt=\"downloads\" src=\"https://static.pepy.tech/badge/huggingface_hub/month\"></a>\n    <a href=\"https://codecov.io/gh/huggingface/huggingface_hub\"><img alt=\"Code coverage\" src=\"https://codecov.io/gh/huggingface/huggingface_hub/branch/main/graph/badge.svg?token=RXP95LE2XL\"></a>\n</p>\n\n<h4 align=\"center\">\n    <p>\n        <b>English</b> |\n        <a href=\"https://github.com/huggingface/huggingface_hub/blob/main/README_de.md\">Deutsch</a> |\n        <a href=\"https://github.com/huggingface/huggingface_hub/blob/main/README_hi.md\">\u0939\u093f\u0902\u0926\u0940</a> | \n        <a href=\"https://github.com/huggingface/huggingface_hub/blob/main/README_ko.md\">\ud55c\uad6d\uc5b4</a> |\n        <a href=\"https://github.com/huggingface/huggingface_hub/blob/main/README_cn.md\">\u4e2d\u6587\uff08\u7b80\u4f53\uff09</a> \n    <p>\n</h4>\n---\n\n**Documentation**: <a href=\"https://hf.co/docs/huggingface_hub\" target=\"_blank\">https://hf.co/docs/huggingface_hub</a>\n\n**Source Code**: <a href=\"https://github.com/huggingface/huggingface_hub\" target=\"_blank\">https://github.com/huggingface/huggingface_hub</a>\n\n---\n\n## Welcome to the huggingface_hub library\n\nThe `huggingface_hub` library allows you to interact with the [Hugging Face Hub](https://huggingface.co/), a platform democratizing open-source Machine Learning for creators and collaborators. Discover pre-trained models and datasets for your projects or play with the thousands of machine learning apps hosted on the Hub. You can also create and share your own models, datasets and demos with the community. The `huggingface_hub` library provides a simple way to do all these things with Python.\n\n## Key features\n\n- [Download files](https://huggingface.co/docs/huggingface_hub/en/guides/download) from the Hub.\n- [Upload files](https://huggingface.co/docs/huggingface_hub/en/guides/upload) to the Hub.\n- [Manage your repositories](https://huggingface.co/docs/huggingface_hub/en/guides/repository).\n- [Run Inference](https://huggingface.co/docs/huggingface_hub/en/guides/inference) on deployed models.\n- [Search](https://huggingface.co/docs/huggingface_hub/en/guides/search) for models, datasets and Spaces.\n- [Share Model Cards](https://huggingface.co/docs/huggingface_hub/en/guides/model-cards) to document your models.\n- [Engage with the community](https://huggingface.co/docs/huggingface_hub/en/guides/community) through PRs and comments.\n\n## Installation\n\nInstall the `huggingface_hub` package with [pip](https://pypi.org/project/huggingface-hub/):\n\n```bash\npip install huggingface_hub\n```\n\nIf you prefer, you can also install it with [conda](https://huggingface.co/docs/huggingface_hub/en/installation#install-with-conda).\n\nIn order to keep the package minimal by default, `huggingface_hub` comes with optional dependencies useful for some use cases. For example, if you want have a complete experience for Inference, run:\n\n```bash\npip install huggingface_hub[inference]\n```\n\nTo learn more installation and optional dependencies, check out the [installation guide](https://huggingface.co/docs/huggingface_hub/en/installation).\n\n## Quick start\n\n### Download files\n\nDownload a single file\n\n```py\nfrom huggingface_hub import hf_hub_download\n\nhf_hub_download(repo_id=\"tiiuae/falcon-7b-instruct\", filename=\"config.json\")\n```\n\nOr an entire repository\n\n```py\nfrom huggingface_hub import snapshot_download\n\nsnapshot_download(\"stabilityai/stable-diffusion-2-1\")\n```\n\nFiles will be downloaded in a local cache folder. More details in [this guide](https://huggingface.co/docs/huggingface_hub/en/guides/manage-cache).\n\n### Login\n\nThe Hugging Face Hub uses tokens to authenticate applications (see [docs](https://huggingface.co/docs/hub/security-tokens)). To login your machine, run the following CLI:\n\n```bash\nhuggingface-cli login\n# or using an environment variable\nhuggingface-cli login --token $HUGGINGFACE_TOKEN\n```\n\n### Create a repository\n\n```py\nfrom huggingface_hub import create_repo\n\ncreate_repo(repo_id=\"super-cool-model\")\n```\n\n### Upload files\n\nUpload a single file\n\n```py\nfrom huggingface_hub import upload_file\n\nupload_file(\n    path_or_fileobj=\"/...", "Signing commits with GPG\n\n`git` has an authentication layer to control who can push commits to a repo, but it does not authenticate the actual commit authors.\n\nIn other words, you can commit changes as `Elon Musk <elon@tesla.com>`, push them to your preferred `git` host (for instance github.com), and your commit will link to Elon's GitHub profile. (Try it! But don't blame us if Elon gets mad at you for impersonating him.)\n\nThe reasons we implemented GPG signing were:\n- To provide finer-grained security, especially as more and more Enterprise users rely on the Hub.\n- To provide ML benchmarks backed by a cryptographically-secure source.\n\nSee Ale Segala's [How (and why) to sign `git` commits](https://withblue.ink/2020/05/17/how-and-why-to-sign-git-commits.html) for more context.\n\nYou can prove a commit was authored by you with GNU Privacy Guard (GPG) and a key server. GPG is a cryptographic tool used to verify the authenticity of a message's origin. We'll explain how to set this up on Hugging Face below.\n\nThe Pro Git book is, as usual, a good resource about commit signing: [Pro Git: Signing your work](https://git-scm.com/book/en/v2/Git-Tools-Signing-Your-Work).\n\n## Setting up signed commits verification\n\nYou will need to install [GPG](https://gnupg.org/) on your system in order to execute the following commands.\n> It's included by default in most Linux distributions.\n> On Windows, it is included in Git Bash (which comes with `git` for Windows).\n\nYou can sign your commits locally using [GPG](https://gnupg.org/).\nThen configure your profile to mark these commits as **verified** on the Hub,\nso other people can be confident that they come from a trusted source.\n\nFor a more in-depth explanation of how git and GPG interact, please visit the the [git documentation on the subject](https://git-scm.com/book/en/v2/Git-Tools-Signing-Your-Work)\n\nCommits can have the following signing statuses:\n\n| Status            | Explanation                                                  |\n| ----------------- | ------------------------------------------------------------ |\n| Verified          | The commit is signed and the signature is verified           |\n| Unverified        | The commit is signed but the signature could not be verified |\n| No signing status | The commit is not signed                                     |\n\nFor a commit to be marked as **verified**, you need to upload the public key used to sign it on your Hugging Face account.\n\nUse the `gpg --list-secret-keys` command to list the GPG keys for which you have both a public and private key.\nA private key is required for signing commits or tags.\n\nIf you don't have a GPG key pair or you don't want to use the existing keys to sign your commits, go to **Generating a new GPG key**.\n\nOtherwise, go straight to  [Adding a GPG key to your account](#adding-a-gpg-key-to-your-account).\n\n## Generating a new GPG key\n\nTo generate a GPG key, run the following:\n\n```bash\ngpg --gen-key\n```\n\nGPG will then guide you through the process of creating a GPG key pair.\n\nMake sure you specify an email address for this key, and that the email address matches the one you specified in your Hugging Face [account](https://huggingface.co/settings/account).\n\n## Adding a GPG key to your account\n\n1. First, select or generate a GPG key on your computer. Make sure the email address of the key matches the one in your Hugging Face [account](https://huggingface.co/settings/account) and that the email of your account is verified.\n\n2. Export the public part of the selected key:\n\n```bash\ngpg --armor --export <YOUR KEY ID>\n```\n\n3. Then visit your profile [settings page](https://huggingface.co/settings/keys) and click on **Add GPG Key**.\n\nCopy & paste the output of the `gpg --export` command in the text area and click on **Add Key**.\n\n4. Congratulations! \ud83c\udf89  You've just added a GPG key to your account!\n\n## Configure git to sign your commits with GPG\n\nThe last step is to configure git to sign your commits:\n\n```bash\ngit config user.signingkey <Your GPG Key ID>\ngit config user.email <Your email on hf.co>\n```\n\nThen add the `-S` flag to your `git commit` commands to sign your commits!\n\n```bash\ngit commit -S -m \"My first signed commit\"\n```\n\nOnce pushed on the Hub, you should see the commit with a \"Verified\" badge.\n\n<Tip>\nTo sign all commits by default in any local repository on your computer, you can run <code>git config --global commit.gpgsign true</code>.\n</Tip>"], "test_settings": "chunk_7000_embeddings_openai-text-embedding-3-small_reader-model_gpt-4", "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response is mostly correct and factual, closely matching the criteria for a high score, with minor clarity issues due to the inclusion of \"[\ubb38\uc11c 0]\"."}, {"question": "What is the purpose of the /healthcheck endpoint in the Datasets server API?\n", "true_answer": "Ensure the app is running", "source_doc": "huggingface/datasets-server/blob/main/services/api/README.md", "generated_answer": "The purpose of the /healthcheck endpoint in the Datasets server API is to ensure that the application is running.", "retrieved_docs": ["Datasets server API\n\n> API on \ud83e\udd17 datasets\n\n## Configuration\n\nThe service can be configured using environment variables. They are grouped by scope.\n\n### API service\n\nSee [../../libs/libapi/README.md](../../libs/libapi/README.md) for more information about the API configuration.\n\n### Common\n\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration.\n\n## Endpoints\n\nSee https://huggingface.co/docs/datasets-server\n\n- /healthcheck: Ensure the app is running\n- /metrics: Return a list of metrics in the Prometheus format\n- /webhook: Add, update or remove a dataset\n- /is-valid: Tell if a dataset is [valid](https://huggingface.co/docs/datasets-server/valid)\n- /splits: List the [splits](https://huggingface.co/docs/datasets-server/splits) names for a dataset\n- /first-rows: Extract the [first rows](https://huggingface.co/docs/datasets-server/first_rows) for a dataset split\n- /parquet: List the [parquet files](https://huggingface.co/docs/datasets-server/parquet) auto-converted for a dataset", "Check dataset validity\n\nBefore you download a dataset from the Hub, it is helpful to know if a specific dataset you're interested in is available. Datasets Server provides the `/is-valid` endpoint to check if a specific dataset works without any errors.\n\nThe API endpoint will return an error for datasets that cannot be loaded with the [\ud83e\udd17 Datasets](https://github.com/huggingface/datasets) library, for example, because the data hasn't been uploaded or the format is not supported.\n\n<Tip warning={true}>\n  The largest datasets are partially supported by Datasets Server. If they are{\" \"}\n  <a href=\"https://huggingface.co/docs/datasets/stream\">streamable</a>, Datasets\n  Server can extract the first 100 rows without downloading the whole dataset.\n  This is especially useful for previewing large datasets where downloading the\n  whole dataset may take hours! See the <code>preview</code> field in the\n  response of <code>/is-valid</code> to check if a dataset is partially\n  supported.\n</Tip>\n\nThis guide shows you how to check dataset validity programmatically, but free to try it out with [Postman](https://www.postman.com/huggingface/workspace/hugging-face-apis/request/23242779-17b761d0-b2b8-4638-a4f7-73be9049c324), [RapidAPI](https://rapidapi.com/hugging-face-hugging-face-default/api/hugging-face-datasets-api), or [ReDoc](https://redocly.github.io/redoc/?url=https://datasets-server.huggingface.co/openapi.json#operation/isValidDataset).\n\n## Check if a dataset is valid\n\n`/is-valid` checks whether a specific dataset loads without any error. This endpoint's query parameter requires you to specify the name of the dataset:\n\n<inferencesnippet>\n<python>\n```python\nimport requests\nheaders = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\nAPI_URL = \"https://datasets-server.huggingface.co/is-valid?dataset=rotten_tomatoes\"\ndef query():\n    response = requests.get(API_URL, headers=headers)\n    return response.json()\ndata = query()\n```\n</python>\n<js>\n```js\nimport fetch from \"node-fetch\";\nasync function query(data) {\n    const response = await fetch(\n        \"https://datasets-server.huggingface.co/is-valid?dataset=rotten_tomatoes\",\n        {\n            headers: { Authorization: `Bearer ${API_TOKEN}` },\n            method: \"GET\"\n        }\n    );\n    const result = await response.json();\n    return result;\n}\nquery().then((response) => {\n    console.log(JSON.stringify(response));\n});\n```\n</js>\n<curl>\n```curl\ncurl https://datasets-server.huggingface.co/is-valid?dataset=rotten_tomatoes \\\n        -X GET \\\n        -H \"Authorization: Bearer ${API_TOKEN}\"\n```\n</curl>\n</inferencesnippet>\n\nThe response looks like this if a dataset is valid:\n\n```json\n{\n  \"viewer\": true,\n  \"preview\": true\n}\n```\n\nIf only the first rows of a dataset are available, then the response looks like:\n\n```json\n{\n  \"viewer\": false,\n  \"preview\": true\n}\n```\n\nFinally, if the dataset is not valid at all, then the response is:\n\n```json\n{\n  \"viewer\": false,\n  \"preview\": false\n}\n```\n\nSome cases where a dataset is not valid are:\n\n- the dataset viewer is disabled\n- the dataset is gated but the access is not granted: no token is passed or the passed token is not authorized\n- the dataset is private\n- the dataset contains no data or the data format is not supported\n\n<Tip>\n  Remember if a dataset is <a href=\"./quick_start#gated-datasets\">gated</a>,\n  you'll need to provide your user token to submit a successful query!\n</Tip>", "Datasets server API - rows endpoint\n\n> /rows endpoint\n\n## Configuration\n\nThe service can be configured using environment variables. They are grouped by scope.\n\n### API service\n\nSee [../../libs/libapi/README.md](../../libs/libapi/README.md) for more information about the API configuration.\n\n### Common\n\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration.\n\n## Endpoints\n\nSee https://huggingface.co/docs/datasets-server\n\n- /healthcheck: ensure the app is running\n- /metrics: return a list of metrics in the Prometheus format\n- /rows: get a slice of rows of a dataset split"], "test_settings": "chunk_7000_embeddings_openai-text-embedding-3-small_reader-model_gpt-4", "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response accurately captures the primary purpose of the /healthcheck endpoint, which is to verify that the application is running. It aligns well with the criteria for a 5-point answer, as it is correct, precise, and factual. The response is slightly more detailed than the criteria answer but remains fully accurate and relevant."}, {"question": "What is the default context window size for Local Attention in the LongT5 model?\n", "true_answer": "127 tokens", "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/longt5.md", "generated_answer": "The default context window size for Local Attention in the LongT5 model is 127 tokens to the left and right of a given token [\ubb38\uc11c 0].", "retrieved_docs": ["!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# LongT5\n\n## Overview\n\nThe LongT5 model was proposed in [LongT5: Efficient Text-To-Text Transformer for Long Sequences](https://arxiv.org/abs/2112.07916)\nby Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung and Yinfei Yang. It's an\nencoder-decoder transformer pre-trained in a text-to-text denoising generative setting. LongT5 model is an extension of\nT5 model, and it enables using one of the two different efficient attention mechanisms - (1) Local attention, or (2)\nTransient-Global attention.\n\n\nThe abstract from the paper is the following:\n\n*Recent work has shown that either (1) increasing the input length or (2) increasing model size can improve the\nperformance of Transformer-based neural models. In this paper, we present a new model, called LongT5, with which we\nexplore the effects of scaling both the input length and model size at the same time. Specifically, we integrated\nattention ideas from long-input transformers (ETC), and adopted pre-training strategies from summarization pre-training\n(PEGASUS) into the scalable T5 architecture. The result is a new attention mechanism we call {\\em Transient Global}\n(TGlobal), which mimics ETC's local/global attention mechanism, but without requiring additional side-inputs. We are\nable to achieve state-of-the-art results on several summarization tasks and outperform the original T5 models on\nquestion answering tasks.*\n\nThis model was contributed by [stancld](https://huggingface.co/stancld).\nThe original code can be found [here](https://github.com/google-research/longt5).\n\n## Usage tips\n\n- [`LongT5ForConditionalGeneration`] is an extension of [`T5ForConditionalGeneration`] exchanging the traditional\nencoder *self-attention* layer with efficient either *local* attention or *transient-global* (*tglobal*) attention.\n- Unlike the T5 model, LongT5 does not use a task prefix. Furthermore, it uses a different pre-training objective\ninspired by the pre-training of [`PegasusForConditionalGeneration`].\n- LongT5 model is designed to work efficiently and very well on long-range *sequence-to-sequence* tasks where the\ninput sequence exceeds commonly used 512 tokens. It is capable of handling input sequences of a length up to 16,384 tokens.\n- For *Local Attention*, the sparse sliding-window local attention operation allows a given token to attend only `r`\ntokens to the left and right of it (with `r=127` by default). *Local Attention* does not introduce any new parameters\nto the model. The complexity of the mechanism is linear in input sequence length `l`: `O(l*r)`.\n- *Transient Global Attention* is an extension of the *Local Attention*. It, furthermore, allows each input token to\ninteract with all other tokens in the layer. This is achieved via splitting an input sequence into blocks of a fixed\nlength `k` (with a default `k=16`). Then, a global token for such a block is obtained via summing and normalizing the embeddings of every token\nin the block. Thanks to this, the attention allows each token to attend to both nearby tokens like in Local attention, and\nalso every global token like in the case of standard global attention (*transient* represents the fact the global tokens\nare constructed dynamically within each attention operation).  As a consequence, *TGlobal* attention introduces\na few new parameters -- global relative position biases and a layer normalization for global token's embedding.\nThe complexity of this mechanism is `O(l(r + l/k))`.\n- An example showing how to evaluate a fine-tuned LongT5 model on the [pubmed dataset](https://huggingface.co/datasets/scientific_papers) is below.\n\n```python\n>>> import evaluate\n>>> from datasets import load_dataset\n>>> from transformers import AutoTokenizer, LongT5ForConditionalGeneration\n\n>>> dataset = load_dataset(\"scientific_papers\", \"pubmed\", split=\"validation\")\n>>> model = (\n...     LongT5ForConditionalGeneration.from_pretrained(\"Stancld/longt5-tglobal-large-16384-pubmed-3k_steps\")\n...     .to(\"cuda\")\n...     .half()\n... )\n>>> tokenizer = AutoTokenizer.from_pretrained(\"Stancld/longt5-tglobal-large-16384-pubmed-3k_steps\")\n\n\n>>> def generate_answers(batch):\n...     inputs_dict = tokenizer(\n...         batch[\"article\"], max_length=16384, padding=\"max_length\", truncat...", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Attention mechanisms\n\nMost transformer models use full attention in the sense that the attention matrix is square. It can be a big\ncomputational bottleneck when you have long texts. Longformer and reformer are models that try to be more efficient and\nuse a sparse version of the attention matrix to speed up training.\n\n## LSH attention\n\n[Reformer](#reformer) uses LSH attention. In the softmax(QK^t), only the biggest elements (in the softmax\ndimension) of the matrix QK^t are going to give useful contributions. So for each query q in Q, we can consider only\nthe keys k in K that are close to q. A hash function is used to determine if q and k are close. The attention mask is\nmodified to mask the current token (except at the first position), because it will give a query and a key equal (so\nvery similar to each other). Since the hash can be a bit random, several hash functions are used in practice\n(determined by a n_rounds parameter) and then are averaged together.\n\n## Local attention\n\n[Longformer](#longformer) uses local attention: often, the local context (e.g., what are the two tokens to the\nleft and right?) is enough to take action for a given token. Also, by stacking attention layers that have a small\nwindow, the last layer will have a receptive field of more than just the tokens in the window, allowing them to build a\nrepresentation of the whole sentence.\n\nSome preselected input tokens are also given global attention: for those few tokens, the attention matrix can access\nall tokens and this process is symmetric: all other tokens have access to those specific tokens (on top of the ones in\ntheir local window). This is shown in Figure 2d of the paper, see below for a sample attention mask:\n\n<div class=\"flex justify-center\">\n    <img scale=\"50 %\" align=\"center\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/local_attention_mask.png\"/>\n</div>\n\nUsing those attention matrices with less parameters then allows the model to have inputs having a bigger sequence\nlength.\n\n## Other tricks\n\n### Axial positional encodings\n\n[Reformer](#reformer) uses axial positional encodings: in traditional transformer models, the positional encoding\nE is a matrix of size \\\\(l\\\\) by \\\\(d\\\\), \\\\(l\\\\) being the sequence length and \\\\(d\\\\) the dimension of the\nhidden state. If you have very long texts, this matrix can be huge and take way too much space on the GPU. To alleviate\nthat, axial positional encodings consist of factorizing that big matrix E in two smaller matrices E1 and E2, with\ndimensions \\\\(l_{1} \\times d_{1}\\\\) and \\\\(l_{2} \\times d_{2}\\\\), such that \\\\(l_{1} \\times l_{2} = l\\\\) and\n\\\\(d_{1} + d_{2} = d\\\\) (with the product for the lengths, this ends up being way smaller). The embedding for time\nstep \\\\(j\\\\) in E is obtained by concatenating the embeddings for timestep \\\\(j \\% l1\\\\) in E1 and \\\\(j // l1\\\\)\nin E2.", "Important to remember is that for each output vector \\\\(\\mathbf{z}_{i}\\\\), the whole input sequence \\\\(\\mathbf{X}\\\\) is processed. The tensor of the inner dot-product \\\\(\\mathbf{Q}\\mathbf{K}^T\\\\) has an asymptotic memory complexity of \\\\(\\mathcal{O}(n^2)\\\\) which usually represents the memory bottleneck in a transformer model. \n\nThis is also the reason why `bert-base-cased` has a `config.max_position_embedding_size` of only 512.\n\n### Local Self-Attention\n\n **Local self-attention** is the obvious solution to reducing the \\\\(\\mathcal{O}(n^2)\\\\) memory bottleneck, allowing us to model longer sequences with a reduced computational cost. \nIn local self-attention the input \\\\( \\mathbf{X} = \\mathbf{X}_{1:n} = \\mathbf{x}_{1}, \\ldots, \\mathbf{x}_{n} \\\\) \nis cut into \\\\(n_{c}\\\\) chunks: \\\\( \\mathbf{X} = \\left[\\mathbf{X}_{1:l_{c}}, \\ldots, \\mathbf{X}_{(n_{c} - 1) * l_{c} : n_{c} * l_{c}}\\right] \\\\) each \nof length `config.local_chunk_length`, *i.e.* \\\\(l_{c}\\\\), and subsequently global self-attention is applied on each chunk separately.\n\nLet's take our input sequence for \\\\(n=16, d_h=3\\\\) again for visualization:\n\n![alt text](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/reformer_benchmark/input.png)\n\nAssuming \\\\(l_{c} = 4, n_{c} = 4\\\\), chunked attention can be illustrated as follows:\n\n![alt text](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/reformer_benchmark/chunked_attention_1.png)\n\nAs can be seen, the attention operation is applied for each chunk \\\\(\\mathbf{X}_{1:4}, \\mathbf{X}_{5:8}, \\mathbf{X}_{9:12}, \\mathbf{X}_{13:16}\\\\) individually.\nThe first drawback of this architecture becomes obvious: Some input vectors have no access to their immediate context, *e.g.* \\\\(\\mathbf{x}_9\\\\) has no access to \\\\(\\mathbf{x}_{8}\\\\) and vice-versa in our example. This is problematic because these tokens are not able to learn word representations that take their immediate context into account.\n\nA simple remedy is to augment each chunk with `config.local_num_chunks_before`, *i.e.* \\\\(n_{p}\\\\), chunks and `config.local_num_chunks_after`, *i.e.* \\\\(n_{a}\\\\), so that every input vector has at least access to \\\\(n_{p}\\\\) previous input vectors and \\\\(n_{a}\\\\) following input vectors. This can also be understood as chunking with overlap whereas \\\\(n_{p}\\\\) and \\\\(n_{a}\\\\) define the amount of overlap each chunk has with all previous chunks and following chunks. We denote this extended local self-attention as follows: \n\n$$\\mathbf{Z}^{\\text{loc}} = \\left[\\mathbf{Z}_{1:l_{c}}^{\\text{loc}}, \\ldots, \\mathbf{Z}_{(n_{c} - 1) * l_{c} : n_{c} * l_{c}}^{\\text{loc}}\\right], $$ \nwith\n$$\\mathbf{Z}_{l_{c} * (i - 1) + 1 : l_{c} * i}^{\\text{loc}} = \\text{SelfAttn}(\\mathbf{X}_{l_{c} * (i - 1 - n_{p}) + 1: l_{c} * (i + n_{a})})\\left[n_{p} * l_{c}: -n_{a} * l_{c}\\right], \\forall i \\in \\{1, \\ldots, n_{c} \\}$$\n\nOkay, this formula looks quite complicated. Let's make it easier.\nIn Reformer's self-attention layers \\\\(n_{a}\\\\) is usually set to 0 and \\\\(n_{p}\\\\) is set to 1, so let's write down the formula again for \\\\(i = 1\\\\):\n\n$$\\mathbf{Z}_{1:l_{c}}^{\\text{loc}} = \\text{SelfAttn}(\\mathbf{X}_{-l_{c} + 1: l_{c}})\\left[l_{c}:\\right]$$\n\nWe notice that we have a circular relationship so that the first segment can attend the last segment as well. Let's illustrate this slightly enhanced local attention again. First, we apply self-attention within each windowed segment and keep only the central output segment.\n\n![alt text](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/reformer_benchmark/local_attention_2.png)\n\nFinally, the relevant output is concatenated to \\\\(\\mathbf{Z}^{\\text{loc}}\\\\) and looks as follows.\n\n![alt text](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/reformer_benchmark/local_attention_3.png)\n\nNote that local self-attention is implemented efficiently way so that no output is computed and subsequently \"thrown-out\" as shown here for illustration purposes by the red cross.\n\nIt's important to note here that extending the input vectors for each chunked self-attention function allows *each* single output vector \\\\( \\mathbf{z}_{i} \\\\) of this self-attention function to learn better vector representations. E.g. each of the output vectors \\\\( \\mathbf{z}_{5}^{\\text{loc}}, \\mathbf{z}_{6}^{\\text{loc}}, \\mathbf{z}_{7}^{\\text{loc}}, \\mathbf{z}_{8}^{\\text{loc}} \\\\) can take into account all of the input vectors \\\\( \\mathbf{X}_{1:8} \\\\) to learn better representations.\n\nThe gain in memory consumption is quite obvious: The \\\\( \\mathcal{O}(n^2) \\\\) memory complexity is broken down for each segment individually so that the total asymptotic memory consumption is reduced to \\\\( \\mathcal{O}(n_{c} * l_{c}^2) = \\mathcal{O}(n * l_{c}) \\\\).\n\nThis enhanced local self-attention is better than the vanilla local self-attention architecture but still has a major drawback in that every input vector can only attend to a local context of predefined size. For NLP t..."], "test_settings": "chunk_7000_embeddings_openai-text-embedding-3-small_reader-model_gpt-4", "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response provides the correct information regarding the default context window size for Local Attention in the LongT5 model, which is 127 tokens. However, it includes additional details about the tokens being to the left and right of a given token, which were not part of the required answer. While this extra information is not incorrect, it is unnecessary for the question asked. The response is accurate but slightly verbose compared to the concise nature of the ideal answer. Therefore, it does not fully meet the criteria for a perfect score."}, {"question": "What method is used to load a checkpoint for a task using `AutoPipeline`?\n", "true_answer": "from_pretrained()", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/pipelines/auto_pipeline.md", "generated_answer": "The method used to load a checkpoint for a task using `AutoPipeline` is the `from_pretrained()` method [\ubb38\uc11c 0, \ubb38\uc11c 1].", "retrieved_docs": ["!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# AutoPipeline\n\n\ud83e\udd17 Diffusers is able to complete many different tasks, and you can often reuse the same pretrained weights for multiple tasks such as text-to-image, image-to-image, and inpainting. If you're new to the library and diffusion models though, it may be difficult to know which pipeline to use for a task. For example, if you're using the [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5) checkpoint for text-to-image, you might not know that you could also use it for image-to-image and inpainting by loading the checkpoint with the [`StableDiffusionImg2ImgPipeline`] and [`StableDiffusionInpaintPipeline`] classes respectively.\n\nThe `AutoPipeline` class is designed to simplify the variety of pipelines in \ud83e\udd17 Diffusers. It is a generic, *task-first* pipeline that lets you focus on the task. The `AutoPipeline` automatically detects the correct pipeline class to use, which makes it easier to load a checkpoint for a task without knowing the specific pipeline class name.\n\n<Tip>\n\nTake a look at the [AutoPipeline](../api/pipelines/auto_pipeline) reference to see which tasks are supported. Currently, it supports text-to-image, image-to-image, and inpainting.\n\n</Tip>\n\nThis tutorial shows you how to use an `AutoPipeline` to automatically infer the pipeline class to load for a specific task, given the pretrained weights.\n\n## Choose an AutoPipeline for your task\n\nStart by picking a checkpoint. For example, if you're interested in text-to-image with the [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5) checkpoint, use [`AutoPipelineForText2Image`]:\n\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True\n).to(\"cuda\")\nprompt = \"peasant and dragon combat, wood cutting style, viking era, bevel with rune\"\n\nimage = pipeline(prompt, num_inference_steps=25).images[0]\nimage\n```\n\n<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/autopipeline-text2img.png\" alt=\"generated image of peasant fighting dragon in wood cutting style\"/>\n</div>\n\nUnder the hood, [`AutoPipelineForText2Image`]:\n\n1. automatically detects a `\"stable-diffusion\"` class from the [`model_index.json`](https://huggingface.co/runwayml/stable-diffusion-v1-5/blob/main/model_index.json) file\n2. loads the corresponding text-to-image [`StableDiffusionPipeline`] based on the `\"stable-diffusion\"` class name\n\nLikewise, for image-to-image, [`AutoPipelineForImage2Image`] detects a `\"stable-diffusion\"` checkpoint from the `model_index.json` file and it'll load the corresponding [`StableDiffusionImg2ImgPipeline`] behind the scenes. You can also pass any additional arguments specific to the pipeline class such as `strength`, which determines the amount of noise or variation added to an input image:\n\n```py\nfrom diffusers import AutoPipelineForImage2Image\nimport torch\nimport requests\nfrom PIL import Image\nfrom io import BytesIO\n\npipeline = AutoPipelineForImage2Image.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\",\n    torch_dtype=torch.float16,\n    use_safetensors=True,\n).to(\"cuda\")\nprompt = \"a portrait of a dog wearing a pearl earring\"\n\nurl = \"https://upload.wikimedia.org/wikipedia/commons/thumb/0/0f/1665_Girl_with_a_Pearl_Earring.jpg/800px-1665_Girl_with_a_Pearl_Earring.jpg\"\n\nresponse = requests.get(url)\nimage = Image.open(BytesIO(response.content)).convert(\"RGB\")\nimage.thumbnail((768, 768))\n\nimage = pipeline(prompt, image, num_inference_steps=200, strength=0.75, guidance_scale=10.5).images[0]\nimage\n```\n\n<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/autopipeline-img2img.png\" alt=\"generated image of a vermeer portrait of a dog wearing a pearl earring\"/>\n</div>\n\nAnd if you want to do inpainting, then [`AutoPipelineForInpainting`] loads the underlying [`StableDiffusionInpaintPipeline`] class in the same way:\n\n```py\nfrom diffusers import AutoPipelineForInpainting\nfrom diffusers.utils import load_image\nimport torch\n\npipeline = AutoPipelineForInpainting.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, use_safetensors=True\n).to(\"cuda\")\n\nimg_url = \"https://raw.github...", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# AutoPipeline\n\n`AutoPipeline` is designed to:\n\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n2. use multiple pipelines in your workflow\n\nBased on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from_pretrained()` method.\n\nTo seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the `from_pipe()` method to transfer the components from the original pipeline to the new one.\n\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True\n).to(\"cuda\")\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n\nimage = pipeline(prompt, num_inference_steps=25).images[0]\n```\n\n<Tip>\n\nCheck out the [AutoPipeline](../../tutorials/autopipeline) tutorial to learn how to use this API!\n\n</Tip>\n\n`AutoPipeline` supports text-to-image, image-to-image, and inpainting for the following diffusion models:\n\n- [Stable Diffusion](./stable_diffusion/overview)\n- [ControlNet](./controlnet)\n- [Stable Diffusion XL (SDXL)](./stable_diffusion/stable_diffusion_xl)\n- [DeepFloyd IF](./deepfloyd_if)\n- [Kandinsky 2.1](./kandinsky)\n- [Kandinsky 2.2](./kandinsky_v22)\n\n\n## AutoPipelineForText2Image\n\n[[autodoc]] AutoPipelineForText2Image\n\t- all\n\t- from_pretrained\n\t- from_pipe\n\n## AutoPipelineForImage2Image\n\n[[autodoc]] AutoPipelineForImage2Image\n\t- all\n\t- from_pretrained\n\t- from_pipe\n\n## AutoPipelineForInpainting\n\n[[autodoc]] AutoPipelineForInpainting\n\t- all\n\t- from_pretrained\n\t- from_pipe", "The [`~AutoPipelineForImage2Image.from_pipe`] method detects the original pipeline class and maps it to the new pipeline class corresponding to the task you want to do. For example, if you load a `\"stable-diffusion\"` class pipeline for text-to-image:\n\n```py\nfrom diffusers import AutoPipelineForText2Image, AutoPipelineForImage2Image\nimport torch\n\npipeline_text2img = AutoPipelineForText2Image.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True\n)\nprint(type(pipeline_text2img))\n\"<class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'>\"\n```\n\nThen [`~AutoPipelineForImage2Image.from_pipe`] maps the original `\"stable-diffusion\"` pipeline class to [`StableDiffusionImg2ImgPipeline`]:\n\n```py\npipeline_img2img = AutoPipelineForImage2Image.from_pipe(pipeline_text2img)\nprint(type(pipeline_img2img))\n\"<class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_img2img.StableDiffusionImg2ImgPipeline'>\"\n```\n\nIf you passed an optional argument - like disabling the safety checker - to the original pipeline, this argument is also passed on to the new pipeline:\n\n```py\nfrom diffusers import AutoPipelineForText2Image, AutoPipelineForImage2Image\nimport torch\n\npipeline_text2img = AutoPipelineForText2Image.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\",\n    torch_dtype=torch.float16,\n    use_safetensors=True,\n    requires_safety_checker=False,\n).to(\"cuda\")\n\npipeline_img2img = AutoPipelineForImage2Image.from_pipe(pipeline_text2img)\nprint(pipeline_img2img.config.requires_safety_checker)\n\"False\"\n```\n\nYou can overwrite any of the arguments and even configuration from the original pipeline if you want to change the behavior of the new pipeline. For example, to turn the safety checker back on and add the `strength` argument:\n\n```py\npipeline_img2img = AutoPipelineForImage2Image.from_pipe(pipeline_text2img, requires_safety_checker=True, strength=0.3)\nprint(pipeline_img2img.config.requires_safety_checker)\n\"True\"\n```"], "test_settings": "chunk_7000_embeddings_openai-text-embedding-3-small_reader-model_gpt-4", "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response correctly identifies the method `from_pretrained()` as the one used to load a checkpoint for a task using `AutoPipeline`. The inclusion of \"[\ubb38\uc11c 0, \ubb38\uc11c 1]\" is unnecessary and does not contribute to the accuracy or completeness of the answer. However, the core information provided is accurate and aligns with the criteria for a correct response. Therefore, the response is mostly accurate and factual, but the extraneous reference to documents slightly detracts from its precision."}, {"question": "What is the purpose of Diffusers library?\n", "true_answer": "To serve as a modular toolbox for both inference and training of state-of-the-art pretrained diffusion models across multiple modalities.", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/conceptual/philosophy.md", "generated_answer": "The purpose of the Diffusers library is to provide a state-of-the-art, modular toolbox for pretrained diffusion models that can generate images, audio, and 3D structures of molecules. It supports both simple inference solutions and the training of custom diffusion models, with a focus on usability, simplicity, and customizability [\ubb38\uc11c 0].", "retrieved_docs": ["!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n<p align=\"center\">\n    <br>\n    <img src=\"https://raw.githubusercontent.com/huggingface/diffusers/77aadfee6a891ab9fcfb780f87c693f7a5beeb8e/docs/source/imgs/diffusers_library.jpg\" width=\"400\"/>\n    <br>\n</p>\n\n# Diffusers\n\n\ud83e\udd17 Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you're looking for a simple inference solution or want to train your own diffusion model, \ud83e\udd17 Diffusers is a modular toolbox that supports both. Our library is designed with a focus on [usability over performance](conceptual/philosophy#usability-over-performance), [simple over easy](conceptual/philosophy#simple-over-easy), and [customizability over abstractions](conceptual/philosophy#tweakable-contributorfriendly-over-abstraction).\n\nThe library has three main components:\n\n- State-of-the-art diffusion pipelines for inference with just a few lines of code. There are many pipelines in \ud83e\udd17 Diffusers, check out the table in the pipeline [overview](api/pipelines/overview) for a complete list of available pipelines and the task they solve.\n- Interchangeable [noise schedulers](api/schedulers/overview) for balancing trade-offs between generation speed and quality.\n- Pretrained [models](api/models) that can be used as building blocks, and combined with schedulers, for creating your own end-to-end diffusion systems.\n\n<div class=\"mt-10\">\n  <div class=\"w-full flex flex-col space-y-4 md:space-y-0 md:grid md:grid-cols-2 md:gap-y-4 md:gap-x-5\">\n    <a class=\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\" href=\"./tutorials/tutorial_overview\"\n      ><div class=\"w-full text-center bg-gradient-to-br from-blue-400 to-blue-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed\">Tutorials</div>\n      <p class=\"text-gray-700\">Learn the fundamental skills you need to start generating outputs, build your own diffusion system, and train a diffusion model. We recommend starting here if you're using \ud83e\udd17 Diffusers for the first time!</p>\n    </a>\n    <a class=\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\" href=\"./using-diffusers/loading_overview\"\n      ><div class=\"w-full text-center bg-gradient-to-br from-indigo-400 to-indigo-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed\">How-to guides</div>\n      <p class=\"text-gray-700\">Practical guides for helping you load pipelines, models, and schedulers. You'll also learn how to use pipelines for specific tasks, control how outputs are generated, optimize for inference speed, and different training techniques.</p>\n    </a>\n    <a class=\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\" href=\"./conceptual/philosophy\"\n      ><div class=\"w-full text-center bg-gradient-to-br from-pink-400 to-pink-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed\">Conceptual guides</div>\n      <p class=\"text-gray-700\">Understand why the library was designed the way it was, and learn more about the ethical guidelines and safety implementations for using the library.</p>\n   </a>\n    <a class=\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\" href=\"./api/models/overview\"\n      ><div class=\"w-full text-center bg-gradient-to-br from-purple-400 to-purple-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed\">Reference</div>\n      <p class=\"text-gray-700\">Technical descriptions of how \ud83e\udd17 Diffusers classes and methods work.</p>\n    </a>\n  </div>\n</div>", "\ud83e\udde8 Diffusers Experimental\n\nWe are adding experimental code to support novel applications and usages of the Diffusers library.\nCurrently, the following experiments are supported:\n* Reinforcement learning via an implementation of the [Diffuser](https://arxiv.org/abs/2205.09991) model.", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Overview\n\nThe APIs in this section are more experimental and prone to breaking changes. Most of them are used internally for development, but they may also be useful to you if you're interested in building a diffusion model with some custom parts or if you're interested in some of our helper utilities for working with \ud83e\udd17 Diffusers."], "test_settings": "chunk_7000_embeddings_openai-text-embedding-3-small_reader-model_gpt-4", "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response accurately describes the purpose of the Diffusers library, highlighting its role as a modular toolbox for pretrained diffusion models. It correctly mentions the library's capabilities in generating images, audio, and 3D structures, as well as its support for both inference and training of custom models. The response aligns well with the key aspects of the criteria answer, focusing on usability, simplicity, and customizability. However, the inclusion of \"[\ubb38\uc11c 0]\" is unnecessary and detracts slightly from the clarity of the response. Overall, the response is mostly accurate and factual, with minor room for improvement in presentation."}, {"question": "What method does the EulerAncestralDiscreteScheduler use for sampling?\n", "true_answer": "Ancestral sampling with Euler method steps.", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/schedulers/euler_ancestral.md", "generated_answer": "The EulerAncestralDiscreteScheduler uses ancestral sampling with Euler method steps for sampling [\ubb38\uc11c 0].", "retrieved_docs": ["!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# EulerAncestralDiscreteScheduler\n\nA scheduler that uses ancestral sampling with Euler method steps. This is a fast scheduler which can often generate good outputs in 20-30 steps. The scheduler is based on the original [k-diffusion](https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L72) implementation by [Katherine Crowson](https://github.com/crowsonkb/).\n\n## EulerAncestralDiscreteScheduler\n[[autodoc]] EulerAncestralDiscreteScheduler\n\n## EulerAncestralDiscreteSchedulerOutput\n[[autodoc]] schedulers.scheduling_euler_ancestral_discrete.EulerAncestralDiscreteSchedulerOutput", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# EulerDiscreteScheduler\n\nThe Euler scheduler (Algorithm 2) is from the [Elucidating the Design Space of Diffusion-Based Generative Models](https://huggingface.co/papers/2206.00364) paper by Karras et al. This is a fast scheduler which can often generate good outputs in 20-30 steps. The scheduler is based on the original [k-diffusion](https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L51) implementation by [Katherine Crowson](https://github.com/crowsonkb/).\n\n\n## EulerDiscreteScheduler\n[[autodoc]] EulerDiscreteScheduler\n\n## EulerDiscreteSchedulerOutput\n[[autodoc]] schedulers.scheduling_euler_discrete.EulerDiscreteSchedulerOutput", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# KDPM2AncestralDiscreteScheduler\n\nThe `KDPM2DiscreteScheduler` with ancestral sampling is inspired by the [Elucidating the Design Space of Diffusion-Based Generative Models](https://huggingface.co/papers/2206.00364) paper, and the scheduler is ported from and created by [Katherine Crowson](https://github.com/crowsonkb/).\n\nThe original codebase can be found at [crowsonkb/k-diffusion](https://github.com/crowsonkb/k-diffusion).\n\n## KDPM2AncestralDiscreteScheduler\n[[autodoc]] KDPM2AncestralDiscreteScheduler\n\n## SchedulerOutput\n[[autodoc]] schedulers.scheduling_utils.SchedulerOutput"], "test_settings": "chunk_7000_embeddings_openai-text-embedding-3-small_reader-model_gpt-4", "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response correctly identifies the sampling method used by the EulerAncestralDiscreteScheduler as \"ancestral sampling with Euler method steps.\" The inclusion of \"[\ubb38\uc11c 0]\" is unnecessary but does not affect the accuracy of the response. Overall, the response is mostly correct and factual."}, {"question": "What is the name of the large multimodal model that can solve image-text tasks and is based on Flamingo?\n", "true_answer": "IDEFICS", "source_doc": "huggingface/transformers/blob/main/docs/source/en/tasks/idefics.md", "generated_answer": "The large multimodal model based on Flamingo that can solve image-text tasks is named \"FLAVA\" [0].", "retrieved_docs": ["!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# FLAVA\n\n## Overview\n\nThe FLAVA model was proposed in [FLAVA: A Foundational Language And Vision Alignment Model](https://arxiv.org/abs/2112.04482) by Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela and is accepted at CVPR 2022.\n\nThe paper aims at creating a single unified foundation model which can work across vision, language\nas well as vision-and-language multimodal tasks.\n\nThe abstract from the paper is the following:\n\n*State-of-the-art vision and vision-and-language models rely on large-scale visio-linguistic pretraining for obtaining good performance on a variety\nof downstream tasks. Generally, such models are often either cross-modal (contrastive) or multi-modal\n(with earlier fusion) but not both; and they often only target specific modalities or tasks. A promising\ndirection would be to use a single holistic universal model, as a \"foundation\", that targets all modalities\nat once -- a true vision and language foundation model should be good at vision tasks, language tasks, and\ncross- and multi-modal vision and language tasks. We introduce FLAVA as such a model and demonstrate\nimpressive performance on a wide range of 35 tasks spanning these target modalities.*\n\nThis model was contributed by [aps](https://huggingface.co/aps). The original code can be found [here](https://github.com/facebookresearch/multimodal/tree/main/examples/flava).\n\n## FlavaConfig\n\n[[autodoc]] FlavaConfig\n\n## FlavaTextConfig\n\n[[autodoc]] FlavaTextConfig\n\n## FlavaImageConfig\n\n[[autodoc]] FlavaImageConfig\n\n## FlavaMultimodalConfig\n\n[[autodoc]] FlavaMultimodalConfig\n\n## FlavaImageCodebookConfig\n\n[[autodoc]] FlavaImageCodebookConfig\n\n## FlavaProcessor\n\n[[autodoc]] FlavaProcessor\n\n## FlavaFeatureExtractor\n\n[[autodoc]] FlavaFeatureExtractor\n\n## FlavaImageProcessor\n\n[[autodoc]] FlavaImageProcessor\n    - preprocess\n\n## FlavaForPreTraining\n\n[[autodoc]] FlavaForPreTraining\n    - forward\n\n## FlavaModel\n\n[[autodoc]] FlavaModel\n    - forward\n    - get_text_features\n    - get_image_features\n\n## FlavaImageCodebook\n\n[[autodoc]] FlavaImageCodebook\n    - forward\n    - get_codebook_indices\n    - get_codebook_probs\n\n## FlavaTextModel\n\n[[autodoc]] FlavaTextModel\n    - forward\n\n## FlavaImageModel\n\n[[autodoc]] FlavaImageModel\n    - forward\n\n## FlavaMultimodalModel\n\n[[autodoc]] FlavaMultimodalModel\n    - forward", "Note that this section is a non-exhaustive list, and there are various other approaches, as well as hybrid strategies such as [Unified-IO](https://arxiv.org/abs/2206.08916). For a more comprehensive review of multi-modal models, refer to [this work.](https://arxiv.org/abs/2210.09263)\n\n### 1) Contrastive Learning\n\n<p align=\"center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/128_vision_language_pretraining/contrastive_learning.png\" alt=\"Contrastive Learning\"><br>\n    <em>Contrastive pre-training and zero-shot image classification as shown <a href=https://openai.com/blog/clip>here</a>.</em>\n</p>\n\nContrastive learning is a commonly used pre-training objective for vision models and has proven to be a highly effective pre-training objective for vision-language models as well. Recent works such as [CLIP](https://arxiv.org/abs/2103.00020), [CLOOB](https://arxiv.org/abs/2110.11316), [ALIGN](https://arxiv.org/abs/2102.05918), and [DeCLIP](https://arxiv.org/abs/2110.05208) bridge the vision and language modalities by learning a text encoder and an image encoder jointly with a contrastive loss, using large datasets consisting of {image, caption} pairs. Contrastive learning aims to map input images and texts to the same feature space such that the distance between the embeddings of image-text pairs is minimized if they match or maximized if they don\u2019t. \n\nFor CLIP, the distance is simply the cosine distance between the text and image embeddings, whereas models such as ALIGN and DeCLIP design their own distance metrics to account for noisy datasets. \n\nAnother work, [LiT](https://arxiv.org/abs/2111.07991), introduces a simple method for fine-tuning the text encoder using the CLIP pre-training objective while keeping the image encoder frozen. The authors interpret this idea as _a way to teach the text encoder to better read image embeddings from the image encoder_. This approach has been shown to be effective and is more sample efficient than CLIP. Other works, such as [FLAVA](https://arxiv.org/abs/2112.04482), use a combination of contrastive learning and other pretraining strategies to align vision and language embeddings.\n\n### 2) PrefixLM\n\n<p align=\"center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/128_vision_language_pretraining/prefixlm.png\" alt=\"PrefixLM\"><br>\n    <em>A diagram of the PrefixLM pre-training strategy (<a ahref=https://ai.googleblog.com/2021/10/simvlm-simple-visual-language-model-pre.html>image source<a>)</em>\n</p>\n\nAnother approach to training vision-language models is using a PrefixLM objective. Models such as [SimVLM](https://arxiv.org/abs/2108.10904) and [VirTex](https://arxiv.org/abs/2006.06666v3) use this pre-training objective and feature a unified multi-modal architecture consisting of a transformer encoder and transformer decoder, similar to that of an autoregressive language model.\n\nLet\u2019s break this down and see how this works. Language models with a prefix objective predict the next token given an input text as the prefix. For example, given the sequence \u201cA man is standing at the corner\u201d, we can use \u201cA man is standing at the\u201d as the prefix and train the model with the objective of predicting the next token - \u201ccorner\u201d or another plausible continuation of the prefix. \n\nVisual transformers (ViT) apply the same concept of the prefix to images by dividing each image into a number of patches and sequentially feeding these patches to the model as inputs. Leveraging this idea, SimVLM features an architecture where the encoder receives a concatenated image patch sequence and prefix text sequence as the prefix input, and the decoder then predicts the continuation of the textual sequence. The diagram above depicts this idea. The SimVLM model is first pre-trained on a text dataset without image patches present in the prefix and then on an aligned image-text dataset. These models are used for image-conditioned text generation/captioning and VQA tasks. \n\nModels that leverage a unified multi-modal architecture to fuse visual information into a language model (LM) for image-guided tasks show impressive capabilities. However, models that solely use the PrefixLM strategy can be limited in terms of application areas as they are mainly designed for image captioning or visual question-answering downstream tasks. For example, given an image of a group of people, we can query the image to write a description of the image (e.g., \u201cA group of people is standing together in front of a building and smiling\u201d) or query it with questions that require visual reasoning: \u201cHow many people are wearing red t-shirts?\u201d. On the other hand, models that learn multi-modal representations or adopt hybrid approaches can be adapted for various other downstream tasks, such as object detection and image segmentation.\n\n#### Frozen PrefixLM\n\n<p align=\"center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-im...", "1. **[DiT](https://huggingface.co/docs/transformers/model_doc/dit)** (from Microsoft Research) released with the paper [DiT: Self-supervised Pre-training for Document Image Transformer](https://arxiv.org/abs/2203.02378) by Junlong Li, Yiheng Xu, Tengchao Lv, Lei Cui, Cha Zhang, Furu Wei.\n1. **[Donut](https://huggingface.co/docs/transformers/model_doc/donut)** (from NAVER), released together with the paper [OCR-free Document Understanding Transformer](https://arxiv.org/abs/2111.15664) by Geewook Kim, Teakgyu Hong, Moonbin Yim, Jeongyeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, Seunghyun Park.\n1. **[DPR](https://huggingface.co/docs/transformers/model_doc/dpr)** (from Facebook) released with the paper [Dense Passage Retrieval for Open-Domain Question Answering](https://arxiv.org/abs/2004.04906) by Vladimir Karpukhin, Barlas O\u011fuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih.\n1. **[DPT](https://huggingface.co/docs/transformers/master/model_doc/dpt)** (from Intel Labs) released with the paper [Vision Transformers for Dense Prediction](https://arxiv.org/abs/2103.13413) by Ren\u00e9 Ranftl, Alexey Bochkovskiy, Vladlen Koltun.\n1. **[EfficientFormer](https://huggingface.co/docs/transformers/model_doc/efficientformer)** (from Snap Research) released with the paper [EfficientFormer: Vision Transformers at MobileNetSpeed](https://arxiv.org/abs/2206.01191) by Yanyu Li, Geng Yuan, Yang Wen, Ju Hu, Georgios Evangelidis, Sergey Tulyakov, Yanzhi Wang, Jian Ren.\n1. **[EfficientNet](https://huggingface.co/docs/transformers/model_doc/efficientnet)** (from Google Brain) released with the paper [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://arxiv.org/abs/1905.11946) by Mingxing Tan, Quoc V. Le.\n1. **[ELECTRA](https://huggingface.co/docs/transformers/model_doc/electra)** (from Google Research/Stanford University) released with the paper [ELECTRA: Pre-training text encoders as discriminators rather than generators](https://arxiv.org/abs/2003.10555) by Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning.\n1. **[EnCodec](https://huggingface.co/docs/transformers/model_doc/encodec)** (from Meta AI) released with the paper [High Fidelity Neural Audio Compression](https://arxiv.org/abs/2210.13438) by Alexandre D\u00e9fossez, Jade Copet, Gabriel Synnaeve, Yossi Adi.\n1. **[EncoderDecoder](https://huggingface.co/docs/transformers/model_doc/encoder-decoder)** (from Google Research) released with the paper [Leveraging Pre-trained Checkpoints for Sequence Generation Tasks](https://arxiv.org/abs/1907.12461) by Sascha Rothe, Shashi Narayan, Aliaksei Severyn.\n1. **[ERNIE](https://huggingface.co/docs/transformers/model_doc/ernie)** (from Baidu) released with the paper [ERNIE: Enhanced Representation through Knowledge Integration](https://arxiv.org/abs/1904.09223) by Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu, Hao Tian, Hua Wu.\n1. **[ErnieM](https://huggingface.co/docs/transformers/model_doc/ernie_m)** (from Baidu) released with the paper [ERNIE-M: Enhanced Multilingual Representation by Aligning Cross-lingual Semantics with Monolingual Corpora](https://arxiv.org/abs/2012.15674) by Xuan Ouyang, Shuohuan Wang, Chao Pang, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang.\n1. **[ESM](https://huggingface.co/docs/transformers/model_doc/esm)** (from Meta AI) are transformer protein language models.  **ESM-1b** was released with the paper [Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences](https://www.pnas.org/content/118/15/e2016239118) by Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle Ott, C. Lawrence Zitnick, Jerry Ma, and Rob Fergus. **ESM-1v** was released with the paper [Language models enable zero-shot prediction of the effects of mutations on protein function](https://doi.org/10.1101/2021.07.09.450648) by Joshua Meier, Roshan Rao, Robert Verkuil, Jason Liu, Tom Sercu and Alexander Rives. **ESM-2** was released with the paper [Language models of protein sequences at the scale of evolution enable accurate structure prediction](https://doi.org/10.1101/2022.07.20.500902) by Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Allan dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Sal Candido, Alexander Rives.\n1. **[Falcon](https://huggingface.co/docs/transformers/model_doc/falcon)** (from Technology Innovation Institute) by Almazrouei, Ebtesam and Alobeidli, Hamza and Alshamsi, Abdulaziz and Cappelli, Alessandro and Cojocaru, Ruxandra and Debbah, Merouane and Goffinet, Etienne and Heslow, Daniel and Launay, Julien and Malartic, Quentin and Noune, Badreddine and Pannier, Baptiste and Penedo, Guilherme.\n1. **[FLAN-T5](https://huggingface.co/docs/transformers/model_doc/flan-t5)** (from Google AI) released in the repository [google-research/t5x](https://github.com/google-research/t5..."], "test_settings": "chunk_7000_embeddings_openai-text-embedding-3-small_reader-model_gpt-4", "eval_score_GPT4": "1", "eval_feedback_GPT4": "The response is entirely incorrect and does not provide the correct name of the model based on Flamingo. The correct answer is \"IDEFICS,\" not \"FLAVA.\""}, {"question": "What is the purpose of the `gradio.Blocks` API?\n", "true_answer": "The `gradio.Blocks` API allows you to have full control over the data flows and layout of your application, enabling the building of complex, multi-step applications.", "source_doc": "huggingface/course/blob/main/chapters/en/chapter9/7.mdx", "generated_answer": "The purpose of the `gradio.Blocks` API is to provide a low-level approach for designing web apps with flexible layouts and complex data flows. It allows developers to control the placement of components on a page, handle complex interactions where outputs can serve as inputs to other functions, and update the properties or visibility of components based on user interactions, all within Python [\ubb38\uc11c 0].", "retrieved_docs": ["### Custom Demos with `gr.Blocks`\n\nGradio also offers a low-level approach for designing web apps with more flexible layouts and data flows with the `gr.Blocks` class. Blocks allows you to do things like control where components appear on the page, handle complex data flows (e.g. outputs can serve as inputs to other functions), and update properties/visibility of components based on user interaction \u2014 still all in Python. \n\nYou can build very custom and complex applications using `gr.Blocks()`. For example, the popular image generation [Automatic1111 Web UI](https://github.com/AUTOMATIC1111/stable-diffusion-webui) is built using Gradio Blocks. We dive deeper into the `gr.Blocks` on our series on [building with Blocks](https://www.gradio.app/guides/blocks-and-event-listeners).\n\n\n### The Gradio Python & JavaScript Ecosystem\n\nThat's the gist of the core `gradio` Python library, but Gradio is actually so much more! Its an entire ecosystem of Python and JavaScript libraries that let you build machine learning applications, or query them programmatically, in Python or JavaScript. Here are other related parts of the Gradio ecosystem:\n\n* [Gradio Python Client](https://www.gradio.app/guides/getting-started-with-the-python-client) (`gradio_client`): query any Gradio app programmatically in Python.\n* [Gradio JavaScript Client](https://www.gradio.app/guides/getting-started-with-the-js-client) (`@gradio/client`): query any Gradio app programmatically in JavaScript.\n* [Gradio-Lite](https://www.gradio.app/guides/gradio-lite) (`@gradio/lite`): write Gradio apps in Python that run entirely in the browser (no server needed!), thanks to Pyodide. \n* [Hugging Face Spaces](https://huggingface.co/spaces): the most popular place to host Gradio applications \u2014 for free!\n\n## What's Next?\n\nKeep learning about Gradio sequentially using the Gradio Guides, which include explanations as well as example code and embedded interactive demos. Next up: [key features about Gradio demos](https://www.gradio.app/guides/key-features).\n\nOr, if you already know the basics and are looking for something specific, you can search the more [technical API documentation](https://www.gradio.app/docs/).", "Gradio Demo: blocks_static\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\ndemo = gr.Blocks()\n\nwith demo:\n    gr.Image(\n        \"https://images.unsplash.com/photo-1507003211169-0a1dd7228f2d?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=387&q=80\"\n    )\n    gr.Textbox(\"hi\")\n    gr.Number(3)\n\nif __name__ == \"__main__\":\n    demo.launch()\n\n```", "Gradio Demo: audio_component\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    gr.Audio()\n\ndemo.launch()\n```"], "test_settings": "chunk_7000_embeddings_openai-text-embedding-3-small_reader-model_gpt-4", "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response accurately describes the purpose of the `gradio.Blocks` API, highlighting its role in providing a low-level approach for designing web apps with flexible layouts and complex data flows. It correctly mentions the ability to control component placement, handle complex interactions, and update component properties based on user interactions. However, the reference to \"[\ubb38\uc11c 0]\" is unnecessary and detracts slightly from the clarity of the response. Overall, the response aligns well with the criteria for a high score, as it is mostly accurate and factual."}, {"question": "What is the purpose of the two-stage model proposed in the paper \"Hierarchical Text-Conditional Image Generation with CLIP Latents\"?\n", "true_answer": "The purpose of the two-stage model is to generate a CLIP image embedding given a text caption and then generate an image conditioned on the image embedding.", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/pipelines/unclip.md", "generated_answer": "The purpose of the two-stage model proposed in the paper \"Hierarchical Text-Conditional Image Generation with CLIP Latents\" is to leverage the robust representations learned by contrastive models like CLIP for image generation. The model consists of a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. This approach improves image diversity with minimal loss in photorealism and caption similarity, and allows for variations of an image that preserve both its semantics and style while varying non-essential details. Additionally, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion [\ubb38\uc11c 0, \ubb38\uc11c 1].", "retrieved_docs": ["!--Copyright 2023 The HuggingFace Team. All rights reserved.\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# unCLIP\n\n[Hierarchical Text-Conditional Image Generation with CLIP Latents](https://huggingface.co/papers/2204.06125) is by Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen. The unCLIP model in \ud83e\udd17 Diffusers comes from kakaobrain's [karlo](https://github.com/kakaobrain/karlo).\n\nThe abstract from the paper is following:\n\n*Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.*\n\nYou can find lucidrains' DALL-E 2 recreation at [lucidrains/DALLE2-pytorch](https://github.com/lucidrains/DALLE2-pytorch).\n\n<Tip>\n\nMake sure to check out the Schedulers [guide](../../using-diffusers/schedulers) to learn how to explore the tradeoff between scheduler speed and quality, and see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines) section to learn how to efficiently load the same components into multiple pipelines.\n\n</Tip>\n\n## UnCLIPPipeline\n[[autodoc]] UnCLIPPipeline\n\t- all\n\t- __call__\n\n## UnCLIPImageVariationPipeline\n[[autodoc]] UnCLIPImageVariationPipeline\n\t- all\n\t- __call__\n\n## ImagePipelineOutput\n[[autodoc]] pipelines.ImagePipelineOutput", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Prior Transformer\n\nThe Prior Transformer was originally introduced in [Hierarchical Text-Conditional Image Generation with CLIP Latents](https://huggingface.co/papers/2204.06125) by Ramesh et al. It is used to predict CLIP image embeddings from CLIP text embeddings; image embeddings are predicted through a denoising diffusion process.\n\nThe abstract from the paper is:\n\n*Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.*\n\n## PriorTransformer\n\n[[autodoc]] PriorTransformer\n\n## PriorTransformerOutput\n\n[[autodoc]] models.prior_transformer.PriorTransformerOutput", "!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# CLIP\n\n## Overview\n\nThe CLIP model was proposed in [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020) by Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,\nSandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever. CLIP\n(Contrastive Language-Image Pre-Training) is a neural network trained on a variety of (image, text) pairs. It can be\ninstructed in natural language to predict the most relevant text snippet, given an image, without directly optimizing\nfor the task, similarly to the zero-shot capabilities of GPT-2 and 3.\n\nThe abstract from the paper is the following:\n\n*State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This\nrestricted form of supervision limits their generality and usability since additional labeled data is needed to specify\nany other visual concept. Learning directly from raw text about images is a promising alternative which leverages a\nmuch broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes\nwith which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400\nmillion (image, text) pairs collected from the internet. After pre-training, natural language is used to reference\nlearned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study\nthe performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks\nsuch as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The\nmodel transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need\nfor any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot\nwithout needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained\nmodel weights at this https URL.*\n\nThis model was contributed by [valhalla](https://huggingface.co/valhalla). The original code can be found [here](https://github.com/openai/CLIP).\n\n## Usage tips and example\n\nCLIP is a multi-modal vision and language model. It can be used for image-text similarity and for zero-shot image\nclassification. CLIP uses a ViT like transformer to get visual features and a causal language model to get the text\nfeatures. Both the text and visual features are then projected to a latent space with identical dimension. The dot\nproduct between the projected image and text features is then used as a similar score.\n\nTo feed images to the Transformer encoder, each image is split into a sequence of fixed-size non-overlapping patches,\nwhich are then linearly embedded. A [CLS] token is added to serve as representation of an entire image. The authors\nalso add absolute position embeddings, and feed the resulting sequence of vectors to a standard Transformer encoder.\nThe [`CLIPImageProcessor`] can be used to resize (or rescale) and normalize images for the model.\n\nThe [`CLIPTokenizer`] is used to encode the text. The [`CLIPProcessor`] wraps\n[`CLIPImageProcessor`] and [`CLIPTokenizer`] into a single instance to both\nencode the text and prepare the images. The following example shows how to get the image-text similarity scores using\n[`CLIPProcessor`] and [`CLIPModel`].\n\n\n```python\n>>> from PIL import Image\n>>> import requests\n\n>>> from transformers import CLIPProcessor, CLIPModel\n\n>>> model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n>>> processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> inputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True)\n\n>>> outputs = model(**inputs)\n>>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n>>> probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabi..."], "test_settings": "chunk_7000_embeddings_openai-text-embedding-3-small_reader-model_gpt-4", "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response provides a detailed explanation of the two-stage model proposed in the paper \"Hierarchical Text-Conditional Image Generation with CLIP Latents.\" It accurately describes the purpose of the model, which is to leverage CLIP's robust representations for image generation. The response correctly identifies the two stages: generating a CLIP image embedding from a text caption and then generating an image conditioned on that embedding. Additionally, it highlights the benefits of the approach, such as improved image diversity and the ability for zero-shot language-guided image manipulations. While the response includes more information than the concise 5-point answer, it remains accurate and relevant to the question. Therefore, it meets the criteria for a score of 5, as it is entirely correct, precise, and factual."}, {"question": "What command is used to install the requirements for a research project using \ud83e\udd17 Transformers?\n", "true_answer": "pip install -r requirements.txt", "source_doc": "huggingface/transformers/blob/main/examples/research_projects/README.md", "generated_answer": "The command to install the requirements for a research project using \ud83e\udd17 Transformers is:\n\n```bash\npip install -r requirements.txt\n```\n\nThis command should be run inside the folder of your choice for the specific research project. [\ubb38\uc11c 0]", "retrieved_docs": ["!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n\n# Research projects\n\nThis folder contains various research projects using \ud83e\udd17 Transformers. They are not maintained and require a specific\nversion of \ud83e\udd17 Transformers that is indicated in the requirements file of each folder. Updating them to the most recent version of the library will require some work.\n\nTo use any of them, just run the command\n```\npip install -r requirements.txt\n```\ninside the folder of your choice.\n\nIf you need help with any of those, contact the author(s), indicated at the top of the `README` of each folder.", "First, create a virtual environment with the version of Python you're going to use and activate it.\n\nThen, you will need to install at least one of Flax, PyTorch, or TensorFlow.\nPlease refer to [TensorFlow installation page](https://www.tensorflow.org/install/), [PyTorch installation page](https://pytorch.org/get-started/locally/#start-locally) and/or [Flax](https://github.com/google/flax#quick-install) and [Jax](https://github.com/google/jax#installation) installation pages regarding the specific installation command for your platform.\n\nWhen one of those backends has been installed, \ud83e\udd17 Transformers can be installed using pip as follows:\n\n```bash\npip install transformers\n```\n\nIf you'd like to play with the examples or need the bleeding edge of the code and can't wait for a new release, you must [install the library from source](https://huggingface.co/docs/transformers/installation#installing-from-source).\n\n### With conda\n\nSince Transformers version v4.0.0, we now have a conda channel: `huggingface`.\n\n\ud83e\udd17 Transformers can be installed using conda as follows:\n\n```shell script\nconda install -c huggingface transformers\n```\n\nFollow the installation pages of Flax, PyTorch or TensorFlow to see how to install them with conda.\n\n> **_NOTE:_**  On Windows, you may be prompted to activate Developer Mode in order to benefit from caching. If this is not an option for you, please let us know in [this issue](https://github.com/huggingface/huggingface_hub/issues/1062).\n\n## Model architectures\n\n**[All the model checkpoints](https://huggingface.co/models)** provided by \ud83e\udd17 Transformers are seamlessly integrated from the huggingface.co [model hub](https://huggingface.co/models), where they are uploaded directly by [users](https://huggingface.co/users) and [organizations](https://huggingface.co/organizations).\n\nCurrent number of checkpoints: ![](https://img.shields.io/endpoint?url=https://huggingface.co/api/shields/models&color=brightgreen)\n\n\ud83e\udd17 Transformers currently provides the following architectures (see [here](https://huggingface.co/docs/transformers/model_summary) for a high-level summary of each them):", "!---\nCopyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\n\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Installation\n\nInstall \ud83e\udd17 Transformers for whichever deep learning library you're working with, setup your cache, and optionally configure \ud83e\udd17 Transformers to run offline.\n\n\ud83e\udd17 Transformers is tested on Python 3.6+, PyTorch 1.1.0+, TensorFlow 2.0+, and Flax. Follow the installation instructions below for the deep learning library you are using:\n\n* [PyTorch](https://pytorch.org/get-started/locally/) installation instructions.\n* [TensorFlow 2.0](https://www.tensorflow.org/install/pip) installation instructions.\n* [Flax](https://flax.readthedocs.io/en/latest/) installation instructions.\n\n## Install with pip\n\nYou should install \ud83e\udd17 Transformers in a [virtual environment](https://docs.python.org/3/library/venv.html). If you're unfamiliar with Python virtual environments, take a look at this [guide](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/). A virtual environment makes it easier to manage different projects, and avoid compatibility issues between dependencies.\n\nStart by creating a virtual environment in your project directory:\n\n```bash\npython -m venv .env\n```\n\nActivate the virtual environment. On Linux and MacOs:\n\n```bash\nsource .env/bin/activate\n```\nActivate Virtual environment on Windows\n\n```bash\n.env/Scripts/activate\n```\n\nNow you're ready to install \ud83e\udd17 Transformers with the following command:\n\n```bash\npip install transformers\n```\n\nFor CPU-support only, you can conveniently install \ud83e\udd17 Transformers and a deep learning library in one line. For example, install \ud83e\udd17 Transformers and PyTorch with:\n\n```bash\npip install 'transformers[torch]'\n```\n\n\ud83e\udd17 Transformers and TensorFlow 2.0:\n\n```bash\npip install 'transformers[tf-cpu]'\n```\n\n<Tip warning={true}>\n\nM1 / ARM Users\n    \nYou will need to install the following before installing TensorFLow 2.0\n```\nbrew install cmake\nbrew install pkg-config\n```\n\n</Tip>\n\n\ud83e\udd17 Transformers and Flax:\n\n```bash\npip install 'transformers[flax]'\n```\n\nFinally, check if \ud83e\udd17 Transformers has been properly installed by running the following command. It will download a pretrained model:\n\n```bash\npython -c \"from transformers import pipeline; print(pipeline('sentiment-analysis')('we love you'))\"\n```\n\nThen print out the label and score:\n\n```bash\n[{'label': 'POSITIVE', 'score': 0.9998704791069031}]\n```\n\n## Install from source\n\nInstall \ud83e\udd17 Transformers from source with the following command:\n\n```bash\npip install git+https://github.com/huggingface/transformers\n```\n\nThis command installs the bleeding edge `main` version rather than the latest `stable` version. The `main` version is useful for staying up-to-date with the latest developments. For instance, if a bug has been fixed since the last official release but a new release hasn't been rolled out yet. However, this means the `main` version may not always be stable. We strive to keep the `main` version operational, and most issues are usually resolved within a few hours or a day. If you run into a problem, please open an [Issue](https://github.com/huggingface/transformers/issues) so we can fix it even sooner!\n\nCheck if \ud83e\udd17 Transformers has been properly installed by running the following command:\n\n```bash\npython -c \"from transformers import pipeline; print(pipeline('sentiment-analysis')('I love you'))\"\n```\n\n## Editable install\n\nYou will need an editable install if you'd like to:\n\n* Use the `main` version of the source code.\n* Contribute to \ud83e\udd17 Transformers and need to test changes in the code.\n\nClone the repository and install \ud83e\udd17 Transformers with the following commands:\n\n```bash\ngit clone https://github.com/huggingface/transformers.git\ncd transformers\npip install -e .\n```\n\nThese commands will link the folder you cloned the repository to and your Python library paths. Python will now look inside the folder you cloned to in addition to the normal library paths. For example, if your Python packages are typically installed in `~/anaconda3/envs/main/lib/python3.7/site-packages/`, Python will also search the folder you cloned to: `~/transformers/`.\n\n<Tip warning={true}>\n\nYou must keep the `transformers` folder if you want to keep using the library.\n\n</Tip>\n\nNow you can easily update your clone to the latest version of \ud83e\udd17 Transformers with the following command:\n\n```bash\ncd ~/transformers/\ngit pull\n`..."], "test_settings": "chunk_7000_embeddings_openai-text-embedding-3-small_reader-model_gpt-4", "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response correctly identifies the command to install requirements using a `requirements.txt` file, which is a common practice in Python projects, including those using \ud83e\udd17 Transformers. The additional note about running the command inside the project folder is helpful, though not necessary for the core answer. The response is accurate and aligns with the criteria for a 5-point answer, as it provides the correct command without any inaccuracies."}, {"question": "What task does the `roberta-large-mnli` checkpoint perform?\n", "true_answer": "Text classification", "source_doc": "huggingface/course/blob/main/chapters/en/chapter1/10.mdx", "generated_answer": "The `roberta-large-mnli` checkpoint is typically used for the task of natural language inference (NLI), which involves determining the relationship between a pair of sentences, such as entailment, contradiction, or neutrality.", "retrieved_docs": ["## [LoRA](https://arxiv.org/abs/2106.09685)\n\nPEFT, Parameter Efficient Fine-Tuning, is a collection of techniques (p-tuning, prefix-tuning, IA3, Adapters, and LoRa) designed to fine-tune large models using a much smaller set of training parameters while preserving the performance levels typically achieved through full fine-tuning. \n\nLoRA, Low-Rank Adaptation, is a PEFT method that shares similarities with Adapter layers. Its primary objective is to reduce the model's trainable parameters. LoRA's operation involves \nlearning a low rank update matrix while keeping the pre-trained weights frozen.\n\n![image](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/Lora-for-sequence-classification-with-Roberta-Llama-Mistral/lora.png)\n\n## Setup\n\nRoBERTa has a limitatiom of maximum sequence length of 512, so we set the `MAX_LEN=512` for all models to ensure a fair comparison.\n\n```python\nMAX_LEN = 512 \nroberta_checkpoint = \"roberta-large\"\nmistral_checkpoint = \"mistralai/Mistral-7B-v0.1\"\nllama_checkpoint = \"meta-llama/Llama-2-7b-hf\"\n```\n\n## Data preparation\n### Data loading\n\nWe will load the dataset from Hugging Face:\n```python\nfrom datasets import load_dataset\ndataset = load_dataset(\"mehdiiraqui/twitter_disaster\")\n```\n Now, let's split the dataset into training and validation datasets. Then add the test set:\n\n```python\nfrom datasets import Dataset\n# Split the dataset into training and validation datasets\ndata = dataset['train'].train_test_split(train_size=0.8, seed=42)\n# Rename the default \"test\" split to \"validation\"\ndata['val'] = data.pop(\"test\")\n# Convert the test dataframe to HuggingFace dataset and add it into the first dataset\ndata['test'] = dataset['test']\n```\n\nHere's an overview of the dataset:\n\n```bash\nDatasetDict({\n    train: Dataset({\n        features: ['id', 'keyword', 'location', 'text', 'target'],\n        num_rows: 6090\n    })\n    val: Dataset({\n        features: ['id', 'keyword', 'location', 'text', 'target'],\n        num_rows: 1523\n    })\n    test: Dataset({\n        features: ['id', 'keyword', 'location', 'text', 'target'],\n        num_rows: 3263\n    })\n})\n```\n\nLet's check the data distribution:\n\n```python\nimport pandas as pd\n\ndata['train'].to_pandas().info()\ndata['test'].to_pandas().info()\n```\n\n- Train dataset\n\n```<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 7613 entries, 0 to 7612\nData columns (total 5 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   id        7613 non-null   int64 \n 1   keyword   7552 non-null   object\n 2   location  5080 non-null   object\n 3   text      7613 non-null   object\n 4   target    7613 non-null   int64 \ndtypes: int64(2), object(3)\nmemory usage: 297.5+ KB\n```\n\n\n- Test dataset\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 3263 entries, 0 to 3262\nData columns (total 5 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   id        3263 non-null   int64 \n 1   keyword   3237 non-null   object\n 2   location  2158 non-null   object\n 3   text      3263 non-null   object\n 4   target    3263 non-null   int64 \ndtypes: int64(2), object(3)\nmemory usage: 127.6+ KB\n```\n\n**Target distribution in the train dataset**\n```\ntarget\n0    4342\n1    3271\nName: count, dtype: int64\n```\n\nAs the classes are not balanced, we will compute the positive and negative weights and use them for loss calculation later:\n```python\npos_weights = len(data['train'].to_pandas()) / (2 * data['train'].to_pandas().target.value_counts()[1])\nneg_weights = len(data['train'].to_pandas()) / (2 * data['train'].to_pandas().target.value_counts()[0])\n```\n\nThe final weights are: \n```\nPOS_WEIGHT, NEG_WEIGHT = (1.1637114032405993, 0.8766697374481806)\n```\n\nThen, we compute the maximum length of the column text:\n```python\n# Number of Characters\nmax_char = data['train'].to_pandas()['text'].str.len().max()\n# Number of Words\nmax_words = data['train'].to_pandas()['text'].str.split().str.len().max()\n```\n\n```\nThe maximum number of characters is 152.\nThe maximum number of words is 31.\n```\n\n### Data Processing\n\nLet's take a look to one row example of training data: \n\n```python\ndata['train'][0]\n```\n\n```\n{'id': 5285,\n 'keyword': 'fear',\n 'location': 'Thibodaux, LA',\n 'text': 'my worst fear. https://t.co/iH8UDz8mq3',\n 'target': 0}\n```\n\nThe data comprises a keyword, a location and the text of the tweet. For the sake of simplicity, we select the `text` feature as the only input to the LLM. \n\nAt this stage, we prepared the train, validation, and test sets in the HuggingFace format expected by the pre-trained LLMs. The next step is to define the tokenized dataset for training using the appropriate tokenizer to transform the `text` feature into two Tensors of sequence of token ids and attention masks. As each model has its specific tokenizer, we will need to define three different datasets. \n\n\nWe start by defining the RoBERTa dataloader: \n\n-  Load the tokenizer: \n```python\nfrom transformers import AutoTokenizer\nroberta_tokenizer ...", "<PipelineTag pipeline=\"fill-mask\"/>\n\n- A blog on [How to train a new language model from scratch using Transformers and Tokenizers](https://huggingface.co/blog/how-to-train) with RoBERTa.\n- [`RobertaForMaskedLM`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling#robertabertdistilbert-and-masked-language-modeling) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb).\n- [`TFRobertaForMaskedLM`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/language-modeling#run_mlmpy) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb).\n- [`FlaxRobertaForMaskedLM`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/flax/language-modeling#masked-language-modeling) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/masked_language_modeling_flax.ipynb).\n- [Masked language modeling](https://huggingface.co/course/chapter7/3?fw=pt) chapter of the \ud83e\udd17 Hugging Face Course.\n- [Masked language modeling task guide](../tasks/masked_language_modeling)\n\n<PipelineTag pipeline=\"question-answering\"/>\n\n- A blog on [Accelerated Inference with Optimum and Transformers Pipelines](https://huggingface.co/blog/optimum-inference) with RoBERTa for question answering.\n- [`RobertaForQuestionAnswering`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering.ipynb).\n- [`TFRobertaForQuestionAnswering`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/question-answering) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering-tf.ipynb).\n- [`FlaxRobertaForQuestionAnswering`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/flax/question-answering).\n- [Question answering](https://huggingface.co/course/chapter7/7?fw=pt) chapter of the \ud83e\udd17 Hugging Face Course.\n- [Question answering task guide](../tasks/question_answering)\n\n**Multiple choice**\n- [`RobertaForMultipleChoice`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/multiple-choice) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice.ipynb).\n- [`TFRobertaForMultipleChoice`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/multiple-choice) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice-tf.ipynb).\n- [Multiple choice task guide](../tasks/multiple_choice)\n\n## RobertaConfig\n\n[[autodoc]] RobertaConfig\n\n## RobertaTokenizer\n\n[[autodoc]] RobertaTokenizer\n    - build_inputs_with_special_tokens\n    - get_special_tokens_mask\n    - create_token_type_ids_from_sequences\n    - save_vocabulary\n\n## RobertaTokenizerFast\n\n[[autodoc]] RobertaTokenizerFast\n    - build_inputs_with_special_tokens\n\n<frameworkcontent>\n<pt>\n\n## RobertaModel\n\n[[autodoc]] RobertaModel\n    - forward\n\n## RobertaForCausalLM\n\n[[autodoc]] RobertaForCausalLM\n    - forward\n\n## RobertaForMaskedLM\n\n[[autodoc]] RobertaForMaskedLM\n    - forward\n\n## RobertaForSequenceClassification\n\n[[autodoc]] RobertaForSequenceClassification\n    - forward\n\n## RobertaForMultipleChoice\n\n[[autodoc]] RobertaForMultipleChoice\n    - forward\n\n## RobertaForTokenClassification\n\n[[autodoc]] RobertaForTokenClassification\n    - forward\n\n## RobertaForQuestionAnswering\n\n[[autodoc]] RobertaForQuestionAnswering\n    - forward\n\n</pt>\n<tf>\n\n## TFRobertaModel\n\n[[autodoc]] TFRobertaModel\n    - call\n\n## TFRobertaForCausalLM\n\n[[autodoc]] TFRobertaForCausalLM\n    - call\n\n## TFRobertaForMaskedLM\n\n[[autodoc]] TFRobertaForMaskedLM\n    - call\n\n## TFRobertaForSequenceClassification\n\n[[autodoc]] TFRobertaForSequenceClassification\n    - call\n\n## TFRobertaForMultipleChoice\n\n[[autodoc]] TFRobertaForMultipleChoice\n    - call\n\n## TFRobertaForTokenClassification\n\n[[autodoc]] TFRobertaForTokenClassification\n    - call\n\n## TFRobertaForQuestionAnswering\n\n[[autodoc]] TFRobertaForQuestionAnswering\n    - call\n\n</tf>\n<jax>\n\n## FlaxRobertaModel\n\n[[autodoc]] FlaxRobertaModel\n    - __call__\n\n## FlaxRobertaForCausalLM\n\n[[autodoc]] FlaxRobertaForCausalLM\n    - __call__\n\n## FlaxRobertaForMaskedLM\n\n[[autodoc]] FlaxRobertaForMaskedLM\n    - __call__\n\n## FlaxRobertaForSequenceClassification\n\n[[autodoc]] FlaxRobertaForSequenceClassification\n    - __call__\n\n## FlaxRobertaForMultipleChoice\n\n[[autodoc]] FlaxRobertaForMultipleChoice\n    - __call__\n\n## FlaxRobert...", "**Note:** we deleted the undesired columns from our data: id, keyword, location and text. We have deleted the text because we have already converted it into the inputs ids and the attention mask:\n\n\nWe can have a look into our tokenized training dataset:\n```python\nroberta_tokenized_datasets['train'][0]\n```\n\n```\n{'label': tensor(0),\n 'input_ids': tensor([    0,   127,  2373,  2490,     4,  1205,   640,    90,     4,   876,\n            73,   118,   725,   398, 13083,   329,   398,   119,  1343,   246,\n             2]),\n 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n```\n\n\n- For generating the training batches, we also need to pad the rows of a given batch to the maximum length found in the batch. For that, we will use the `DataCollatorWithPadding` class:\n\n```python\n# Data collator for padding a batch of examples to the maximum length seen in the batch\nfrom transformers import DataCollatorWithPadding\nroberta_data_collator = DataCollatorWithPadding(tokenizer=roberta_tokenizer)\n```\n\n\nYou can follow the same steps for preparing the data for Mistral 7B and Llama 2 models: \n\n**Note** that Llama 2 and Mistral 7B don't have a default `pad_token_id`. So, we use the `eos_token_id` for padding as well.\n\n- Mistral 7B: \n\n```python\n# Load Mistral 7B Tokenizer\nfrom transformers import AutoTokenizer, DataCollatorWithPadding\nmistral_tokenizer = AutoTokenizer.from_pretrained(mistral_checkpoint, add_prefix_space=True)\nmistral_tokenizer.pad_token_id = mistral_tokenizer.eos_token_id\nmistral_tokenizer.pad_token = mistral_tokenizer.eos_token\n\ndef mistral_preprocessing_function(examples):\n    return mistral_tokenizer(examples['text'], truncation=True, max_length=MAX_LEN)\n\nmistral_tokenized_datasets = data.map(mistral_preprocessing_function, batched=True, remove_columns=col_to_delete)\nmistral_tokenized_datasets = mistral_tokenized_datasets.rename_column(\"target\", \"label\")\nmistral_tokenized_datasets.set_format(\"torch\")\n\n# Data collator for padding a batch of examples to the maximum length seen in the batch\nmistral_data_collator = DataCollatorWithPadding(tokenizer=mistral_tokenizer)\n```\n\n- Llama 2:\n```python\n# Load Llama 2 Tokenizer\nfrom transformers import AutoTokenizer, DataCollatorWithPadding\nllama_tokenizer = AutoTokenizer.from_pretrained(llama_checkpoint, add_prefix_space=True)\nllama_tokenizer.pad_token_id = llama_tokenizer.eos_token_id\nllama_tokenizer.pad_token = llama_tokenizer.eos_token\n\ndef llama_preprocessing_function(examples):\n    return llama_tokenizer(examples['text'], truncation=True, max_length=MAX_LEN)\n\nllama_tokenized_datasets = data.map(llama_preprocessing_function, batched=True, remove_columns=col_to_delete)\nllama_tokenized_datasets = llama_tokenized_datasets.rename_column(\"target\", \"label\")\nllama_tokenized_datasets.set_format(\"torch\")\n\n# Data collator for padding a batch of examples to the maximum length seen in the batch\nllama_data_collator = DataCollatorWithPadding(tokenizer=llama_tokenizer)\n```\n\nNow that we have prepared the tokenized datasets, the next section will showcase how to load the pre-trained LLMs checkpoints and how to set the LoRa weights. \n\n## Models\n\n### RoBERTa\n\n#### Load RoBERTa Checkpoints for the Classification Task\n\nWe load the pre-trained RoBERTa model with a sequence classification head using the Hugging Face `AutoModelForSequenceClassification` class:\n\n```python\nfrom transformers import AutoModelForSequenceClassification \nroberta_model = AutoModelForSequenceClassification.from_pretrained(roberta_checkpoint, num_labels=2)\n```\n\n\n####  LoRA setup for RoBERTa classifier\n\nWe import LoRa configuration and set some parameters for RoBERTa classifier:\n- TaskType: Sequence classification\n- r(rank): Rank for our decomposition matrices\n- lora_alpha: Alpha parameter to scale the learned weights. LoRA paper advises fixing alpha at 16\n- lora_dropout: Dropout probability of the LoRA layers\n- bias: Whether to add bias term to LoRa layers\n\nThe code below uses the values recommended by the [Lora paper](https://arxiv.org/abs/2106.09685). [Later in this post](#hyperparameter-tuning) we will perform hyperparameter tuning of these parameters using `wandb`.\n\n```python\nfrom peft import get_peft_model, LoraConfig, TaskType\nroberta_peft_config = LoraConfig(\n    task_type=TaskType.SEQ_CLS, r=2, lora_alpha=16, lora_dropout=0.1, bias=\"none\",\n)\nroberta_model = get_peft_model(roberta_model, roberta_peft_config)\nroberta_model.print_trainable_parameters()\n```\n\nWe can see that the number of trainable parameters represents only 0.64% of the RoBERTa model parameters:\n```bash\ntrainable params: 2,299,908 || all params: 356,610,052 || trainable%: 0.6449363911929212\n```\n\n\n### Mistral\n\n#### Load checkpoints for the classfication model\n\nLet's load the pre-trained Mistral-7B model with a sequence classification head:\n\n```python\nfrom transformers import AutoModelForSequenceClassification\nimport torch\nmistral_model =  AutoModelForSequenceClassification.from_pretrained(\n  pretrained_model_name_..."], "test_settings": "chunk_7000_embeddings_openai-text-embedding-3-small_reader-model_gpt-4", "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response accurately describes the primary function of the `roberta-large-mnli` checkpoint, which is used for natural language inference (NLI). This involves classifying the relationship between pairs of sentences into categories such as entailment, contradiction, or neutrality. While the rubric's 5-point answer is \"Text classification,\" the response provides a more detailed and specific explanation of the type of text classification performed by the model. Given that the response is factually correct and aligns with the intended use of the model, it deserves a high score. However, since the rubric's 5-point answer is more general, the response could be seen as slightly more specific than necessary. Nonetheless, it is mostly accurate and factual."}, {"question": "What service is replacing the Paid tier of the Inference API at Hugging Face?\n", "true_answer": "Inference Endpoints", "source_doc": "huggingface/blog/blob/main/pricing-update.md", "generated_answer": "The Paid tier of the Inference API at Hugging Face is being replaced by the Inference Endpoints service [Document 0].", "retrieved_docs": ["--\ntitle: Introducing our new pricing\nthumbnail: /blog/assets/114_pricing-update/thumbnail.png\nauthors:\n- user: sbrandeis\n- user: pierric\n---\n\n# Introducing our new pricing\n\n\nAs you might have noticed, our [pricing page](https://huggingface.co/pricing) has changed a lot recently.\n\nFirst of all, we are sunsetting the Paid tier of the Inference API service. The Inference API will still be available for everyone to use for free. But if you're looking for a fast, enterprise-grade inference as a service, we recommend checking out our brand new solution for this: [Inference Endpoints](https://huggingface.co/inference-endpoints).\n\nAlong with Inference Endpoints, we've recently introduced hardware upgrades for [Spaces](https://huggingface.co/spaces/launch), which allows running ML demos with the hardware of your choice. No subscription is required to use these services; you only need to add a credit card to your account from your [billing settings](https://huggingface.co/settings/billing). You can also attach a payment method to any of [your organizations](https://huggingface.co/settings/organizations).\n\nYour billing settings centralize everything about our paid services. From there, you can manage your personal PRO subscription, update your payment method, and visualize your usage for the past three months. Usage for all our paid services and subscriptions will be charged at the start of each month, and a consolidated invoice will be available for your records.\n\n**TL;DR**: **At HF we monetize by providing simple access to compute for AI**, with services like AutoTrain, Spaces and Inference Endpoints, directly accessible from the Hub. [Read more](https://huggingface.co/docs/hub/billing) about our pricing and billing system.\n\nIf you have any questions, feel free to reach out. We welcome your feedback \ud83d\udd25", "Access \ud83e\udd17 Inference Endpoints\n\nTo access the [Inference Endpoints web application](https://ui.endpoints.huggingface.co/), you or your organization need to [add a valid payment method](https://huggingface.co/settings/billing) to your Hugging Face account.\n\n<Tip>\n  \nYou can check your [billing](https://huggingface.co/settings/billing) if you're unsure whether you have an active payment method.\n  \n</Tip>\n\nThere are two pricing plans:\n\n- Inference Endpoints pricing is based on your hourly compute, and billed monthly. This can be as low as $0.06 per CPU core/hr and $0.6 per GPU/hr depending on your needs. \n- There is also an [Enterprise](https://huggingface.co/support) plan for Inference Endpoints which offers dedicated support, 24/7 SLAs, and uptime guarantees. Pricing for Enterprise is custom and based on volume commit and annual contracts; [contact us](https://huggingface.co/inference-endpoints/enterprise) for a quote if you're interested!\n\nAfter you've added a valid payment method to your account, access the [Inference Endpoints web application](https://ui.endpoints.huggingface.co/) and start deploying! \ud83e\udd73", "Inference API\n\nPlease refer to [Inference API Documentation](https://huggingface.co/docs/api-inference) for detailed information.\n\n\n## What technology do you use to power the inference API?\n\nFor \ud83e\udd17 Transformers models, [Pipelines](https://huggingface.co/docs/transformers/main_classes/pipelines) power the API.\n\nOn top of `Pipelines` and depending on the model type, there are several production optimizations like:\n- compiling models to optimized intermediary representations (e.g. [ONNX](https://medium.com/microsoftazure/accelerate-your-nlp-pipelines-using-hugging-face-transformers-and-onnx-runtime-2443578f4333)),\n- maintaining a Least Recently Used cache, ensuring that the most popular models are always loaded,\n- scaling the underlying compute infrastructure on the fly depending on the load constraints.\n\nFor models from [other libraries](./models-libraries), the API uses [Starlette](https://www.starlette.io) and runs in [Docker containers](https://github.com/huggingface/api-inference-community/tree/main/docker_images). Each library defines the implementation of [different pipelines](https://github.com/huggingface/api-inference-community/tree/main/docker_images/sentence_transformers/app/pipelines).\n\n## How can I turn off the inference API for my model?\n\nSpecify `inference: false` in your model card's metadata.\n\n## Why don't I see an inference widget or why can't I use the inference API?\n\nFor some tasks, there might not be support in the inference API, and, hence, there is no widget.\nFor all libraries (except \ud83e\udd17 Transformers), there is a [library-to-tasks.ts file](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/library-to-tasks.ts) of supported tasks in the API. When a model repository has a task that is not supported by the repository library, the repository has `inference: false` by default.\n\n\n## Can I send large volumes of requests? Can I get accelerated APIs?\n\nIf you are interested in accelerated inference, higher volumes of requests, or an SLA, please contact us at `api-enterprise at huggingface.co`.\n\n## How can I see my usage?\n\nYou can head to the [Inference API dashboard](https://api-inference.huggingface.co/dashboard/). Learn more about it in the [Inference API documentation](https://huggingface.co/docs/api-inference/usage).\n\n## Is there programmatic access to the Inference API?\n\nYes, the `huggingface_hub` library has a client wrapper documented [here](https://huggingface.co/docs/huggingface_hub/how-to-inference)."], "test_settings": "chunk_7000_embeddings_openai-text-embedding-3-small_reader-model_gpt-4", "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response correctly identifies \"Inference Endpoints\" as the service replacing the Paid tier of the Inference API at Hugging Face, which aligns with the information provided in the criteria answer. However, the inclusion of \"[Document 0]\" is unnecessary and does not contribute to the accuracy or factual nature of the response. Despite this minor issue, the core information is accurate and factual."}, {"question": "What architectural feature does SqueezeBERT use instead of fully-connected layers for the Q, K, V, and FFN layers?\n", "true_answer": "Grouped convolutions", "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/squeezebert.md", "generated_answer": "SqueezeBERT uses grouped convolutions instead of fully-connected layers for the Q, K, V, and FFN layers [\ubb38\uc11c 0].", "retrieved_docs": ["!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# SqueezeBERT\n\n## Overview\n\nThe SqueezeBERT model was proposed in [SqueezeBERT: What can computer vision teach NLP about efficient neural networks?](https://arxiv.org/abs/2006.11316) by Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, Kurt W. Keutzer. It's a\nbidirectional transformer similar to the BERT model. The key difference between the BERT architecture and the\nSqueezeBERT architecture is that SqueezeBERT uses [grouped convolutions](https://blog.yani.io/filter-group-tutorial)\ninstead of fully-connected layers for the Q, K, V and FFN layers.\n\nThe abstract from the paper is the following:\n\n*Humans read and write hundreds of billions of messages every day. Further, due to the availability of large datasets,\nlarge computing systems, and better neural network models, natural language processing (NLP) technology has made\nsignificant strides in understanding, proofreading, and organizing these messages. Thus, there is a significant\nopportunity to deploy NLP in myriad applications to help web users, social networks, and businesses. In particular, we\nconsider smartphones and other mobile devices as crucial platforms for deploying NLP models at scale. However, today's\nhighly-accurate NLP neural network models such as BERT and RoBERTa are extremely computationally expensive, with\nBERT-base taking 1.7 seconds to classify a text snippet on a Pixel 3 smartphone. In this work, we observe that methods\nsuch as grouped convolutions have yielded significant speedups for computer vision networks, but many of these\ntechniques have not been adopted by NLP neural network designers. We demonstrate how to replace several operations in\nself-attention layers with grouped convolutions, and we use this technique in a novel network architecture called\nSqueezeBERT, which runs 4.3x faster than BERT-base on the Pixel 3 while achieving competitive accuracy on the GLUE test\nset. The SqueezeBERT code will be released.*\n\nThis model was contributed by [forresti](https://huggingface.co/forresti).\n\n## Usage tips\n\n- SqueezeBERT is a model with absolute position embeddings so it's usually advised to pad the inputs on the right\n  rather than the left.\n- SqueezeBERT is similar to BERT and therefore relies on the masked language modeling (MLM) objective. It is therefore\n  efficient at predicting masked tokens and at NLU in general, but is not optimal for text generation. Models trained\n  with a causal language modeling (CLM) objective are better in that regard.\n- For best results when finetuning on sequence classification tasks, it is recommended to start with the\n  *squeezebert/squeezebert-mnli-headless* checkpoint.\n\n## Resources\n\n- [Text classification task guide](../tasks/sequence_classification)\n- [Token classification task guide](../tasks/token_classification)\n- [Question answering task guide](../tasks/question_answering)\n- [Masked language modeling task guide](../tasks/masked_language_modeling)\n- [Multiple choice task guide](../tasks/multiple_choice)\n\n## SqueezeBertConfig\n\n[[autodoc]] SqueezeBertConfig\n\n## SqueezeBertTokenizer\n\n[[autodoc]] SqueezeBertTokenizer\n    - build_inputs_with_special_tokens\n    - get_special_tokens_mask\n    - create_token_type_ids_from_sequences\n    - save_vocabulary\n\n## SqueezeBertTokenizerFast\n\n[[autodoc]] SqueezeBertTokenizerFast\n\n## SqueezeBertModel\n\n[[autodoc]] SqueezeBertModel\n\n## SqueezeBertForMaskedLM\n\n[[autodoc]] SqueezeBertForMaskedLM\n\n## SqueezeBertForSequenceClassification\n\n[[autodoc]] SqueezeBertForSequenceClassification\n\n## SqueezeBertForMultipleChoice\n\n[[autodoc]] SqueezeBertForMultipleChoice\n\n## SqueezeBertForTokenClassification\n\n[[autodoc]] SqueezeBertForTokenClassification\n\n## SqueezeBertForQuestionAnswering\n\n[[autodoc]] SqueezeBertForQuestionAnswering", "**Fun Fact:** Masking has been around a long time - [1953 Paper on Cloze procedure (or \u2018Masking\u2019)](https://psycnet.apa.org/record/1955-00850-001). \n\n### 2.3 What is Next Sentence Prediction?\n\nNSP (Next Sentence Prediction) is used to help BERT learn about relationships between sentences by predicting if a given sentence follows the previous sentence or not. \n\n**Next Sentence Prediction Example:**\n\n1. Paul went shopping. He bought a new shirt. (correct sentence pair)\n2. Ramona made coffee. Vanilla ice cream cones for sale. (incorrect sentence pair)\n\nIn training, 50% correct sentence pairs are mixed in with 50% random sentence pairs to help BERT increase next sentence prediction accuracy.\n\n**Fun Fact:** BERT is trained on both MLM (50%) and NSP (50%) at the same time.\n\n### 2.4 Transformers\n\nThe Transformer architecture makes it possible to parallelize ML training extremely efficiently. Massive parallelization thus makes it feasible to train BERT on large amounts of data in a relatively short period of time. \n\nTransformers use an attention mechanism to observe relationships between words. A concept originally proposed in the popular [2017 Attention Is All You Need](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) paper sparked the use of Transformers in NLP models all around the world.\n\n\n<p align=\"center\">\n\n>Since their introduction in 2017, Transformers have rapidly become the state-of-the-art approach to tackle tasks in many domains such as natural language processing, speech recognition, and computer vision. In short, if you\u2019re doing deep learning, then you need Transformers!\n\n<p class=\"text-center px-6\">Lewis Tunstall, Hugging Face ML Engineer & <a href=\"https://www.amazon.com/Natural-Language-Processing-Transformers-Applications/dp/1098103246)\">Author of Natural Language Processing with Transformers</a></p>\n</p>\n\nTimeline of popular Transformer model releases:\n\n<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Transformer model timeline\" src=\"assets/52_bert_101/transformers-timeline.png\"></medium-zoom>\n  <figcaption><a href=\"https://huggingface.co/course/chapter1/4\">Source</a></figcaption>\n</figure>\n\n#### 2.4.1 How do Transformers work?\n\nTransformers work by leveraging attention, a powerful deep-learning algorithm, first seen in computer vision models.\n\n\u2014Not all that different from how we humans process information through attention. We are incredibly good at forgetting/ignoring mundane daily inputs that don\u2019t pose a threat or require a response from us. For example, can you remember everything you saw and heard coming home last Tuesday? Of course not! Our brain\u2019s memory is limited and valuable. Our recall is aided by our ability to forget trivial inputs. \n\nSimilarly, Machine Learning models need to learn how to pay attention only to the things that matter and not waste computational resources processing irrelevant information. Transformers create differential weights signaling which words in a sentence are the most critical to further process.\n\n<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Encoder and Decoder\" src=\"assets/52_bert_101/encoder-and-decoder-transformers-blocks.png\"></medium-zoom>\n</figure>\n\nA transformer does this by successively processing an input through a stack of transformer layers, usually called the encoder. If necessary, another stack of transformer layers - the decoder - can be used to predict a target output. \u2014BERT however, doesn\u2019t use a decoder. Transformers are uniquely suited for unsupervised learning because they can efficiently process millions of data points.\n\nFun Fact: Google has been using your reCAPTCHA selections to label training data since 2011. The entire Google Books archive and 13 million articles from the New York Times catalog have been transcribed/digitized via people entering reCAPTCHA text. Now, reCAPTCHA is asking us to label Google Street View images, vehicles, stoplights, airplanes, etc. Would be neat if Google made us aware of our participation in this effort (as the training data likely has future commercial intent) but I digress..\n\n<p class=\"text-center\">\n    To learn more about Transformers check out our <a href=\"https://huggingface.co/course/chapter1/1\">Hugging Face Transformers Course</a>.\n</p>\n\n## 3. BERT model size & architecture\n\nLet\u2019s break down the architecture for the two original BERT models:\n\n<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Original BERT models architecture\" src=\"assets/52_bert_101/BERT-size-and-architecture.png\"></medium-zoom>\n</figure>\n\n\nML Architecture Glossary:\n\n| ML Architecture Parts | Definition                                                                                                                                                        |\n|-----------------------|-----------------------------------------------------------...", "(query): Linear(in_features=768, out_features=768, bias=True)\n                    (key): Linear(in_features=768, out_features=768, bias=True)\n                    (value): Linear(in_features=768, out_features=768, bias=True)\n                    (dropout): Dropout(p=0.1, inplace=False)\n                  )\n                  (output): BertSelfOutput(\n                    (dense): Linear(in_features=768, out_features=768, bias=True)\n                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                    (dropout): Dropout(p=0.1, inplace=False)\n                  )\n                )\n                (intermediate): BertIntermediate(\n                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n                )\n                (output): BertOutput(\n                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n              )\n            )\n          )\n        )\n        (cls): BertOnlyMLMHead(\n          (predictions): BertLMPredictionHead(\n            (transform): BertPredictionHeadTransform(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            )\n            (decoder): Linear(in_features=768, out_features=30522, bias=True)\n          )\n        )\n      )\n    )\n```"], "test_settings": "chunk_7000_embeddings_openai-text-embedding-3-small_reader-model_gpt-4", "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response accurately identifies the architectural feature used by SqueezeBERT, which is grouped convolutions, aligning with the criterion answer. The additional reference \"[\ubb38\uc11c 0]\" is extraneous but does not detract from the correctness of the response."}, {"question": "What type of license is the HuggingFace Team's software distributed under?\n", "true_answer": "Apache License, Version 2.0", "source_doc": "huggingface/simulate/blob/main/docs/source/tutorials/interaction.mdx", "generated_answer": "The HuggingFace Team's software is distributed under the Apache License, Version 2.0 [\ubb38\uc11c 0].", "retrieved_docs": ["!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Under construction", "Licenses\n\nYou are able to add a license to any repo that you create on the Hugging Face Hub to let other users know about the permissions that you want to attribute to your code or data. The license can be specified in your repository's `README.md` file, known as a *card* on the Hub, in the card's metadata section. Remember to seek out and respect a project's license if you're considering using their code or data.\n\nA full list of the available licenses is available here:\n\n<!-- IMPORTANT: do not remove or alter the \"region\" comments below -->\n\n<!-- region licenses -->\nFullname | License identifier (to use in model card)\n--- | ---\nApache license 2.0 | `apache-2.0`\nMIT | `mit`\nOpenRAIL license family | `openrail`\nBigScience OpenRAIL-M | `bigscience-openrail-m`\nCreativeML OpenRAIL-M | `creativeml-openrail-m`\nBigScience BLOOM RAIL 1.0 | `bigscience-bloom-rail-1.0`\nBigCode Open RAIL-M v1 | `bigcode-openrail-m`\nAcademic Free License v3.0 | `afl-3.0`\nArtistic license 2.0 | `artistic-2.0`\nBoost Software License 1.0 | `bsl-1.0`\nBSD license family | `bsd`\nBSD 2-clause \"Simplified\" license | `bsd-2-clause`\nBSD 3-clause \"New\" or \"Revised\" license | `bsd-3-clause`\nBSD 3-clause Clear license | `bsd-3-clause-clear`\nComputational Use of Data Agreement | `c-uda`\nCreative Commons license family | `cc`\nCreative Commons Zero v1.0 Universal | `cc0-1.0`\nCreative Commons Attribution 2.0 | `cc-by-2.0`\nCreative Commons Attribution 2.5 | `cc-by-2.5`\nCreative Commons Attribution 3.0 | `cc-by-3.0`\nCreative Commons Attribution 4.0 | `cc-by-4.0`\nCreative Commons Attribution Share Alike 3.0 | `cc-by-sa-3.0`\nCreative Commons Attribution Share Alike 4.0 | `cc-by-sa-4.0`\nCreative Commons Attribution Non Commercial 2.0 | `cc-by-nc-2.0`\nCreative Commons Attribution Non Commercial 3.0 | `cc-by-nc-3.0`\nCreative Commons Attribution Non Commercial 4.0 | `cc-by-nc-4.0`\nCreative Commons Attribution No Derivatives 4.0 | `cc-by-nd-4.0`\nCreative Commons Attribution Non Commercial No Derivatives 3.0 | `cc-by-nc-nd-3.0`\nCreative Commons Attribution Non Commercial No Derivatives 4.0 | `cc-by-nc-nd-4.0`\nCreative Commons Attribution Non Commercial Share Alike 2.0 | `cc-by-nc-sa-2.0`\nCreative Commons Attribution Non Commercial Share Alike 3.0 | `cc-by-nc-sa-3.0`\nCreative Commons Attribution Non Commercial Share Alike 4.0 | `cc-by-nc-sa-4.0`\nCommunity Data License Agreement \u2013 Sharing, Version 1.0 | `cdla-sharing-1.0`\nCommunity Data License Agreement \u2013 Permissive, Version 1.0 | `cdla-permissive-1.0`\nCommunity Data License Agreement \u2013 Permissive, Version 2.0 | `cdla-permissive-2.0`\nDo What The F*ck You Want To Public License | `wtfpl`\nEducational Community License v2.0 | `ecl-2.0`\nEclipse Public License 1.0 | `epl-1.0`\nEclipse Public License 2.0 | `epl-2.0`\nEtalab Open License 2.0 | `etalab-2.0`\nEuropean Union Public License 1.1 | `eupl-1.1`\nGNU Affero General Public License v3.0 | `agpl-3.0`\nGNU Free Documentation License family | `gfdl`\nGNU General Public License family | `gpl`\nGNU General Public License v2.0 | `gpl-2.0`\nGNU General Public License v3.0 | `gpl-3.0`\nGNU Lesser General Public License family | `lgpl`\nGNU Lesser General Public License v2.1 | `lgpl-2.1`\nGNU Lesser General Public License v3.0 | `lgpl-3.0`\nISC | `isc`\nLaTeX Project Public License v1.3c | `lppl-1.3c`\nMicrosoft Public License | `ms-pl`\nMozilla Public License 2.0 | `mpl-2.0`\nOpen Data Commons License Attribution family | `odc-by`\nOpen Database License family | `odbl`\nOpen Rail++-M License | `openrail++`\nOpen Software License 3.0 | `osl-3.0`\nPostgreSQL License | `postgresql`\nSIL Open Font License 1.1 | `ofl-1.1`\nUniversity of Illinois/NCSA Open Source License | `ncsa`\nThe Unlicense | `unlicense`\nzLib License | `zlib`\nOpen Data Commons Public Domain Dedication and License | `pddl`\nLesser General Public License For Linguistic Resources | `lgpl-lr`\nDeepFloyd IF Research License Agreement | `deepfloyd-if-license`\nLlama 2 Community License Agreement | `llama2`\nUnknown | `unknown`\nOther | `other`\n<!-- endregion -->\n\nIn case of `license: other` please add the license's text to a `LICENSE` file inside your repo (or contact us to add the license you use to this list), and set a name for it in `license_name`.", "|status|pr_url                                                                      |model_id|nb_downloads|nb_likes                 |suggested_license_name                                                                        |suggested_license_link                                                                        |license_url                                                                                        |license_url_2                                                                                                        |license_url_3                                                                      |\n|------|----------------------------------------------------------------------------|--------|------------|-------------------------|----------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------|\n|Opened|[Hub PR](https://huggingface.co/decapoda-research/llama-7b-hf/discussions/130)|[decapoda-research/llama-7b-hf](https://huggingface.co/decapoda-research/llama-7b-hf)|379224      |1223                     |llama-license                                                                                 |https://huggingface.co/decapoda-research/llama-7b-hf/blob/main/LICENSE                        |[LICENSE](https://huggingface.co/decapoda-research/llama-7b-hf/blob/main/LICENSE)                  |                                                                                                                     |                                                                                   |\n|Opened|[HUB_PR](https://huggingface.co/huggyllama/llama-7b/discussions/7)|[huggyllama/llama-7b](https://huggingface.co/huggyllama/llama-7b)|319220      |200                      |llama-license                                                                                 |https://huggingface.co/huggyllama/llama-7b/blob/main/LICENSE                                  |[LICENSE](https://huggingface.co/huggyllama/llama-7b/blob/main/LICENSE)                            |                                                                                                                     |                                                                                   |\n|      |                                                                            |[TheBloke/MythoMax-L2-13B-GPTQ](https://huggingface.co/TheBloke/MythoMax-L2-13B-GPTQ)|183096      |69                       |llama-2-community-license                                                                     |https://ai.meta.com/llama/license/                                                            |[LICENSE.txt](https://huggingface.co/TheBloke/MythoMax-L2-13B-GPTQ/blob/main/LICENSE.txt)          |                                                                                                                     |                                                                                   |\n|Opened|[hub_pr](https://huggingface.co/elinas/chronos-13b-v2/discussions/3) |[elinas/chronos-13b-v2](https://huggingface.co/elinas/chronos-13b-v2)|35941       |16                       |llama-2-community-license                                                                     |https://ai.meta.com/llama/license/                                                            |[LICENSE.txt](https://huggingface.co/elinas/chronos-13b-v2/blob/main/LICENSE.txt)                  |                                                                                                                     |                                                                                   |\n|Opened|[HUB_PR](https://huggingface.co/huggyllama/llama-65b/discussions/1)                                                                            |[huggyllama/llama-65b](https://huggingface.co/huggyllama/llama-65b)|25299       |64                       |llama-license                                                                                 |https://huggingface.co/huggyllama/llama-65b/blob/main/LICENSE                                 |[LICENSE](https://huggingface.co/huggyllama/llama-65b/blob/main/LICENSE)                           |                                                                                                                     |                                                                                   |\n|Opened|[HUB_PR](https://huggingface.co/TheBloke/OpenAssistant-Llama2-13B-Orca-v2-8K-3166-GPTQ/discussions/3)                                                                            |[TheBloke/OpenAssistant-Llama2-13B-Orca-v2-8K-316..."], "test_settings": "chunk_7000_embeddings_openai-text-embedding-3-small_reader-model_gpt-4", "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response correctly identifies the type of license under which the HuggingFace Team's software is distributed, which is the Apache License, Version 2.0. The additional reference \"[\ubb38\uc11c 0]\" is unnecessary and does not contribute to the accuracy or completeness of the response. However, it does not detract from the correctness of the main information provided. The response is factually accurate and aligns with the criteria for a high-quality answer."}, {"question": "What are the two parameter-reduction techniques proposed in the ALBERT model to lower memory consumption and increase training speed?\n", "true_answer": "Splitting the embedding matrix into two smaller matrices and using repeating layers split among groups.", "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/albert.md", "generated_answer": "The two parameter-reduction techniques proposed in the ALBERT model are:\n\n1. Splitting the embedding matrix into two smaller matrices.\n2. Using repeating layers split among groups. [\ubb38\uc11c 0]", "retrieved_docs": ["!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# ALBERT\n\n<div class=\"flex flex-wrap space-x-1\">\n<a href=\"https://huggingface.co/models?filter=albert\">\n<img alt=\"Models\" src=\"https://img.shields.io/badge/All_model_pages-albert-blueviolet\">\n</a>\n<a href=\"https://huggingface.co/spaces/docs-demos/albert-base-v2\">\n<img alt=\"Spaces\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue\">\n</a>\n</div>\n\n## Overview\n\nThe ALBERT model was proposed in [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942) by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma,\nRadu Soricut. It presents two parameter-reduction techniques to lower memory consumption and increase the training\nspeed of BERT:\n\n- Splitting the embedding matrix into two smaller matrices.\n- Using repeating layers split among groups.\n\nThe abstract from the paper is the following:\n\n*Increasing model size when pretraining natural language representations often results in improved performance on\ndownstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations,\nlonger training times, and unexpected model degradation. To address these problems, we present two parameter-reduction\ntechniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows\nthat our proposed methods lead to models that scale much better compared to the original BERT. We also use a\nself-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks\nwith multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and\nSQuAD benchmarks while having fewer parameters compared to BERT-large.*\n\nThis model was contributed by [lysandre](https://huggingface.co/lysandre). This model jax version was contributed by\n[kamalkraj](https://huggingface.co/kamalkraj). The original code can be found [here](https://github.com/google-research/ALBERT).\n\n## Usage tips\n\n- ALBERT is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather\n  than the left.\n- ALBERT uses repeating layers which results in a small memory footprint, however the computational cost remains\n  similar to a BERT-like architecture with the same number of hidden layers as it has to iterate through the same\n  number of (repeating) layers.\n- Embedding size E is different from hidden size H justified because the embeddings are context independent (one embedding vector represents one token), whereas hidden states are context dependent (one hidden state represents a sequence of tokens) so it's more logical to have H >> E. Also, the embedding matrix is large since it's V x E (V being the vocab size). If E < H, it has less parameters.\n- Layers are split in groups that share parameters (to save memory).\nNext sentence prediction is replaced by a sentence ordering prediction: in the inputs, we have two sentences A and B (that are consecutive) and we either feed A followed by B or B followed by A. The model must predict if they have been swapped or not.\n\n\n\nThis model was contributed by [lysandre](https://huggingface.co/lysandre). This model jax version was contributed by\n[kamalkraj](https://huggingface.co/kamalkraj). The original code can be found [here](https://github.com/google-research/ALBERT).\n\n\n## Resources\n\n\nThe resources provided in the following sections consist of a list of official Hugging Face and community (indicated by \ud83c\udf0e) resources to help you get started with AlBERT. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n\n<PipelineTag pipeline=\"text-classification\"/>\n\n\n- [`AlbertForSequenceClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification).\n\n\n- [`TFAlbertForSequenceClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/text-classification).\n\n- [`FlaxAlbertForSequenceClassific...", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n-->\n# Optimizing LLMs for Speed and Memory\n\n[[open-in-colab]]\n\nLarge Language Models (LLMs) such as GPT3/4, [Falcon](https://huggingface.co/tiiuae/falcon-40b), and [Llama](https://huggingface.co/meta-llama/Llama-2-70b-hf) are rapidly advancing in their ability to tackle human-centric tasks, establishing themselves as essential tools in modern knowledge-based industries.\nDeploying these models in real-world tasks remains challenging, however:\n\n-   To exhibit near-human text understanding and generation capabilities, LLMs currently require to be composed of billions of parameters (see [Kaplan et al](https://arxiv.org/abs/2001.08361), [Wei et. al](https://arxiv.org/abs/2206.07682)). This consequently amplifies the memory demands for inference.\n-   In many real-world tasks, LLMs need to be given extensive contextual information. This necessitates the model's capability to manage very long input sequences during inference.\n\nThe crux of these challenges lies in augmenting the computational and memory capabilities of LLMs, especially when handling expansive input sequences.\n\nIn this guide, we will go over the effective techniques for efficient LLM deployment:\n\n1.  **Lower Precision:** Research has shown that operating at reduced numerical precision, namely [8-bit and 4-bit](./main_classes/quantization.md) can achieve computational advantages without a considerable decline in model performance.\n\n2.  **Flash Attention:** Flash Attention is a variation of the attention algorithm that not only provides a more memory-efficient approach but also realizes increased efficiency due to optimized GPU memory utilization.\n\n3.  **Architectural Innovations:** Considering that LLMs are always deployed in the same way during inference, namely autoregressive text generation with a long input context, specialized model architectures have been proposed that allow for more efficient inference. The most important advancement in model architectures hereby are [Alibi](https://arxiv.org/abs/2108.12409), [Rotary embeddings](https://arxiv.org/abs/2104.09864), [Multi-Query Attention (MQA)](https://arxiv.org/abs/1911.02150) and [Grouped-Query-Attention (GQA)]((https://arxiv.org/abs/2305.13245)).\n\nThroughout this guide, we will offer an analysis of auto-regressive generation from a tensor's perspective. We delve into the pros and cons of adopting lower precision, provide a comprehensive exploration of the latest attention algorithms, and discuss improved LLM architectures. While doing so, we run practical examples showcasing each of the feature improvements.\n\n## 1. Lower Precision\n\nMemory requirements of LLMs can be best understood by seeing the LLM as a set of weight matrices and vectors and the text inputs as a sequence of vectors. In the following, the definition *weights* will be used to signify all model weight matrices and vectors.\n\nAt the time of writing this guide, LLMs consist of at least a couple billion parameters. Each parameter thereby is made of a decimal number, e.g. `4.5689` which is usually stored in either [float32](https://en.wikipedia.org/wiki/Single-precision_floating-point_format), [bfloat16](https://en.wikipedia.org/wiki/Bfloat16_floating-point_format), or [float16](https://en.wikipedia.org/wiki/Half-precision_floating-point_format) format. This allows us to easily compute the memory requirement to load the LLM into memory:\n\n> *Loading the weights of a model having X billion parameters requires roughly 4 * X GB of VRAM in float32 precision*\n\nNowadays, models are however rarely trained in full float32 precision, but usually in bfloat16 precision or less frequently in float16 precision. Therefore the rule of thumb becomes:\n\n> *Loading the weights of a model having X billion parameters requires roughly 2 * X GB of VRAM in bfloat16/float16 precision*\n\nFor shorter text inputs (less than 1024 tokens), the memory requirement for inference is very much dominated by the memory requirement to load the weights. Therefore, for now, let's assume that the memory requirement for inference is equal to the memory requirement to load the model into the GPU VRAM.\n\nTo give some examples of how much VRAM it roughly takes to load a model in bfloat16:\n\n-   **GPT3** requires 2 \\* 175 GB = **350 GB** VRAM\n-   [**Bloom*...", "--\ntitle: \"Optimizing your LLM in production\"\nthumbnail: /blog/assets/163_optimize_llm/optimize_llm.png\nauthors:\n- user: patrickvonplaten\n---\n\n# Optimizing your LLM in production\n\n\n<a target=\"_blank\" href=\"https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Getting_the_most_out_of_LLMs.ipynb\">\n    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n</a>\n\n***Note***: *This blog post is also available as a documentation page on [Transformers](https://huggingface.co/docs/transformers/llm_tutorial_optimization).*\n\nLarge Language Models (LLMs) such as GPT3/4, [Falcon](https://huggingface.co/tiiuae/falcon-40b), and [LLama](https://huggingface.co/meta-llama/Llama-2-70b-hf) are rapidly advancing in their ability to tackle human-centric tasks, establishing themselves as essential tools in modern knowledge-based industries.\nDeploying these models in real-world tasks remains challenging, however:\n\n-   To exhibit near-human text understanding and generation capabilities, LLMs currently require to be composed of billions of parameters (see [Kaplan et al](https://arxiv.org/abs/2001.08361), [Wei et. al](https://arxiv.org/abs/2206.07682)). This consequently amplifies the memory demands for inference.\n-   In many real-world tasks, LLMs need to be given extensive contextual information. This necessitates the model's capability to manage very long input sequences during inference.\n\nThe crux of these challenges lies in augmenting the computational and memory capabilities of LLMs, especially when handling expansive input sequences.\n\nIn this blog post, we will go over the most effective techniques at the time of writing this blog post to tackle these challenges for efficient LLM deployment:\n\n1.  **Lower Precision**: Research has shown that operating at reduced numerical precision, namely 8-bit and 4-bit, can achieve computational advantages without a considerable decline in model performance.\n\n2.  **Flash Attention:** Flash Attention is a variation of the attention algorithm that not only provides a more memory-efficient approach but also realizes increased efficiency due to optimized GPU memory utilization.\n\n3.  **Architectural Innovations:** Considering that LLMs are always deployed in the same way during inference, namely autoregressive text generation with a long input context, specialized model architectures have been proposed that allow for more efficient inference. The most important advancement in model architectures hereby are [Alibi](https://arxiv.org/abs/2108.12409), [Rotary embeddings](https://arxiv.org/abs/2104.09864), [Multi-Query Attention (MQA)](https://arxiv.org/abs/1911.02150) and [Grouped-Query-Attention (GQA)]((https://arxiv.org/abs/2305.13245)).\n\nThroughout this notebook, we will offer an analysis of auto-regressive generation from a tensor's perspective. We delve into the pros and cons of adopting lower precision, provide a comprehensive exploration of the latest attention algorithms, and discuss improved LLM architectures. While doing so, we run practical examples showcasing each of the feature improvements.\n\n## 1. Harnessing the Power of Lower Precision\n\nMemory requirements of LLMs can be best understood by seeing the LLM as a set of weight matrices and vectors and the text inputs as a sequence of vectors. In the following, the definition *weights* will be used to signify all model weight matrices and vectors.\n\nAt the time of writing this post, LLMs consist of at least a couple billion parameters. Each parameter thereby is made of a decimal number, e.g. `4.5689` which is usually stored in either [float32](https://en.wikipedia.org/wiki/Single-precision_floating-point_format), [bfloat16](https://en.wikipedia.org/wiki/Bfloat16_floating-point_format), or [float16](https://en.wikipedia.org/wiki/Half-precision_floating-point_format) format. This allows us to easily compute the memory requirement to load the LLM into memory:\n\n> *Loading the weights of a model having X billion parameters requires roughly 4 * X GB of VRAM in float32 precision*\n\nNowadays, models are however rarely trained in full float32 precision, but usually in bfloat16 precision or less frequently in float16 precision. Therefore the rule of thumb becomes:\n\n> *Loading the weights of a model having X billion parameters requires roughly 2 * X GB of VRAM in bfloat16/float16 precision*\n\nFor shorter text inputs (less than 1024 tokens), the memory requirement for inference is very much dominated by the memory requirement to load the weights. Therefore, for now, let's assume that the memory requirement for inference is equal to the memory requirement to load the model into the GPU VRAM.\n\nTo give some examples of how much VRAM it roughly takes to load a model in bfloat16:\n\n-   **GPT3** requires 2 \\* 175 GB = **350 GB** VRAM\n-   [**Bloom**](https://huggingface.co/bigscience/bloom) requires 2 \\* 176 GB = **352 GB** VRAM\n-   [**Llama-2-70b**](https://huggingface.co/meta-llama/Llama-2-70b-hf) ..."], "test_settings": "chunk_7000_embeddings_openai-text-embedding-3-small_reader-model_gpt-4", "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response correctly identifies the two parameter-reduction techniques proposed in the ALBERT model: splitting the embedding matrix into two smaller matrices and using repeating layers split among groups. The response is accurate and aligns with the criteria for a 5-point answer. However, the inclusion of \"[\ubb38\uc11c 0]\" is unnecessary and could be seen as a formatting error, but it does not detract from the correctness of the content. Overall, the response is factually correct and precise."}, {"question": "What are the three main steps for fine-tuning a model with the \ud83e\udd17 Datasets library?\n", "true_answer": "1. Load a dataset from the Hugging Face Hub. 2. Preprocess the data with `Dataset.map()`. 3. Load and compute metrics.", "source_doc": "huggingface/course/blob/main/chapters/en/chapter5/1.mdx", "generated_answer": "The three main steps for fine-tuning a model with the \ud83e\udd17 Datasets library are:\n\n1. Load a dataset from the Hugging Face Hub.\n2. Preprocess the data with `Dataset.map()`.\n3. Load and compute metrics. \n\n[\ubb38\uc11c 0]", "retrieved_docs": ["Introduction[[introduction]]\n\n<CourseFloatingBanner\n    chapter={5}\n    classNames=\"absolute z-10 right-0 top-0\"\n/>\n\nIn [Chapter 3](/course/chapter3) you got your first taste of the \ud83e\udd17 Datasets library and saw that there were three main steps when it came to fine-tuning a model:\n\n1. Load a dataset from the Hugging Face Hub.\n2. Preprocess the data with `Dataset.map()`.\n3. Load and compute metrics.\n\nBut this is just scratching the surface of what \ud83e\udd17 Datasets can do! In this chapter, we will take a deep dive into the library. Along the way, we'll find answers to the following questions:\n\n* What do you do when your dataset is not on the Hub?\n* How can you slice and dice a dataset? (And what if you _really_ need to use Pandas?)\n* What do you do when your dataset is huge and will melt your laptop's RAM?\n* What the heck are \"memory mapping\" and Apache Arrow?\n* How can you create your own dataset and push it to the Hub?\n\nThe techniques you learn here will prepare you for the advanced tokenization and fine-tuning tasks in [Chapter 6](/course/chapter6) and [Chapter 7](/course/chapter7) -- so grab a coffee and let's get started!", "Overview\n\nWelcome to the \ud83e\udd17 Datasets tutorials! These beginner-friendly tutorials will guide you through the fundamentals of working with \ud83e\udd17 Datasets. You'll load and prepare a dataset for training with your machine learning framework of choice. Along the way, you'll learn how to load different dataset configurations and splits, interact with and see what's inside your dataset, preprocess, and share a dataset to the [Hub](https://huggingface.co/datasets).\n\nThe tutorials assume some basic knowledge of Python and a machine learning framework like PyTorch or TensorFlow. If you're already familiar with these, feel free to check out the [quickstart](./quickstart) to see what you can do with \ud83e\udd17 Datasets.\n\n<Tip>\n\nThe tutorials only cover the basic skills you need to use \ud83e\udd17 Datasets. There are many other useful functionalities and applications that aren't discussed here. If you're interested in learning more, take a look at [Chapter 5](https://huggingface.co/course/chapter5/1?fw=pt) of the Hugging Face course.\n\n</Tip>\n\nIf you have any questions about \ud83e\udd17 Datasets, feel free to join and ask the community on our [forum](https://discuss.huggingface.co/c/datasets/10).\n\nLet's get started! \ud83c\udfc1", "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Fine-tune a pretrained model\n\n[[open-in-colab]]\n\nThere are significant benefits to using a pretrained model. It reduces computation costs, your carbon footprint, and allows you to use state-of-the-art models without having to train one from scratch. \ud83e\udd17 Transformers provides access to thousands of pretrained models for a wide range of tasks. When you use a pretrained model, you train it on a dataset specific to your task. This is known as fine-tuning, an incredibly powerful training technique. In this tutorial, you will fine-tune a pretrained model with a deep learning framework of your choice:\n\n* Fine-tune a pretrained model with \ud83e\udd17 Transformers [`Trainer`].\n* Fine-tune a pretrained model in TensorFlow with Keras.\n* Fine-tune a pretrained model in native PyTorch.\n\n<a id='data-processing'></a>\n\n## Prepare a dataset\n\n<Youtube id=\"_BZearw7f0w\"/>\n\nBefore you can fine-tune a pretrained model, download a dataset and prepare it for training. The previous tutorial showed you how to process data for training, and now you get an opportunity to put those skills to the test!\n\nBegin by loading the [Yelp Reviews](https://huggingface.co/datasets/yelp_review_full) dataset:\n\n```py\n>>> from datasets import load_dataset\n\n>>> dataset = load_dataset(\"yelp_review_full\")\n>>> dataset[\"train\"][100]\n{'label': 0,\n 'text': 'My expectations for McDonalds are t rarely high. But for one to still fail so spectacularly...that takes something special!\\\\nThe cashier took my friends\\'s order, then promptly ignored me. I had to force myself in front of a cashier who opened his register to wait on the person BEHIND me. I waited over five minutes for a gigantic order that included precisely one kid\\'s meal. After watching two people who ordered after me be handed their food, I asked where mine was. The manager started yelling at the cashiers for \\\\\"serving off their orders\\\\\" when they didn\\'t have their food. But neither cashier was anywhere near those controls, and the manager was the one serving food to customers and clearing the boards.\\\\nThe manager was rude when giving me my order. She didn\\'t make sure that I had everything ON MY RECEIPT, and never even had the decency to apologize that I felt I was getting poor service.\\\\nI\\'ve eaten at various McDonalds restaurants for over 30 years. I\\'ve worked at more than one location. I expect bad days, bad moods, and the occasional mistake. But I have yet to have a decent experience at this store. It will remain a place I avoid unless someone in my party needs to avoid illness from low blood sugar. Perhaps I should go back to the racially biased service of Steak n Shake instead!'}\n```\n\nAs you now know, you need a tokenizer to process the text and include a padding and truncation strategy to handle any variable sequence lengths. To process your dataset in one step, use \ud83e\udd17 Datasets [`map`](https://huggingface.co/docs/datasets/process#map) method to apply a preprocessing function over the entire dataset:\n\n```py\n>>> from transformers import AutoTokenizer\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n\n\n>>> def tokenize_function(examples):\n...     return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n\n\n>>> tokenized_datasets = dataset.map(tokenize_function, batched=True)\n```\n\nIf you like, you can create a smaller subset of the full dataset to fine-tune on to reduce the time it takes:\n\n```py\n>>> small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n>>> small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n```\n\n<a id='trainer'></a>\n\n## Train\n\nAt this point, you should follow the section corresponding to the framework you want to use. You can use the links\nin the right sidebar to jump to the one you want - and if you want to hide all of the content for a given framework,\njust use the button at the top-right of that framework's block!\n\n<frameworkcontent>\n<pt>\n<Youtube id=\"nvBXf7s7vTI\"/>\n\n## Train with PyTorch Trainer\n\n\ud83e\udd17 Transformers provides a [`Trainer`] class optimized for training \ud83e\udd17 Transformers models, making it easier to start training without manually writing your own training loop. The [`Trainer`] API supports a wide range of training options and features such as..."], "test_settings": "chunk_7000_embeddings_openai-text-embedding-3-small_reader-model_gpt-4", "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response accurately lists the three main steps for fine-tuning a model with the \ud83e\udd17 Datasets library, as outlined in the provided criteria. Each step is clearly stated and matches the expected answer. The response is concise and directly addresses the question without any errors or omissions. Therefore, it meets the criteria for a perfect score."}, {"question": "What is the maximum improvement in throughput achieved by Hugging Face Infinity compared to vanilla transformers?\n", "true_answer": "+800%", "source_doc": "huggingface/blog/blob/main/infinity-cpu-performance.md", "generated_answer": "The maximum improvement in throughput achieved by Hugging Face Infinity compared to vanilla transformers is up to 800% [\ubb38\uc11c 1].", "retrieved_docs": ["--\ntitle: \"Case Study: Millisecond Latency using Hugging Face Infinity and modern CPUs\"\nthumbnail: /blog/assets/46_infinity_cpu_performance/thumbnail.png\nauthors:\n- user: philschmid\n- user: jeffboudier\n- user: mfuntowicz\n---\n# Case Study: Millisecond Latency using Hugging Face Infinity and modern CPUs\n\n\n<script async defer src=\"https://unpkg.com/medium-zoom-element@0/dist/medium-zoom-element.min.js\"></script>\n\n<br>\n<div style=\"background-color: #e6f9e6; padding: 16px 32px; outline: 2px solid; border-radius: 10px;\">\n  December 2022 Update: Infinity is no longer offered by Hugging Face as a commercial inference solution. To deploy and accelerate your models, we recommend the following new solutions:\n\n  * [Inference Endpoints](https://huggingface.co/docs/inference-endpoints/index) to easily deploy models on dedicated infrastructure managed by Hugging Face.\n\n  * Our open-source optimization libraries, [\ud83e\udd17 Optimum Intel](https://huggingface.co/blog/openvino) and [\ud83e\udd17 Optimum ONNX Runtime](https://huggingface.co/docs/optimum/main/en/onnxruntime/overview), to get the highest efficiency out of training and running models for inference.\n\n  * Hugging Face [Expert Acceleration Program](https://huggingface.co/support), a commercial service for Hugging Face experts to work directly with your team to accelerate your Machine Learning roadmap and models.\n</div>\n\n\n## Introduction \n\nTransfer learning has changed Machine Learning by reaching new levels of accuracy from Natural Language Processing (NLP) to Audio and Computer Vision tasks. At Hugging Face, we work hard to make these new complex models and large checkpoints as easily accessible and usable as possible. But while researchers and data scientists have converted to the new world of Transformers, few companies have been able to deploy these large, complex models in production at scale.\n\nThe main bottleneck is the latency of predictions which can make large deployments expensive to run and real-time use cases impractical. Solving this is a difficult engineering challenge for any Machine Learning Engineering team and requires the use of advanced techniques to optimize models all the way down to the hardware.\n\nWith [Hugging Face Infinity](https://huggingface.co/infinity), we offer a containerized solution that makes it easy to deploy low-latency, high-throughput, hardware-accelerated inference pipelines for the most popular Transformer models. Companies can get both the accuracy of Transformers and the efficiency necessary for large volume deployments, all in a simple to use package. In this blog post, we want to share detailed performance results for Infinity running on the latest generation of Intel Xeon CPU, to achieve optimal cost, efficiency, and latency for your Transformer deployments.\n\n\n## What is Hugging Face Infinity\n\nHugging Face Infinity is a containerized solution for customers to deploy end-to-end optimized inference pipelines for State-of-the-Art Transformer models, on any infrastructure.\n\nHugging Face Infinity consists of 2 main services:\n* The Infinity Container is a hardware-optimized inference solution delivered as a Docker container.\n* Infinity Multiverse is a Model Optimization Service through which a Hugging Face Transformer model is optimized for the Target Hardware. Infinity Multiverse is compatible with Infinity Container.\n\nThe Infinity Container is built specifically to run on a Target Hardware architecture and exposes an HTTP /predict endpoint to run inference.\n\n<br>\n<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Product overview\" src=\"assets/46_infinity_cpu_performance/overview.png\"></medium-zoom>\n  <figcaption>Figure 1. Infinity Overview</figcaption>\n</figure>\n<br>\n\nAn Infinity Container is designed to serve 1 Model and 1 Task. A Task corresponds to machine learning tasks as defined in the [Transformers Pipelines documentation](https://huggingface.co/docs/transformers/master/en/main_classes/pipelines). As of the writing of this blog post, supported tasks include feature extraction/document embedding, ranking, sequence classification, and token classification.\n\nYou can find more information about Hugging Face Infinity at [hf.co/infinity](https://huggingface.co/infinity), and if you are interested in testing it for yourself, you can sign up for a free trial at [hf.co/infinity-trial](https://huggingface.co/infinity-trial).\n\n---\n\n## Benchmark \n\nInference performance benchmarks often only measure the execution of the model. In this blog post, and when discussing the performance of Infinity, we always measure the end-to-end pipeline including pre-processing, prediction, post-processing. Please keep this in mind when comparing these results with other latency measurements. \n\n<br>\n<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Pipeline\" src=\"assets/46_infinity_cpu_performance/pipeline.png\"></medium-zoom>\n  <figcaption>Figure 2. Infinity End-to-End P...", "---\n\n## Results\n\nTo run the benchmark, we created an infinity container for the [EC2 C6i instance](https://aws.amazon.com/ec2/instance-types/c6i) (Ice-lake) and optimized a [DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert) model for sequence classification using Infinity Multiverse. \n\nThis ice-lake optimized Infinity Container can achieve up to 34% better latency & throughput compared to existing cascade-lake-based instances, and up to 800% better latency & throughput compared to vanilla transformers running on ice-lake.\n\nThe Benchmark we created consists of 192 different experiments and configurations. We ran experiments for: \n* Physical CPU cores: 1, 2, 4, 8\n* Sequence length: 8, 16, 32, 64, 128, 256, 384, 512\n* Batch_size: 1, 2, 4, 8, 16, 32\n\nIn each experiment, we collect numbers for:\n* Throughput (requests per second)\n* Latency (min, max, avg, p90, p95, p99)\n\nYou can find the full data of the benchmark in this google spreadsheet: [\ud83e\udd17 Infinity: CPU Ice-Lake Benchmark](https://docs.google.com/spreadsheets/d/1GWFb7L967vZtAS1yHhyTOZK1y-ZhdWUFqovv7-73Plg/edit?usp=sharing).\n\nIn this blog post, we will highlight a few results of the benchmark including the best latency and throughput configurations.\n\nIn addition to this, we deployed the [DistilBERT](https://huggingface.co/bhadresh-savani/distilbert-base-uncased-emotion) model we used for the benchmark as an API endpoint on 2 physical cores. You can test it and get a feeling for the performance of Infinity. Below you will find a `curl` command on how to send a request to the hosted endpoint. The API returns a `x-compute-time` HTTP Header, which contains the duration of the end-to-end pipeline.\n\n```bash\ncurl --request POST `-i` \\\n  --url https://infinity.huggingface.co/cpu/distilbert-base-uncased-emotion \\\n  --header 'Content-Type: application/json' \\\n  --data '{\"inputs\":\"I like you. I love you\"}'\n```\n\n### Throughput\n\nBelow you can find the throughput comparison for running infinity on 2 physical cores with batch size 1, compared with vanilla transformers.\n\n<br>\n<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Throughput\" src=\"assets/46_infinity_cpu_performance/throughput.png\"></medium-zoom>\n  <figcaption>Figure 3. Throughput: Infinity vs Transformers</figcaption>\n</figure>\n<br>\n\n\n| Sequence Length | Infinity    | Transformers | improvement |\n|-----------------|-------------|--------------|-------------|\n| 8               | 248 req/sec | 49 req/sec   | +506%       |\n| 16              | 212 req/sec | 50 req/sec   | +424%       |\n| 32              | 150 req/sec | 40 req/sec   | +375%       |\n| 64              | 97 req/sec  | 28 req/sec   | +346%       |\n| 128             | 55 req/sec  | 18 req/sec   | +305%       |\n| 256             | 27 req/sec  | 9 req/sec    | +300%       |\n| 384             | 17 req/sec  | 5 req/sec    | +340%       |\n| 512             | 12 req/sec  | 4 req/sec    | +300%       |\n\n\n### Latency \n\nBelow, you can find the latency results for an experiment running Hugging Face Infinity on 2 Physical Cores with Batch Size 1. It is remarkable to see how robust and constant Infinity is, with minimal deviation for p95, p99, or p100 (max latency). This result is confirmed for other experiments as well in the benchmark.  \n\n<br>\n<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Latency\" src=\"assets/46_infinity_cpu_performance/latency.png\"></medium-zoom>\n  <figcaption>Figure 4. Latency (Batch=1, Physical Cores=2)</figcaption>\n</figure>\n<br>\n\n---\n\n## Conclusion\n\nIn this post, we showed how Hugging Face Infinity performs on the new Intel Ice Lake Xeon CPU. We created a detailed benchmark with over 190 different configurations sharing the results you can expect when using Hugging Face Infinity on CPU, what would be the best configuration to optimize your Infinity Container for latency, and what would be the best configuration to maximize throughput.\n\nHugging Face Infinity can deliver up to 800% higher throughput compared to vanilla transformers, and down to 1-4ms latency for sequence lengths up to 64 tokens.\n\nThe flexibility to optimize transformer models for throughput, latency, or both enables businesses to either reduce the amount of infrastructure cost for the same workload or to enable real-time use cases that were not possible before. \n\nIf you are interested in trying out Hugging Face Infinity sign up for your trial at [hf.co/infinity-trial](https://hf.co/infinity-trial) \n\n\n## Resources\n\n* [Hugging Face Infinity](https://huggingface.co/infinity)\n* [Hugging Face Infinity Trial](https://huggingface.co/infinity-trial)\n* [Amazon EC2 C6i instances](https://aws.amazon.com/ec2/instance-types/c6i) \n* [DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert)\n* [DistilBERT paper](https://arxiv.org/abs/1910.01108)\n* [DistilBERT model](https://huggingface.co/bhadresh-savani/distilbert-base-uncased-emotion)\n* [\ud83e\udd17 Infinity: ...", "--\ntitle: \"How we sped up transformer inference 100x for \ud83e\udd17 API customers\"\nthumbnail: /blog/assets/09_accelerated_inference/thumbnail.png\n---\n\n# How we sped up transformer inference 100x for \ud83e\udd17 API customers\n\n\n\ud83e\udd17 Transformers has become the default library for data scientists all around the world to explore state of the art NLP models and build new NLP features. With over 5,000 pre-trained and fine-tuned models available, in over 250 languages, it is a rich playground, easily accessible whichever framework you are working in.\n\nWhile experimenting with models in \ud83e\udd17 Transformers is easy, deploying these large models into production with maximum performance, and managing them into an architecture that scales with usage is a **hard engineering challenge** for any Machine Learning Engineer. \n\nThis 100x performance gain and built-in scalability is why subscribers of our hosted [Accelerated Inference API](https://huggingface.co/pricing) chose to build their NLP features on top of it. To get to the **last 10x of performance** boost, the optimizations need to be low-level, specific to the model, and to the target hardware.\n\nThis post shares some of our approaches squeezing every drop of compute juice for our customers. \ud83c\udf4b\n\n\n## Getting to the first 10x speedup\n\nThe first leg of the optimization journey is the most accessible, all about using the best combination of techniques offered by the [Hugging Face libraries](https://github.com/huggingface/), independent of the target hardware. \n\nWe use the most efficient methods built into Hugging Face model [pipelines](https://huggingface.co/transformers/main_classes/pipelines.html) to reduce the amount of computation during each forward pass. These methods are specific to the architecture of the model and the target task, for instance for a text-generation task on a GPT architecture, we reduce the dimensionality of the attention matrices computation by focusing on the new attention of the last token in each pass:\n\n-| Naive version                                                                                             | Optimized version                                                                                       |\n-|:---------------------------------------------------------------------------------------------------------:|:-------------------------------------------------------------------------------------------------------:|\n-|![](/blog/assets/09_accelerated_inference/unoptimized_graph.png)|![](/blog/assets/09_accelerated_inference/optimized_graph.png)|\n\nTokenization is often a bottleneck for efficiency during inference. We use the most efficient methods from the [\ud83e\udd17 Tokenizers](https://github.com/huggingface/tokenizers/) library, leveraging the Rust implementation of the model tokenizer in combination with smart caching to get up to 10x speedup for the overall latency.\n\nLeveraging the latest features of the Hugging Face libraries, we achieve a reliable 10x speed up compared to an out-of-box deployment for a given model/hardware pair. As new releases of Transformers and Tokenizers typically ship every month, our API customers do not need to constantly adapt to new optimization opportunities, their models just keep running faster.\n\n## Compilation FTW: the hard to get 10x\nNow this is where it gets really tricky. In order to get the best possible performance we will need to modify the model and compile it targeting the specific hardware for inference. The choice of hardware itself will depend on both the model (size in memory) and the demand profile (request batching). Even when serving predictions from the same model, some API customers may benefit more from Accelerated CPU inference, and others from Accelerated GPU inference, each with different optimization techniques and libraries applied.\n\nOnce the compute platform has been selected for the use case, we can go to work. Here are some CPU-specific techniques that can be applied with a static graph:\n- Optimizing the graph (Removing unused flow)\n- Fusing layers (with specific CPU instructions)\n- Quantizing the operations\n\nUsing out-of-box functions from open source libraries (e.g. \ud83e\udd17 Transformers with [ONNX Runtime](https://github.com/microsoft/onnxruntime)) won\u2019t produce the best results, or could result in a significant loss of accuracy, particularly during quantization. There is no silver bullet, and the best path is different for each model architecture. But diving deep into the Transformers code and ONNX Runtime documentation, the stars can be aligned to achieve another 10x speedup.\n\n## Unfair advantage\n\nThe Transformer architecture was a decisive inflection point for Machine Learning performance, starting with NLP, and over the last 3 years the rate of improvement in Natural Language Understanding and Generation has been steep and accelerating. Another metric which accelerated accordingly, is the average size of the models, from the 110M parameters of BERT to the now 175Bn of GPT-3.\n\nThis trend has introduced dau..."], "test_settings": "chunk_7000_embeddings_openai-text-embedding-3-small_reader-model_gpt-4", "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response provides the correct information regarding the maximum improvement in throughput achieved by Hugging Face Infinity compared to vanilla transformers, which is up to 800%. The inclusion of \"[\ubb38\uc11c 1]\" suggests a reference to a document, which is not necessary for the answer itself but does not detract from the accuracy of the response. The response is clear and aligns with the expected answer format, although it could be more concise by simply stating \"+800%\". Overall, the response is accurate and factual."}, {"question": "What is the command to upload a spaCy pipeline to the Hugging Face Hub?\n", "true_answer": "python -m spacy huggingface-hub push en_ner_fashion-0.0.0-py3-none-any.whl", "source_doc": "huggingface/blog/blob/main/spacy.md", "generated_answer": "The command to upload a spaCy pipeline to the Hugging Face Hub is:\n\n```bash\npython -m spacy huggingface-hub push [whl_path] [--org] [--msg] [--local-repo] [--verbose]\n```\n\nThis command uses the `spacy-huggingface-hub` library to push a packaged model to the Hub. [Refer to Document 1 for details.]", "retrieved_docs": ["--\ntitle: \"Welcome spaCy to the Hugging Face Hub\"\nthumbnail: /blog/assets/23_spacy/thumbnail.png\n\nauthors:\n- user: osanseviero\n- user: ines\n---\n\n# Welcome spaCy to the Hugging Face Hub\n\n\n[spaCy](https://github.com/explosion/spaCy) is a popular library for advanced Natural Language Processing used widely across industry. spaCy makes it easy to use and train pipelines for tasks like named entity recognition, text classification, part of speech tagging and more, and lets you build powerful applications to process and analyze large volumes of text.\n\nHugging Face makes it really easy to share your spaCy pipelines with the community! With a single command, you can upload any pipeline package, with a pretty model card and all required metadata auto-generated for you. The inference API currently supports NER out-of-the-box, and you can try out your pipeline interactively in your browser. You'll also get a live URL for your package that you can `pip install` from anywhere for a smooth path from prototype all the way to production!\n\n### Finding models\n\nOver 60 canonical models can be found in the [spaCy](https://hf.co/spacy) org. These models are from the [latest 3.1 release](https://explosion.ai/blog/spacy-v3-1), so you can try the latest realesed models right now! On top of this, you can find all spaCy models from the community here https://huggingface.co/models?filter=spacy.\n\n\n### Widgets\n\nThis integration includes support for NER widgets, so all models with a NER component will have this out of the box! Coming soon there will be support for text classification and POS.", "Using spaCy at Hugging Face\n\n`spaCy` is a popular library for advanced Natural Language Processing used widely across industry. `spaCy` makes it easy to use and train pipelines for tasks like named entity recognition, text classification, part of speech tagging and more, and lets you build powerful applications to process and analyze large volumes of text.\n\n## Exploring spaCy models in the Hub\n\nThe official models from `spaCy` 3.3 are in the `spaCy` [Organization Page](https://huggingface.co/spacy). Anyone in the community can also share their `spaCy` models, which you can find by filtering at the left of the [models page](https://huggingface.co/models?library=spacy).\n\nAll models on the Hub come up with useful features\n1. An automatically generated model card with label scheme, metrics, components, and more.\n2. An evaluation sections at top right where you can look at the metrics.\n3. Metadata tags that help for discoverability and contain information such as license and language.\n4. An interactive widget you can use to play out with the model directly in the browser\n5. An Inference API that allows to make inference requests.\n\n<div class=\"flex justify-center\">\n<img class=\"block dark:hidden\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-spacy_widget.png\"/>\n<img class=\"hidden dark:block\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-spacy_widget-dark.png\"/>\n</div>\n\n\n## Using existing models\n\nAll `spaCy` models from the Hub can be directly installed using pip install.\n\n```bash\npip install https://huggingface.co/spacy/en_core_web_sm/resolve/main/en_core_web_sm-any-py3-none-any.whl\n```\n\nTo find the link of interest, you can go to a repository with a `spaCy` model. When you open the repository, you can click `Use in spaCy` and you will be given a working snippet that you can use to install and load the model!\n\n<div class=\"flex justify-center\">\n<img class=\"block dark:hidden\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-spacy_snippet.png\"/>\n<img class=\"hidden dark:block\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-spacy_snippet-dark.png\"/>\n</div>\n<div class=\"flex justify-center\">\n<img class=\"block dark:hidden\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-spacy_snippet2.png\"/>\n<img class=\"hidden dark:block\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-spacy_snippet2-dark.png\"/>\n</div>\n\nOnce installed, you can load the model as any spaCy pipeline.\n\n```python\n# Using spacy.load().\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Importing as module.\nimport en_core_web_sm\nnlp = en_core_web_sm.load()\n```\n\n## Sharing your models\n\n### Using the spaCy CLI (recommended)\n\nThe `spacy-huggingface-hub` library extends `spaCy` native CLI so people can easily push their packaged models to the Hub.\n\nYou can install spacy-huggingface-hub from pip:\n\n```bash\npip install spacy-huggingface-hub\n```\n\nYou can then check if the command has been registered successfully\n\n```bash\npython -m spacy huggingface-hub --help\n```\n\nTo push with the CLI, you can use the `huggingface-hub push` command as seen below.\n\n```bash\npython -m spacy huggingface-hub push [whl_path] [--org] [--msg] [--local-repo] [--verbose]\n```\n\n| Argument             | Type         | Description                                                                                                                   |\n| -------------------- | ------------ | ----------------------------------------------------------------------------------------------------------------------------- |\n| `whl_path`           | str / `Path` | The path to the `.whl` file packaged with [`spacy package`](https://spacy.io/api/cli#package).                                |\n| `--org`, `-o`        | str          | Optional name of organization to which the pipeline should be uploaded.                                                       |\n| `--msg`, `-m`        | str          | Commit message to use for update. Defaults to `\"Update spaCy pipeline\"`.                                                      |\n| `--local-repo`, `-l` | str / `Path` | Local path to the model repository (will be created if it doesn't exist). Defaults to `hub` in the current working directory. |\n| `--verbose`, `-V`    | bool         | Output additional info for debugging, e.g. the full generated hub metadata.                                                   |\n\n\nYou can then upload any pipeline packaged with [`spacy package`](https://spacy.io/api/cli#package). Make sure to set `--build wheel` to output a binary .whl file. The uploader will read all metadata from the pipeline package, including the auto-generated pretty `README.md` and the model details available in the `meta.json`.\n\n```bash\nhuggingface-cli login\npython -m spacy package...", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Push files to the Hub\n\n[[open-in-colab]]\n\n\ud83e\udd17 Diffusers provides a [`~diffusers.utils.PushToHubMixin`] for uploading your model, scheduler, or pipeline to the Hub. It is an easy way to store your files on the Hub, and also allows you to share your work with others. Under the hood, the [`~diffusers.utils.PushToHubMixin`]:\n\n1. creates a repository on the Hub\n2. saves your model, scheduler, or pipeline files so they can be reloaded later\n3. uploads folder containing these files to the Hub\n\nThis guide will show you how to use the [`~diffusers.utils.PushToHubMixin`] to upload your files to the Hub.\n\nYou'll need to log in to your Hub account with your access [token](https://huggingface.co/settings/tokens) first:\n\n```py\nfrom huggingface_hub import notebook_login\n\nnotebook_login()\n```\n\n## Models\n\nTo push a model to the Hub, call [`~diffusers.utils.PushToHubMixin.push_to_hub`] and specify the repository id of the model to be stored on the Hub:\n\n```py\nfrom diffusers import ControlNetModel\n\ncontrolnet = ControlNetModel(\n    block_out_channels=(32, 64),\n    layers_per_block=2,\n    in_channels=4,\n    down_block_types=(\"DownBlock2D\", \"CrossAttnDownBlock2D\"),\n    cross_attention_dim=32,\n    conditioning_embedding_out_channels=(16, 32),\n)\ncontrolnet.push_to_hub(\"my-controlnet-model\")\n```\n\nFor models, you can also specify the [*variant*](loading#checkpoint-variants) of the weights to push to the Hub. For example, to push `fp16` weights:\n\n```py\ncontrolnet.push_to_hub(\"my-controlnet-model\", variant=\"fp16\")\n```\n\nThe [`~diffusers.utils.PushToHubMixin.push_to_hub`] function saves the model's `config.json` file and the weights are automatically saved in the `safetensors` format.\n\nNow you can reload the model from your repository on the Hub:\n\n```py\nmodel = ControlNetModel.from_pretrained(\"your-namespace/my-controlnet-model\")\n```\n\n## Scheduler\n\nTo push a scheduler to the Hub, call [`~diffusers.utils.PushToHubMixin.push_to_hub`] and specify the repository id of the scheduler to be stored on the Hub:\n\n```py\nfrom diffusers import DDIMScheduler\n\nscheduler = DDIMScheduler(\n    beta_start=0.00085,\n    beta_end=0.012,\n    beta_schedule=\"scaled_linear\",\n    clip_sample=False,\n    set_alpha_to_one=False,\n)\nscheduler.push_to_hub(\"my-controlnet-scheduler\")\n```\n\nThe [`~diffusers.utils.PushToHubMixin.push_to_hub`] function saves the scheduler's `scheduler_config.json` file to the specified repository.\n\nNow you can reload the scheduler from your repository on the Hub:\n\n```py\nscheduler = DDIMScheduler.from_pretrained(\"your-namepsace/my-controlnet-scheduler\")\n```\n\n## Pipeline\n\nYou can also push an entire pipeline with all it's components to the Hub. For example, initialize the components of a [`StableDiffusionPipeline`] with the parameters you want:\n\n```py\nfrom diffusers import (\n    UNet2DConditionModel,\n    AutoencoderKL,\n    DDIMScheduler,\n    StableDiffusionPipeline,\n)\nfrom transformers import CLIPTextModel, CLIPTextConfig, CLIPTokenizer\n\nunet = UNet2DConditionModel(\n    block_out_channels=(32, 64),\n    layers_per_block=2,\n    sample_size=32,\n    in_channels=4,\n    out_channels=4,\n    down_block_types=(\"DownBlock2D\", \"CrossAttnDownBlock2D\"),\n    up_block_types=(\"CrossAttnUpBlock2D\", \"UpBlock2D\"),\n    cross_attention_dim=32,\n)\n\nscheduler = DDIMScheduler(\n    beta_start=0.00085,\n    beta_end=0.012,\n    beta_schedule=\"scaled_linear\",\n    clip_sample=False,\n    set_alpha_to_one=False,\n)\n\nvae = AutoencoderKL(\n    block_out_channels=[32, 64],\n    in_channels=3,\n    out_channels=3,\n    down_block_types=[\"DownEncoderBlock2D\", \"DownEncoderBlock2D\"],\n    up_block_types=[\"UpDecoderBlock2D\", \"UpDecoderBlock2D\"],\n    latent_channels=4,\n)\n\ntext_encoder_config = CLIPTextConfig(\n    bos_token_id=0,\n    eos_token_id=2,\n    hidden_size=32,\n    intermediate_size=37,\n    layer_norm_eps=1e-05,\n    num_attention_heads=4,\n    num_hidden_layers=5,\n    pad_token_id=1,\n    vocab_size=1000,\n)\ntext_encoder = CLIPTextModel(text_encoder_config)\ntokenizer = CLIPTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-clip\")\n```\n\nPass all of the components to the [`StableDiffusionPipeline`] and call [`~diffusers.utils.PushToHubMixin.push_to_hub`] to push the pipeline to the Hub:\n\n```py\ncomponents = {\n    \"unet\": unet,\n    \"scheduler\": scheduler,\n    \"vae\": vae,\n    \"text_encoder\": text_encoder,\n    \"tokenizer\": tokenizer,\n    \"safety_checker\": None,\n    \"feature_extractor\": None,\n}\n\npipeline = Sta..."], "test_settings": "chunk_7000_embeddings_openai-text-embedding-3-small_reader-model_gpt-4", "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response provides a command that is mostly correct but includes additional options that are not present in the 5-point reference answer. The inclusion of optional flags like `[--org]`, `[--msg]`, `[--local-repo]`, and `[--verbose]` suggests a more flexible command but deviates from the simplicity of the reference answer. The response also mentions the use of the `spacy-huggingface-hub` library, which is not specified in the reference answer. While these additions may be useful, they are not necessary for a basic understanding of the command. Therefore, the response is mostly accurate but not as concise or aligned with the reference answer as it could be."}, {"question": "What is the time and memory complexity of the Nystr\u00f6mformer's approximation of self-attention?\n", "true_answer": "O(n)", "source_doc": "huggingface/blog/blob/main/nystromformer.md", "generated_answer": "The Nystr\u00f6mformer's approximation of self-attention has a time and memory complexity of \\(O(n)\\) [\ubb38\uc11c 0].", "retrieved_docs": ["--\ntitle: \"Nystr\u00f6mformer: Approximating self-attention in linear time and memory via the Nystr\u00f6m method\"\nthumbnail: /blog/assets/86_nystromformer/thumbnail.png\nauthors:\n- user: asi\n  guest: true\n---\n\n# Nystr\u00f6mformer: Approximating self-attention in linear time and memory via the Nystr\u00f6m method\n\n\n<script async defer src=\"https://unpkg.com/medium-zoom-element@0/dist/medium-zoom-element.min.js\"></script>\n\n## Introduction\n\nTransformers have exhibited remarkable performance on various Natural Language Processing and Computer Vision tasks. Their success can be attributed to the self-attention mechanism, which captures the pairwise interactions between all the tokens in an input. However, the standard self-attention mechanism has a time and memory complexity of \\\\(O(n^2)\\\\) (where \\\\(n\\\\) is the length of the input sequence), making it expensive to train on long input sequences. \n\nThe [Nystr\u00f6mformer](https://arxiv.org/abs/2102.03902) is one of many efficient Transformer models that approximates standard self-attention with \\\\(O(n)\\\\) complexity. Nystr\u00f6mformer exhibits competitive performance on various downstream NLP and CV tasks while improving upon the efficiency of standard self-attention. The aim of this blog post is to give readers an overview of the Nystr\u00f6m method and how it can be adapted to approximate self-attention.\n\n\n## Nystr\u00f6m method for matrix approximation\n\nAt the heart of Nystr\u00f6mformer is the Nystr\u00f6m method for matrix approximation. It allows us to approximate a matrix by sampling some of its rows and columns. Let's consider a matrix \\\\(P^{n \\times n}\\\\), which is expensive to compute in its entirety. So, instead, we approximate it using the Nystr\u00f6m method. We start by sampling \\\\(m\\\\) rows and columns from \\\\(P\\\\). We can then arrange the sampled rows and columns as follows:\n\n<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Representing P as a block matrix\" src=\"assets/86_nystromformer/p_block.png\"></medium-zoom>\n  <figcaption>Representing P as a block matrix</figcaption>\n</figure>\n\nWe now have four submatrices: \\\\(A_P, B_P, F_P,\\\\) and \\\\(C_P\\\\), with sizes \\\\(m \\times m, m \\times (n - m), (n - m) \\times m\\\\) and \n\\\\((n - m) \\times (n - m)\\\\) respectively. The \\\\(m\\\\) sampled columns are contained in \\\\(A_P\\\\) and \\\\(F_P\\\\), whereas the \\\\(m\\\\) sampled rows are contained in \\\\(A_P\\\\) and \\\\(B_P\\\\). So, the entries of \\\\(A_P, B_P,\\\\) and \\\\(F_P\\\\) are known to us, and we will estimate \\\\(C_P\\\\). According to the Nystr\u00f6m method, \\\\(C_P\\\\) is given by:\n\n$$C_P = F_P A_P^+ B_P$$\n\nHere, \\\\(+\\\\) denotes the Moore-Penrose inverse (or pseudoinverse). \nThus, the Nystr\u00f6m approximation of \\\\(P, \\hat{P}\\\\) can be written as:\n\n<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Nystr\u00f6m approximation of P\" src=\"assets/86_nystromformer/p_hat.png\"></medium-zoom>\n  <figcaption>Nystr\u00f6m approximation of P</figcaption>\n</figure>\n\nAs shown in the second line, \\\\(\\hat{P}\\\\) can be expressed as a product of three matrices. The reason for doing so will become clear later.\n\n\n## Can we approximate self-attention with the Nystr\u00f6m method?\n\nOur goal is to ultimately approximate the softmax matrix in standard self attention: S = softmax \\\\( \\frac{QK^T}{\\sqrt{d}} \\\\)\n\nHere, \\\\(Q\\\\) and \\\\(K\\\\) denote the queries and keys respectively. Following the procedure discussed above, we would sample \\\\(m\\\\) rows and columns from \\\\(S\\\\), form four submatrices, and obtain \\\\(\\hat{S}\\\\):\n\n<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Nystr\u00f6m approximation of S\" src=\"assets/86_nystromformer/s_hat.png\"></medium-zoom>\n  <figcaption>Nystr\u00f6m approximation of S</figcaption>\n</figure>\n\nBut, what does it mean to sample a column from \\\\(S\\\\)? It means we select one element from each row. Recall how S is calculated: the final operation is a row-wise softmax. To find a single entry in a row, we must access all other entries (for the denominator in softmax). So, sampling one column requires us to know all other columns in the matrix. Therefore, we cannot directly apply the Nystr\u00f6m method to approximate the softmax matrix.\n\n\n## How can we adapt the Nystr\u00f6m method to approximate self-attention?\n\nInstead of sampling from \\\\(S\\\\), the authors propose to sample landmarks (or Nystr\u00f6m points) from queries and keys. We denote the query landmarks and key landmarks as \\\\(\\tilde{Q}\\\\) and \\\\(\\tilde{K}\\\\) respectively. \\\\(\\tilde{Q}\\\\) and \\\\(\\tilde{K}\\\\) can be used to construct three matrices corresponding to those in the Nystr\u00f6m approximation of \\\\(S\\\\). We define the following matrices:\n\n$$\\tilde{F} = softmax(\\frac{Q\\tilde{K}^T}{\\sqrt{d}}) \\hspace{40pt} \\tilde{A} = softmax(\\frac{\\tilde{Q}\\tilde{K}^T}{\\sqrt{d}})^+ \\hspace{40pt} \\tilde{B} = softmax(\\frac{\\tilde{Q}K^T}{\\sqrt{d}})$$\n\nThe sizes of \\\\(\\tilde{F}\\\\), \\\\(\\tilde{A}\\\\), and \\\\(\\tilde{B}) are \\\\(n \\times m, m \\times m,\\\\) and \\\\(m \\times n\\\\) respectively...", "The overall algorithm is summarised by the following figure from the paper:\n\n<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Efficient self-attention with the Nystr\u00f6m method\" src=\"assets/86_nystromformer/paper_figure.png\"></medium-zoom>\n  <figcaption>Efficient self-attention with the Nystr\u00f6m method</figcaption>\n</figure>\n\nThe three orange matrices above correspond to the three matrices we constructed using the key and query landmarks. Also, notice that there is a DConv box. This corresponds to a skip connection added to the values using a 1D depthwise convolution.\n\n\n## How is Nystr\u00f6mformer implemented?\n\nThe original implementation of Nystr\u00f6mformer can be found [here](https://github.com/mlpen/Nystromformer) and the HuggingFace implementation can be found [here](https://github.com/huggingface/transformers/blob/main/src/transformers/models/nystromformer/modeling_nystromformer.py). Let's take a look at a few lines of code (with some comments added) from the HuggingFace implementation. Note that some details such as normalization, attention masking, and depthwise convolution are avoided for simplicity.\n\n```python\n\nkey_layer = self.transpose_for_scores(self.key(hidden_states)) # K\nvalue_layer = self.transpose_for_scores(self.value(hidden_states)) # V\nquery_layer = self.transpose_for_scores(mixed_query_layer) # Q\n\nq_landmarks = query_layer.reshape(\n    -1,\n    self.num_attention_heads,\n    self.num_landmarks,\n    self.seq_len // self.num_landmarks,\n    self.attention_head_size,\n).mean(dim=-2) # \\tilde{Q}\n\nk_landmarks = key_layer.reshape(\n    -1,\n    self.num_attention_heads,\n    self.num_landmarks,\n    self.seq_len // self.num_landmarks,\n    self.attention_head_size,\n).mean(dim=-2) # \\tilde{K}\n\nkernel_1 = torch.nn.functional.softmax(torch.matmul(query_layer, k_landmarks.transpose(-1, -2)), dim=-1) # \\tilde{F}\nkernel_2 = torch.nn.functional.softmax(torch.matmul(q_landmarks, k_landmarks.transpose(-1, -2)), dim=-1) # \\tilde{A} before pseudo-inverse\n\nattention_scores = torch.matmul(q_landmarks, key_layer.transpose(-1, -2)) # \\tilde{B} before softmax\n\nkernel_3 = nn.functional.softmax(attention_scores, dim=-1) # \\tilde{B}\nattention_probs = torch.matmul(kernel_1, self.iterative_inv(kernel_2)) # \\tilde{F} * \\tilde{A}\nnew_value_layer = torch.matmul(kernel_3, value_layer) # \\tilde{B} * V\ncontext_layer = torch.matmul(attention_probs, new_value_layer) # \\tilde{F} * \\tilde{A} * \\tilde{B} * V\n```\n\n\n## Using Nystr\u00f6mformer with HuggingFace\n\nNystr\u00f6mformer for Masked Language Modeling (MLM) is available on HuggingFace. Currently, there are 4 checkpoints, corresponding to various sequence lengths: [`nystromformer-512`](https://huggingface.co/uw-madison/nystromformer-512), [`nystromformer-1024`](https://huggingface.co/uw-madison/nystromformer-1024), [`nystromformer-2048`](https://huggingface.co/uw-madison/nystromformer-2048), and [`nystromformer-4096`](https://huggingface.co/uw-madison/nystromformer-4096). The number of landmarks, \\\\(m\\\\), can be controlled using the `num_landmarks` parameter in the [`NystromformerConfig`](https://huggingface.co/docs/transformers/v4.18.0/en/model_doc/nystromformer#transformers.NystromformerConfig). Let's take a look at a minimal example of Nystr\u00f6mformer for MLM:\n\n```python\nfrom transformers import AutoTokenizer, NystromformerForMaskedLM\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"uw-madison/nystromformer-512\")\nmodel = NystromformerForMaskedLM.from_pretrained(\"uw-madison/nystromformer-512\")\n\ninputs = tokenizer(\"Paris is the [MASK] of France.\", return_tensors=\"pt\")\n\nwith torch.no_grad():\n    logits = model(**inputs).logits\n\n# retrieve index of [MASK]\nmask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n\npredicted_token_id = logits[0, mask_token_index].argmax(axis=-1)\ntokenizer.decode(predicted_token_id)\n```\n\n<div class=\"output stream stdout\">\n\n    Output:\n    ----------------------------------------------------------------------------------------------------\n    capital\n\n</div>\n\nAlternatively, we can use the [pipeline API](https://huggingface.co/docs/transformers/main_classes/pipelines) (which handles all the complexity for us):\n```python\nfrom transformers import pipeline\nunmasker = pipeline('fill-mask', model='uw-madison/nystromformer-512')\nunmasker(\"Paris is the [MASK] of France.\")\n```\n\n<div class=\"output stream stdout\">\n\n    Output:\n    ----------------------------------------------------------------------------------------------------\n    [{'score': 0.829957902431488,\n      'token': 1030,\n      'token_str': 'capital',\n      'sequence': 'paris is the capital of france.'},\n    {'score': 0.022157637402415276,\n      'token': 16081,\n      'token_str': 'birthplace',\n      'sequence': 'paris is the birthplace of france.'},\n    {'score': 0.01904447190463543,\n      'token': 197,\n      'token_str': 'name',\n      'sequence': 'paris is the name of france.'},\n    {'score': 0.017583081498742104,\n      'token...", "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Nystr\u00f6mformer\n\n## Overview\n\nThe Nystr\u00f6mformer model was proposed in [*Nystr\u00f6mformer: A Nystr\u00f6m-Based Algorithm for Approximating Self-Attention*](https://arxiv.org/abs/2102.03902) by Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn\nFung, Yin Li, and Vikas Singh.\n\nThe abstract from the paper is the following:\n\n*Transformers have emerged as a powerful tool for a broad range of natural language processing tasks. A key component\nthat drives the impressive performance of Transformers is the self-attention mechanism that encodes the influence or\ndependence of other tokens on each specific token. While beneficial, the quadratic complexity of self-attention on the\ninput sequence length has limited its application to longer sequences -- a topic being actively studied in the\ncommunity. To address this limitation, we propose Nystr\u00f6mformer -- a model that exhibits favorable scalability as a\nfunction of sequence length. Our idea is based on adapting the Nystr\u00f6m method to approximate standard self-attention\nwith O(n) complexity. The scalability of Nystr\u00f6mformer enables application to longer sequences with thousands of\ntokens. We perform evaluations on multiple downstream tasks on the GLUE benchmark and IMDB reviews with standard\nsequence length, and find that our Nystr\u00f6mformer performs comparably, or in a few cases, even slightly better, than\nstandard self-attention. On longer sequence tasks in the Long Range Arena (LRA) benchmark, Nystr\u00f6mformer performs\nfavorably relative to other efficient self-attention methods. Our code is available at this https URL.*\n\nThis model was contributed by [novice03](https://huggingface.co/novice03). The original code can be found [here](https://github.com/mlpen/Nystromformer).\n\n## Resources\n\n- [Text classification task guide](../tasks/sequence_classification)\n- [Token classification task guide](../tasks/token_classification)\n- [Question answering task guide](../tasks/question_answering)\n- [Masked language modeling task guide](../tasks/masked_language_modeling)\n- [Multiple choice task guide](../tasks/multiple_choice)\n\n## NystromformerConfig\n\n[[autodoc]] NystromformerConfig\n\n## NystromformerModel\n\n[[autodoc]] NystromformerModel\n    - forward\n\n## NystromformerForMaskedLM\n\n[[autodoc]] NystromformerForMaskedLM\n    - forward\n\n## NystromformerForSequenceClassification\n\n[[autodoc]] NystromformerForSequenceClassification\n    - forward\n\n## NystromformerForMultipleChoice\n\n[[autodoc]] NystromformerForMultipleChoice\n    - forward\n\n## NystromformerForTokenClassification\n\n[[autodoc]] NystromformerForTokenClassification\n    - forward\n\n## NystromformerForQuestionAnswering\n\n[[autodoc]] NystromformerForQuestionAnswering\n    - forward"], "test_settings": "chunk_7000_embeddings_openai-text-embedding-3-small_reader-model_gpt-4", "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response correctly identifies the time and memory complexity of the Nystr\u00f6mformer's approximation of self-attention as \\(O(n)\\), which matches the criterion for a 5-point answer. However, the inclusion of \"[\ubb38\uc11c 0]\" is unnecessary and does not contribute to the accuracy or completeness of the response. Despite this minor issue, the core information provided is accurate and aligns with the expected answer. Therefore, the response is mostly correct and factual, but the extraneous text slightly detracts from its precision."}, {"question": "What is the goal of the Named Entity Recognition task in token classification?\n", "true_answer": "The goal of the Named Entity Recognition task is to find the entities in a piece of text, such as person, location, or organization.", "source_doc": "huggingface/course/blob/main/subtitles/en/raw/tasks.md", "generated_answer": "The goal of the Named Entity Recognition (NER) task in token classification is to find and label entities in a sentence, such as persons, locations, or organizations, by assigning a specific label to each token corresponding to these entities [\ubb38\uc11c 0](#).", "retrieved_docs": ["FrameworkSwitchCourse {fw} />\n\n# Token classification[[token-classification]]\n\n{#if fw === 'pt'}\n\n<CourseFloatingBanner chapter={7}\n  classNames=\"absolute z-10 right-0 top-0\"\n  notebooks={[\n    {label: \"Google Colab\", value: \"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter7/section2_pt.ipynb\"},\n    {label: \"Aws Studio\", value: \"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter7/section2_pt.ipynb\"},\n]} />\n\n{:else}\n\n<CourseFloatingBanner chapter={7}\n  classNames=\"absolute z-10 right-0 top-0\"\n  notebooks={[\n    {label: \"Google Colab\", value: \"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter7/section2_tf.ipynb\"},\n    {label: \"Aws Studio\", value: \"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter7/section2_tf.ipynb\"},\n]} />\n\n{/if}\n\nThe first application we'll explore is token classification. This generic task encompasses any problem that can be formulated as \"attributing a label to each token in a sentence,\" such as:\n\n- **Named entity recognition (NER)**: Find the entities (such as persons, locations, or organizations) in a sentence. This can be formulated as attributing a label to each token by having one class per entity and one class for \"no entity.\"\n- **Part-of-speech tagging (POS)**: Mark each word in a sentence as corresponding to a particular part of speech (such as noun, verb, adjective, etc.).\n- **Chunking**: Find the tokens that belong to the same entity. This task (which can be combined with POS or NER) can be formulated as attributing one label (usually `B-`) to any tokens that are at the beginning of a chunk, another label (usually `I-`) to tokens that are inside a chunk, and a third label (usually `O`) to tokens that don't belong to any chunk.\n\n<Youtube id=\"wVHdVlPScxA\"/>\n\nOf course, there are many other types of token classification problem; those are just a few representative examples. In this section, we will fine-tune a model (BERT) on a NER task, which will then be able to compute predictions like this one:\n\n<iframe src=\"https://course-demos-bert-finetuned-ner.hf.space\" frameBorder=\"0\" height=\"350\" title=\"Gradio app\" class=\"block dark:hidden container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n<a class=\"flex justify-center\" href=\"/huggingface-course/bert-finetuned-ner\">\n<img class=\"block dark:hidden lg:w-3/5\" src=\"https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/model-eval-bert-finetuned-ner.png\" alt=\"One-hot encoded labels for question answering.\"/>\n<img class=\"hidden dark:block lg:w-3/5\" src=\"https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/model-eval-bert-finetuned-ner-dark.png\" alt=\"One-hot encoded labels for question answering.\"/>\n</a>\n\nYou can find the model we'll train and upload to the Hub and double-check its predictions [here](https://huggingface.co/huggingface-course/bert-finetuned-ner?text=My+name+is+Sylvain+and+I+work+at+Hugging+Face+in+Brooklyn).\n\n## Preparing the data[[preparing-the-data]]\n\nFirst things first, we need a dataset suitable for token classification. In this section we will use the [CoNLL-2003 dataset](https://huggingface.co/datasets/conll2003), which contains news stories from Reuters. \n\n<Tip>\n\n\ud83d\udca1 As long as your dataset consists of texts split into words with their corresponding labels, you will be able to adapt the data processing procedures described here to your own dataset. Refer back to [Chapter 5](/course/chapter5) if you need a refresher on how to load your own custom data in a `Dataset`.\n\n</Tip>\n\n### The CoNLL-2003 dataset[[the-conll-2003-dataset]]\n\nTo load the CoNLL-2003 dataset, we use the `load_dataset()` method from the \ud83e\udd17 Datasets library:\n\n```py\nfrom datasets import load_dataset\n\nraw_datasets = load_dataset(\"conll2003\")\n```\n\nThis will download and cache the dataset, like we saw in [Chapter 3](/course/chapter3) for the GLUE MRPC dataset. Inspecting this object shows us the columns present and the split between the training, validation, and test sets:\n\n```py\nraw_datasets\n```\n\n```python out\nDatasetDict({\n    train: Dataset({\n        features: ['chunk_tags', 'id', 'ner_tags', 'pos_tags', 'tokens'],\n        num_rows: 14041\n    })\n    validation: Dataset({\n        features: ['chunk_tags', 'id', 'ner_tags', 'pos_tags', 'tokens'],\n        num_rows: 3250\n    })\n    test: Dataset({\n        features: ['chunk_tags', 'id', 'ner_tags', '...", "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Token classification\n\n[[open-in-colab]]\n\n<Youtube id=\"wVHdVlPScxA\"/>\n\nToken classification assigns a label to individual tokens in a sentence. One of the most common token classification tasks is Named Entity Recognition (NER). NER attempts to find a label for each entity in a sentence, such as a person, location, or organization.\n\nThis guide will show you how to:\n\n1. Finetune [DistilBERT](https://huggingface.co/distilbert-base-uncased) on the [WNUT 17](https://huggingface.co/datasets/wnut_17) dataset to detect new entities.\n2. Use your finetuned model for inference.\n\n<Tip>\nThe task illustrated in this tutorial is supported by the following model architectures:\n\n<!--This tip is automatically generated by `make fix-copies`, do not fill manually!-->\n\n[ALBERT](../model_doc/albert), [BERT](../model_doc/bert), [BigBird](../model_doc/big_bird), [BioGpt](../model_doc/biogpt), [BLOOM](../model_doc/bloom), [BROS](../model_doc/bros), [CamemBERT](../model_doc/camembert), [CANINE](../model_doc/canine), [ConvBERT](../model_doc/convbert), [Data2VecText](../model_doc/data2vec-text), [DeBERTa](../model_doc/deberta), [DeBERTa-v2](../model_doc/deberta-v2), [DistilBERT](../model_doc/distilbert), [ELECTRA](../model_doc/electra), [ERNIE](../model_doc/ernie), [ErnieM](../model_doc/ernie_m), [ESM](../model_doc/esm), [Falcon](../model_doc/falcon), [FlauBERT](../model_doc/flaubert), [FNet](../model_doc/fnet), [Funnel Transformer](../model_doc/funnel), [GPT-Sw3](../model_doc/gpt-sw3), [OpenAI GPT-2](../model_doc/gpt2), [GPTBigCode](../model_doc/gpt_bigcode), [GPT Neo](../model_doc/gpt_neo), [GPT NeoX](../model_doc/gpt_neox), [I-BERT](../model_doc/ibert), [LayoutLM](../model_doc/layoutlm), [LayoutLMv2](../model_doc/layoutlmv2), [LayoutLMv3](../model_doc/layoutlmv3), [LiLT](../model_doc/lilt), [Longformer](../model_doc/longformer), [LUKE](../model_doc/luke), [MarkupLM](../model_doc/markuplm), [MEGA](../model_doc/mega), [Megatron-BERT](../model_doc/megatron-bert), [MobileBERT](../model_doc/mobilebert), [MPNet](../model_doc/mpnet), [MPT](../model_doc/mpt), [MRA](../model_doc/mra), [Nezha](../model_doc/nezha), [Nystr\u00f6mformer](../model_doc/nystromformer), [Phi](../model_doc/phi), [QDQBert](../model_doc/qdqbert), [RemBERT](../model_doc/rembert), [RoBERTa](../model_doc/roberta), [RoBERTa-PreLayerNorm](../model_doc/roberta-prelayernorm), [RoCBert](../model_doc/roc_bert), [RoFormer](../model_doc/roformer), [SqueezeBERT](../model_doc/squeezebert), [XLM](../model_doc/xlm), [XLM-RoBERTa](../model_doc/xlm-roberta), [XLM-RoBERTa-XL](../model_doc/xlm-roberta-xl), [XLNet](../model_doc/xlnet), [X-MOD](../model_doc/xmod), [YOSO](../model_doc/yoso)\n\n<!--End of the generated tip-->\n\n</Tip>\n\nBefore you begin, make sure you have all the necessary libraries installed:\n\n```bash\npip install transformers datasets evaluate seqeval\n```\n\nWe encourage you to login to your Hugging Face account so you can upload and share your model with the community. When prompted, enter your token to login:\n\n```py\n>>> from huggingface_hub import notebook_login\n\n>>> notebook_login()\n```\n\n## Load WNUT 17 dataset\n\nStart by loading the WNUT 17 dataset from the \ud83e\udd17 Datasets library:\n\n```py\n>>> from datasets import load_dataset\n\n>>> wnut = load_dataset(\"wnut_17\")\n```\n\nThen take a look at an example:\n\n```py\n>>> wnut[\"train\"][0]\n{'id': '0',\n 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 8, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0],\n 'tokens': ['@paulwalk', 'It', \"'s\", 'the', 'view', 'from', 'where', 'I', \"'m\", 'living', 'for', 'two', 'weeks', '.', 'Empire', 'State', 'Building', '=', 'ESB', '.', 'Pretty', 'bad', 'storm', 'here', 'last', 'evening', '.']\n}\n```\n\nEach number in `ner_tags` represents an entity. Convert the numbers to their label names to find out what the entities are:\n\n```py\n>>> label_list = wnut[\"train\"].features[f\"ner_tags\"].feature.names\n>>> label_list\n[\n    \"O\",\n    \"B-corporation\",\n    \"I-corporation\",\n    \"B-creative-work\",\n    \"I-creative-work\",\n    \"B-group\",\n    \"I-group\",\n    \"B-location\",\n    \"I-location\",\n    \"B-person\",\n    \"I-person\",\n    \"B-product\",\n    \"I-product\",\n]\n```\n\nThe letter that prefixes each `ner_tag` indicates the token position of the entity:\n\n- `B-` indicates the beginning of an entity.\n- `I-` indicates a ...", "et's have a look inside the token classification pipeline. In the pipeline video, we looked at the different applications the Transformers library supports out of the box, one of them being token classification, for instance predicting for each word in a sentence whether they correspond to a person, an organization or a location. We can even group together the tokens corresponding to the same entity, for instance all the tokens that formed the word Sylvain here, or Hugging and Face. The token classification pipeline works the same way as the text classification pipeline we studied in a previous video. There are three steps: the tokenization, the model, and the post processing. The first two steps are identical to the text classification pipeline, except we use an auto token classification model instead of a sequence classification one. We tokenize our text then feed it to the model. Instead of getting one number for each possible label for the whole sentence, we get one number for each of the possible 9 labels for every token in the sentence, here 19. Like all the other models of the Transformers library, our model outputs logits, which we turn into predictions by using a SoftMax. We also get the predicted label for each token by taking the maximum prediction (since the softmax function preserves the order, we could have done it on the logits if we had no need of the predictions). The model config contains the label mapping in its id2label field. Using it, we can map every token to its corresponding label. The label O correspond to \"no entity\", which is why we didn't see it in our results in the first slide. On top of the label and the probability, those results included the start and end character in the sentence. We will need to use the offset mapping of the tokenizer to get those (look at the video linked below if you don't know about them already). Then, looping through each token that has a label distinct from O, we can build the list of results we got with our first pipeline. The last step is to group together tokens that correspond to the same entity. This is why we had two labels for each type of entity: I-PER and B-PER for instance. It allows us to know if a token is in the same entity as the previous one.() Note that there are two ways of labelling used for token classification, one (in pink here) uses the B-PER label at the beginning of each new entity, but the other (in blue) only uses it to separate two adjacent entities of the same type. In both cases, we can flag a new entity each time we see a new label appearing (either with the I or B prefix) then take all the following tokens labelled the same, with an I-flag. This, coupled with the offset mapping to get the start and end characters allows us to get the span of texts for each entity."], "test_settings": "chunk_7000_embeddings_openai-text-embedding-3-small_reader-model_gpt-4", "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response is entirely correct, accurate, and factual, providing a detailed explanation of the NER task in token classification."}, {"question": "What is the resolution of images used by the CLIPSeg model?\n", "true_answer": "352 x 352 pixels", "source_doc": "huggingface/blog/blob/main/clipseg-zero-shot.md", "generated_answer": "The resolution of images used by the CLIPSeg model is 352 x 352 pixels [\ubb38\uc11c 0].", "retrieved_docs": ["--\ntitle: Zero-shot image segmentation with CLIPSeg\nthumbnail: /blog/assets/123_clipseg-zero-shot/thumb.png\nauthors:\n- user: segments-tobias\n  guest: true\n- user: nielsr\n---\n\n# Zero-shot image segmentation with CLIPSeg\n\n\n<script async defer src=\"https://unpkg.com/medium-zoom-element@0/dist/medium-zoom-element.min.js\"></script>\n\n<a target=\"_blank\" href=\"https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/123_clipseg-zero-shot.ipynb\">\n    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n</a>\n\n**This guide shows how you can use [CLIPSeg](https://huggingface.co/docs/transformers/main/en/model_doc/clipseg), a zero-shot image segmentation model, using [`\ud83e\udd17 transformers`](https://huggingface.co/transformers). CLIPSeg creates rough segmentation masks that can be used for robot perception, image inpainting, and many other tasks. If you need more precise segmentation masks, we\u2019ll show how you can refine the results of CLIPSeg on [Segments.ai](https://segments.ai/?utm_source=hf&utm_medium=blog&utm_campaign=clipseg).**\n\nImage segmentation is a well-known task within the field of computer vision. It allows a computer to not only know what is in an image (classification), where objects are in the image (detection), but also what the outlines of those objects are. Knowing the outlines of objects is essential in fields such as robotics and autonomous driving. For example, a robot has to know the shape of an object to grab it correctly. Segmentation can also be combined with [image inpainting](https://t.co/5q8YHSOfx7) to allow users to describe which part of the image they want to replace.\n\nOne limitation of most image segmentation models is that they only work with a fixed list of categories. For example, you cannot simply use a segmentation model trained on oranges to segment apples. To teach the segmentation model an additional category, you have to label data of the new category and train a new model, which can be costly and time-consuming. But what if there was a model that can already segment almost any kind of object, without any further training? That\u2019s exactly what [CLIPSeg](https://arxiv.org/abs/2112.10003), a zero-shot segmentation model, achieves.\n\nCurrently, CLIPSeg still has its limitations. For example, the model uses images of 352 x 352 pixels, so the output is quite low-resolution. This means we cannot expect pixel-perfect results when we work with images from modern cameras. If we want more precise segmentations, we can fine-tune a state-of-the-art segmentation model, as shown in [our previous blog post](https://huggingface.co/blog/fine-tune-segformer). In that case, we can still use CLIPSeg to generate some rough labels, and then refine them in a labeling tool such as [Segments.ai](https://segments.ai/?utm_source=hf&utm_medium=blog&utm_campaign=clipseg). Before we describe how to do that, let\u2019s first take a look at how CLIPSeg works.\n\n## CLIP: the magic model behind CLIPSeg\n\n[CLIP](https://huggingface.co/docs/transformers/main/en/model_doc/clip), which stands for **C**ontrastive **L**anguage\u2013**I**mage **P**re-training, is a model developed by OpenAI in 2021. You can give CLIP an image or a piece of text, and CLIP will output an abstract *representation* of your input. This abstract representation, also called an *embedding*, is really just a vector (a list of numbers). You can think of this vector as a point in high-dimensional space. CLIP is trained so that the representations of similar pictures and texts are similar as well. This means that if we input an image and a text description that fits that image, the representations of the image and the text will be similar (i.e., the high-dimensional points will be close together).\n\nAt first, this might not seem very useful, but it is actually very powerful. As an example, let\u2019s take a quick look at how CLIP can be used to classify images without ever having been trained on that task. To classify an image, we input the image and the different categories we want to choose from to CLIP (e.g. we input an image and the words \u201capple\u201d, \u201corange\u201d, \u2026).  CLIP then gives us back an embedding of the image and of each category. Now, we simply have to check which category embedding is closest to the embedding of the image, et voil\u00e0! Feels like magic, doesn\u2019t it? \n\n<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Overview of the CLIPSeg model\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/123_clipseg-zero-shot/clip-tv-example.png\"></medium-zoom>\n  <figcaption>Example of image classification using CLIP (<a href=\"https://openai.com/blog/clip/\">source</a>).</figcaption>\n</figure>\n\nWhat\u2019s more, CLIP is not only useful for classification, but it can also be used for [image search](https://huggingface.co/spaces/DrishtiSharma/Text-to-Image-search-using-CLIP) (can you see how this is similar to classification?), [text-to-imag...", "```python\nfrom segments.utils import bitmap2file\nimport numpy as np\n\ninds_np = inds.numpy().astype(np.uint32)\nunique_inds = np.unique(inds_np).tolist()\nf = bitmap2file(inds_np, is_segmentation_bitmap=True)\n\nasset = segments_client.upload_asset(f, \"clipseg_prediction.png\")\n\nattributes = {\n      'format_version': '0.1',\n      'annotations': [{\"id\": i, \"category_id\": i} for i in unique_inds if i != 0],\n      'segmentation_bitmap': { 'url': asset.url },\n  }\n\nsegments_client.add_label(sample.uuid, 'ground-truth', attributes)\n```\n\nIf you take a look at the [uploaded prediction on\nSegments.ai](https://segments.ai/admin-tobias/clipseg/samples/71a80d39-8cf3-4768-a097-e81e0b677517/ground-truth),\nyou can see that it\\'s not perfect. However, you can manually correct\nthe biggest mistakes, and then you can use the corrected dataset to\ntrain a better model than CLIPSeg.\n\n<figure class=\"image table text-center m-0 w-9/12\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Thumbnails of the final segmentation labels on Segments.ai.\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/123_clipseg-zero-shot/segments-thumbs.png\"></medium-zoom>\n</figure>\n\n## Conclusion\n\nCLIPSeg is a zero-shot segmentation model that works with both text and image prompts. The model adds a decoder to CLIP and can segment almost anything. However, the output segmentation masks are still very low-res for now, so you\u2019ll probably still want to fine-tune a different segmentation model if accuracy is important. \n\nNote that there's more research on zero-shot segmentation currently being conducted, so you can expect more models to be added in the near future. One example is [GroupViT](https://huggingface.co/docs/transformers/model_doc/groupvit), which is already available in \ud83e\udd17 Transformers. To stay up to date with the latest news in segmentation research, you can follow us on Twitter: [@TobiasCornille](https://twitter.com/tobiascornille), [@NielsRogge](https://twitter.com/nielsrogge), and [@huggingface](https://twitter.com/huggingface).\n\n\nIf you\u2019re interested in learning how to fine-tune a state-of-the-art segmentation model, check out our previous blog post: [https://huggingface.co/blog/fine-tune-segformer](https://huggingface.co/blog/fine-tune-segformer).", "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# CLIPSeg\n\n## Overview\n\nThe CLIPSeg model was proposed in [Image Segmentation Using Text and Image Prompts](https://arxiv.org/abs/2112.10003) by Timo L\u00fcddecke\nand Alexander Ecker. CLIPSeg adds a minimal decoder on top of a frozen [CLIP](clip) model for zero- and one-shot image segmentation.\n\nThe abstract from the paper is the following:\n\n*Image segmentation is usually addressed by training a\nmodel for a fixed set of object classes. Incorporating additional classes or more complex queries later is expensive\nas it requires re-training the model on a dataset that encompasses these expressions. Here we propose a system\nthat can generate image segmentations based on arbitrary\nprompts at test time. A prompt can be either a text or an\nimage. This approach enables us to create a unified model\n(trained once) for three common segmentation tasks, which\ncome with distinct challenges: referring expression segmentation, zero-shot segmentation and one-shot segmentation.\nWe build upon the CLIP model as a backbone which we extend with a transformer-based decoder that enables dense\nprediction. After training on an extended version of the\nPhraseCut dataset, our system generates a binary segmentation map for an image based on a free-text prompt or on\nan additional image expressing the query. We analyze different variants of the latter image-based prompts in detail.\nThis novel hybrid input allows for dynamic adaptation not\nonly to the three segmentation tasks mentioned above, but\nto any binary segmentation task where a text or image query\ncan be formulated. Finally, we find our system to adapt well\nto generalized queries involving affordances or properties*\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/clipseg_architecture.png\"\nalt=\"drawing\" width=\"600\"/> \n\n<small> CLIPSeg overview. Taken from the <a href=\"https://arxiv.org/abs/2112.10003\">original paper.</a> </small>\n\nThis model was contributed by [nielsr](https://huggingface.co/nielsr).\nThe original code can be found [here](https://github.com/timojl/clipseg).\n\n## Usage tips\n\n- [`CLIPSegForImageSegmentation`] adds a decoder on top of [`CLIPSegModel`]. The latter is identical to [`CLIPModel`].\n- [`CLIPSegForImageSegmentation`] can generate image segmentations based on arbitrary prompts at test time. A prompt can be either a text\n(provided to the model as `input_ids`) or an image (provided to the model as `conditional_pixel_values`). One can also provide custom\nconditional embeddings (provided to the model as `conditional_embeddings`).\n\n## Resources\n\nA list of official Hugging Face and community (indicated by \ud83c\udf0e) resources to help you get started with CLIPSeg. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n<PipelineTag pipeline=\"image-segmentation\"/>\n\n- A notebook that illustrates [zero-shot image segmentation with CLIPSeg](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/CLIPSeg/Zero_shot_image_segmentation_with_CLIPSeg.ipynb).\n\n## CLIPSegConfig\n\n[[autodoc]] CLIPSegConfig\n    - from_text_vision_configs\n\n## CLIPSegTextConfig\n\n[[autodoc]] CLIPSegTextConfig\n\n## CLIPSegVisionConfig\n\n[[autodoc]] CLIPSegVisionConfig\n\n## CLIPSegProcessor\n\n[[autodoc]] CLIPSegProcessor\n\n## CLIPSegModel\n\n[[autodoc]] CLIPSegModel\n    - forward\n    - get_text_features\n    - get_image_features\n\n## CLIPSegTextModel\n\n[[autodoc]] CLIPSegTextModel\n    - forward\n\n## CLIPSegVisionModel\n\n[[autodoc]] CLIPSegVisionModel\n    - forward\n\n## CLIPSegForImageSegmentation\n\n[[autodoc]] CLIPSegForImageSegmentation\n    - forward"], "test_settings": "chunk_7000_embeddings_openai-text-embedding-3-small_reader-model_gpt-4", "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response provides the correct resolution of images used by the CLIPSeg model, which is 352 x 352 pixels. However, it includes an unnecessary reference \"[\ubb38\uc11c 0]\" that is not part of the required answer format. The inclusion of this reference does not affect the factual accuracy of the response but slightly deviates from the expected concise format. Therefore, while the response is factually correct, the additional text prevents it from being a perfect match to the criteria for a score of 5."}, {"question": "What can you use Gradio for?\n", "true_answer": "Create a demo for your machine learning model, share your machine learning model with others, and debug your model.", "source_doc": "huggingface/course/blob/main/chapters/en/chapter9/9.mdx", "generated_answer": "You can use Gradio to build machine learning web applications in Python. It allows you to create demos, chatbots, and custom applications using high-level classes like `Interface` and `ChatInterface`, as well as low-level approaches with `gr.Blocks` for more flexibility in layout and data flow. Gradio is also part of an ecosystem with tools for programmatically querying applications in Python and JavaScript, and it's commonly used to host applications on platforms like Hugging Face Spaces [\ubb38\uc11c 0, \ubb38\uc11c 1].", "retrieved_docs": ["div align=\"center\">\n\n[<img src=\"readme_files/gradio.svg\" alt=\"gradio\" width=400>](https://gradio.app)<br>\n\n[![gradio-backend](https://github.com/gradio-app/gradio/actions/workflows/backend.yml/badge.svg)](https://github.com/gradio-app/gradio/actions/workflows/backend.yml)\n[![gradio-ui](https://github.com/gradio-app/gradio/actions/workflows/ui.yml/badge.svg)](https://github.com/gradio-app/gradio/actions/workflows/ui.yml)  \n [![PyPI](https://img.shields.io/pypi/v/gradio)](https://pypi.org/project/gradio/)\n[![PyPI downloads](https://img.shields.io/pypi/dm/gradio)](https://pypi.org/project/gradio/)\n![Python version](https://img.shields.io/badge/python-3.8+-important)\n[![Twitter follow](https://img.shields.io/twitter/follow/gradio?style=social&label=follow)](https://twitter.com/gradio)\n\n[Website](https://gradio.app)\n| [Documentation](https://gradio.app/docs/)\n| [Guides](https://gradio.app/guides/)\n| [Getting Started](https://gradio.app/getting_started/)\n| [Examples](demo/)\n| [\u4e2d\u6587](readme_files/zh-cn#readme)\n\n</div>\n\n# Gradio: Build Machine Learning Web Apps \u2014 in Python\n\n$getting_started\n\n## Questions?\n\nIf you'd like to report a bug or have a feature request, please create an [issue on GitHub](https://github.com/gradio-app/gradio/issues/new/choose). For general questions about usage, we are available on [our Discord server](https://discord.com/invite/feTf9x3ZSB) and happy to help.\n\nIf you like Gradio, please leave us a \u2b50 on GitHub!\n\n## Open Source Stack\n\nGradio is built on top of many wonderful open-source libraries!\n\n[<img src=\"readme_files/huggingface_mini.svg\" alt=\"huggingface\" height=40>](https://huggingface.co)\n[<img src=\"readme_files/python.svg\" alt=\"python\" height=40>](https://www.python.org)\n[<img src=\"readme_files/fastapi.svg\" alt=\"fastapi\" height=40>](https://fastapi.tiangolo.com)\n[<img src=\"readme_files/encode.svg\" alt=\"encode\" height=40>](https://www.encode.io)\n[<img src=\"readme_files/svelte.svg\" alt=\"svelte\" height=40>](https://svelte.dev)\n[<img src=\"readme_files/vite.svg\" alt=\"vite\" height=40>](https://vitejs.dev)\n[<img src=\"readme_files/pnpm.svg\" alt=\"pnpm\" height=40>](https://pnpm.io)\n[<img src=\"readme_files/tailwind.svg\" alt=\"tailwind\" height=40>](https://tailwindcss.com)\n[<img src=\"readme_files/storybook.svg\" alt=\"storybook\" height=40>](https://storybook.js.org/)\n[<img src=\"readme_files/chromatic.svg\" alt=\"chromatic\" height=40>](https://www.chromatic.com/)\n\n## License\n\nGradio is licensed under the Apache License 2.0 found in the [LICENSE](LICENSE) file in the root directory of this repository.\n\n## Citation\n\nAlso check out the paper _[Gradio: Hassle-Free Sharing and Testing of ML Models in the Wild](https://arxiv.org/abs/1906.02569), ICML HILL 2019_, and please cite it if you use Gradio in your work.\n\n```\n@article{abid2019gradio,\n  title = {Gradio: Hassle-Free Sharing and Testing of ML Models in the Wild},\n  author = {Abid, Abubakar and Abdalla, Ali and Abid, Ali and Khan, Dawood and Alfozan, Abdulrahman and Zou, James},\n  journal = {arXiv preprint arXiv:1906.02569},\n  year = {2019},\n}\n```", "his demo identifies musical instruments from an audio file. It uses Gradio's Audio and Label components.", "```python\nimport gradio as gr\n\ndef greet(name):\n    return \"Hello \" + name + \"!\"\n\ndemo = gr.Interface(fn=greet, inputs=\"textbox\", outputs=\"textbox\")\n    \ndemo.launch(share=True)  # Share your demo with just 1 extra parameter \ud83d\ude80\n```\n\nWhen you run this code, a public URL will be generated for your demo in a matter of seconds, something like:\n\n\ud83d\udc49 &nbsp; `https://a23dsf231adb.gradio.live`\n\nNow, anyone around the world can try your Gradio demo from their browser, while the machine learning model and all computation continues to run locally on your computer.\n\nTo learn more about sharing your demo, read our dedicated guide on [sharing your Gradio application](https://www.gradio.app/guides/sharing-your-app).\n\n\n### An Overview of Gradio\n\nSo far, we've been discussing the `Interface` class, which is a high-level class that lets to build demos quickly with Gradio. But what else does Gradio do?\n\n#### Chatbots with `gr.ChatInterface`\n\nGradio includes another high-level class, `gr.ChatInterface`, which is specifically designed to create Chatbot UIs. Similar to `Interface`, you supply a function and Gradio creates a fully working Chatbot UI. If you're interested in creating a chatbot, you can jump straight [our dedicated guide on `gr.ChatInterface`](https://www.gradio.app/guides/creating-a-chatbot-fast).\n\n#### Custom Demos with `gr.Blocks`\n\nGradio also offers a low-level approach for designing web apps with more flexible layouts and data flows with the `gr.Blocks` class. Blocks allows you to do things like control where components appear on the page, handle complex data flows (e.g. outputs can serve as inputs to other functions), and update properties/visibility of components based on user interaction \u2014 still all in Python. \n\nYou can build very custom and complex applications using `gr.Blocks()`. For example, the popular image generation [Automatic1111 Web UI](https://github.com/AUTOMATIC1111/stable-diffusion-webui) is built using Gradio Blocks. We dive deeper into the `gr.Blocks` on our series on [building with Blocks](https://www.gradio.app/guides/blocks-and-event-listeners).\n\n\n#### The Gradio Python & JavaScript Ecosystem\n\nThat's the gist of the core `gradio` Python library, but Gradio is actually so much more! Its an entire ecosystem of Python and JavaScript libraries that let you build machine learning applications, or query them programmatically, in Python or JavaScript. Here are other related parts of the Gradio ecosystem:\n\n* [Gradio Python Client](https://www.gradio.app/guides/getting-started-with-the-python-client) (`gradio_client`): query any Gradio app programmatically in Python.\n* [Gradio JavaScript Client](https://www.gradio.app/guides/getting-started-with-the-js-client) (`@gradio/client`): query any Gradio app programmatically in JavaScript.\n* [Gradio-Lite](https://www.gradio.app/guides/gradio-lite) (`@gradio/lite`): write Gradio apps in Python that run entirely in the browser (no server needed!), thanks to Pyodide. \n* [Hugging Face Spaces](https://huggingface.co/spaces): the most popular place to host Gradio applications \u2014 for free!\n\n### What's Next?\n\nKeep learning about Gradio sequentially using the Gradio Guides, which include explanations as well as example code and embedded interactive demos. Next up: [key features about Gradio demos](https://www.gradio.app/guides/key-features).\n\nOr, if you already know the basics and are looking for something specific, you can search the more [technical API documentation](https://www.gradio.app/docs/).\n\n\n\n\n## Questions?\n\nIf you'd like to report a bug or have a feature request, please create an [issue on GitHub](https://github.com/gradio-app/gradio/issues/new/choose). For general questions about usage, we are available on [our Discord server](https://discord.com/invite/feTf9x3ZSB) and happy to help.\n\nIf you like Gradio, please leave us a \u2b50 on GitHub!\n\n## Open Source Stack\n\nGradio is built on top of many wonderful open-source libraries!\n\n[<img src=\"readme_files/huggingface_mini.svg\" alt=\"huggingface\" height=40>](https://huggingface.co)\n[<img src=\"readme_files/python.svg\" alt=\"python\" height=40>](https://www.python.org)\n[<img src=\"readme_files/fastapi.svg\" alt=\"fastapi\" height=40>](https://fastapi.tiangolo.com)\n[<img src=\"readme_files/encode.svg\" alt=\"encode\" height=40>](https://www.encode.io)\n[<img src=\"readme_files/svelte.svg\" alt=\"svelte\" height=40>](https://svelte.dev)\n[<img src=\"readme_files/vite.svg\" alt=\"vite\" height=40>](https://vitejs.dev)\n[<img src=\"readme_files/pnpm.svg\" alt=\"pnpm\" height=40>](https://pnpm.io)\n[<img src=\"readme_files/tailwind.svg\" alt=\"tailwind\" height=40>](https://tailwindcss.com)\n[<img src=\"readme_files/storybook.svg\" alt=\"storybook\" height=40>](https://storybook.js.org/)\n[<img src=\"readme_files/chromatic.svg\" alt=\"chromatic\" height=40>](https://www.chromatic.com/)\n\n## License\n\nGradio is licensed under the Apache License 2.0 found in the [LICENSE](LICENSE) file in the root directory of this repository.\n\n## Citation\n\nAlso check out the paper _[G..."], "test_settings": "chunk_7000_embeddings_openai-text-embedding-3-small_reader-model_gpt-4", "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response provides a detailed and accurate description of what Gradio can be used for. It mentions building machine learning web applications, creating demos, chatbots, and custom applications, and highlights the use of high-level classes like `Interface` and `ChatInterface`. Additionally, it notes the flexibility offered by `gr.Blocks` and the integration with platforms like Hugging Face Spaces. While the response is more detailed than the criteria answer, it aligns well with the intended use of Gradio and provides a comprehensive overview. The response is factually correct and covers the key aspects of Gradio's functionality, thus meeting the criteria for a high score."}, {"question": "What TensorFlow API function is used to load a saved tensor file?\n", "true_answer": "safetensors.tensorflow.load_file", "source_doc": "huggingface/safetensors/blob/main/docs/source/api/tensorflow.mdx", "generated_answer": "The TensorFlow API function used to load a saved tensor file is `safetensors.tensorflow.load_file`.", "retrieved_docs": ["Tensorflow API\n\n[[autodoc]] safetensors.tensorflow.load_file\n[[autodoc]] safetensors.tensorflow.load\n[[autodoc]] safetensors.tensorflow.save_file\n[[autodoc]] safetensors.tensorflow.save", "Torch API\n\n[[autodoc]] safetensors.torch.load_file\n[[autodoc]] safetensors.torch.load\n[[autodoc]] safetensors.torch.save_file\n[[autodoc]] safetensors.torch.save\n[[autodoc]] safetensors.torch.load_model\n[[autodoc]] safetensors.torch.save_model", "The identifier used to load the model can be the identifier of any model on the Model Hub, as long as it is compatible with the BERT architecture. The entire list of available BERT checkpoints can be found [here](https://huggingface.co/models?filter=bert).\n\n### Saving methods[[saving-methods]]\n\nSaving a model is as easy as loading one \u2014 we use the `save_pretrained()` method, which is analogous to the `from_pretrained()` method:\n\n```py\nmodel.save_pretrained(\"directory_on_my_computer\")\n```\n\nThis saves two files to your disk:\n\n{#if fw === 'pt'}\n```\nls directory_on_my_computer\n\nconfig.json pytorch_model.bin\n```\n{:else}\n```\nls directory_on_my_computer\n\nconfig.json tf_model.h5\n```\n{/if}\n\nIf you take a look at the *config.json* file, you'll recognize the attributes necessary to build the model architecture. This file also contains some metadata, such as where the checkpoint originated and what \ud83e\udd17 Transformers version you were using when you last saved the checkpoint.\n\n{#if fw === 'pt'}\nThe *pytorch_model.bin* file is known as the *state dictionary*; it contains all your model's weights. The two files go hand in hand; the configuration is necessary to know your model's architecture, while the model weights are your model's parameters.\n\n{:else}\nThe *tf_model.h5* file is known as the *state dictionary*; it contains all your model's weights. The two files go hand in hand; the configuration is necessary to know your model's architecture, while the model weights are your model's parameters.\n\n{/if}\n\n## Using a Transformer model for inference[[using-a-transformer-model-for-inference]]\n\nNow that you know how to load and save a model, let's try using it to make some predictions. Transformer models can only process numbers \u2014 numbers that the tokenizer generates. But before we discuss tokenizers, let's explore what inputs the model accepts.\n\nTokenizers can take care of casting the inputs to the appropriate framework's tensors, but to help you understand what's going on, we'll take a quick look at what must be done before sending the inputs to the model.\n\nLet's say we have a couple of sequences:\n\n```py\nsequences = [\"Hello!\", \"Cool.\", \"Nice!\"]\n```\n\nThe tokenizer converts these to vocabulary indices which are typically called *input IDs*. Each sequence is now a list of numbers! The resulting output is:\n\n```py no-format\nencoded_sequences = [\n    [101, 7592, 999, 102],\n    [101, 4658, 1012, 102],\n    [101, 3835, 999, 102],\n]\n```\n\nThis is a list of encoded sequences: a list of lists. Tensors only accept rectangular shapes (think matrices). This \"array\" is already of rectangular shape, so converting it to a tensor is easy:\n\n{#if fw === 'pt'}\n```py\nimport torch\n\nmodel_inputs = torch.tensor(encoded_sequences)\n```\n{:else}\n```py\nimport tensorflow as tf\n\nmodel_inputs = tf.constant(encoded_sequences)\n```\n{/if}\n\n### Using the tensors as inputs to the model[[using-the-tensors-as-inputs-to-the-model]]\n\nMaking use of the tensors with the model is extremely simple \u2014 we just call the model with the inputs:\n\n```py\noutput = model(model_inputs)\n```\n\nWhile the model accepts a lot of different arguments, only the input IDs are necessary. We'll explain what the other arguments do and when they are required later, \nbut first we need to take a closer look at the tokenizers that build the inputs that a Transformer model can understand."], "test_settings": "chunk_7000_embeddings_openai-text-embedding-3-small_reader-model_gpt-4", "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response is mostly correct in identifying the function `safetensors.tensorflow.load_file` as the TensorFlow API function used to load a saved tensor file. However, the response includes additional context (\"The TensorFlow API function used to load a saved tensor file is\") that is not present in the criteria answer. While this does not affect the factual accuracy, it slightly deviates from the expected concise format. Nonetheless, the core information is accurate and aligns with the criteria answer."}, {"question": "Where can you access the logs of your Endpoints in Hugging Face Endpoints?\n", "true_answer": "In the \"Logs\" tab of your Endpoint through the UI.", "source_doc": "huggingface/hf-endpoints-documentation/blob/main/docs/source/guides/logs.mdx", "generated_answer": "You can access the logs of your Endpoints in Hugging Face Endpoints through the UI in the \u201cLogs\u201d tab of your Endpoint [\ubb38\uc11c 0].", "retrieved_docs": ["Access and read Logs\n\nHugging Face Endpoints provides access to the logs of your Endpoints through the UI in the \u201cLogs\u201d tab of your Endpoint. \n\nYou will have access to the build logs of your Image artifacts as well as access to the Container Logs during inference.\n\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/9_selection.png\" alt=\"select logs\" />\n\nThe Container Logs are only available when your Endpoint is in the \u201cRunning\u201d state. \n\n_Note: If your Endpoint creation is in the \u201cFailed\u201d state, you can check the Build Logs to see what the reason was, e.g. wrong version of a dependency, etc._\n\n**Build Logs:**\n\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/9_build_logs.png\" alt=\"build logs\" />\n\n**Container Logs:**\n\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/9_logs.png\" alt=\"container logs\" />", "Access and view Metrics\n\nHugging Face Endpoints provides access to the metrics and analytics of your Endpoints through the UI on the detailed overview in the \u201cAnalytics\u201d tab of your Endpoints. \n\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/10_metric.png\" alt=\"metric dashboard\" />\n\n## Access Metrics via API\n\nThe Hugging Face Inference Endpoints API exposes a [route to access the metrics](/docs/inference-endpoints/api_reference#getendpointmetrics) of your Endpoints. You can use this route to send customized Prometheus queries to your Endpoints.", "Help & Support \n\nWe have a variety of Inference Endpoints blog posts to help you at https://huggingface.co/blog:\n\n* [Getting Started with Hugging Face Inference Endpoints](https://huggingface.co/blog/inference-endpoints)\n* [Why we\u2019re switching to Hugging Face Inference Endpoints, and maybe you should too](https://huggingface.co/blog/mantis-case-study)\n* [Deploy LLMs with Hugging Face Inference Endpoints](https://huggingface.co/blog/inference-endpoints-llm)\n* [\ud83e\udd17 LLM suggestions in Argilla with HuggingFace Inference Endpoints](https://huggingface.co/blog/alvarobartt/argilla-suggestions-via-inference-endpoints)\n* [Deploy MusicGen in no time with Inference Endpoints](https://huggingface.co/blog/run-musicgen-as-an-api)\n\nNeed more help?\n\nFeel free to ask questions on the forum so the community can also benefit from the answers:\u00a0https://discuss.huggingface.co/. If you have any other questions or issues, please contact us at <api-enterprise@huggingface.co>."], "test_settings": "chunk_7000_embeddings_openai-text-embedding-3-small_reader-model_gpt-4", "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response accurately describes how to access the logs of Endpoints in Hugging Face Endpoints, specifying the \"Logs\" tab through the UI, which aligns with the criteria for a correct and factual answer. However, the inclusion of \"[\ubb38\uc11c 0]\" is unnecessary and does not contribute to the clarity or accuracy of the response. Despite this minor issue, the core information provided is correct and matches the standard of the criteria answer."}, {"question": "What is the latest task added to Hugging Face AutoTrain for Computer Vision?\n", "true_answer": "Image Classification", "source_doc": "huggingface/blog/blob/main/autotrain-image-classification.md", "generated_answer": "The latest task added to Hugging Face AutoTrain for Computer Vision is not mentioned in the provided documents.", "retrieved_docs": ["The Hugging Face Course\n\nThis repo contains the content that's used to create the **[Hugging Face course](https://huggingface.co/course/chapter1/1)**. The course teaches you about applying Transformers to various tasks in natural language processing and beyond. Along the way, you'll learn how to use the [Hugging Face](https://huggingface.co/) ecosystem \u2014 [\ud83e\udd17 Transformers](https://github.com/huggingface/transformers), [\ud83e\udd17 Datasets](https://github.com/huggingface/datasets), [\ud83e\udd17 Tokenizers](https://github.com/huggingface/tokenizers), and [\ud83e\udd17 Accelerate](https://github.com/huggingface/accelerate) \u2014 as well as the [Hugging Face Hub](https://huggingface.co/models). It's completely free and open-source!\n\n## \ud83c\udf0e Languages and translations", "[Hugging Face example scripts](https://github.com/huggingface/transformers/tree/main/examples) include different [self-supervised pre-training strategies](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-pretraining) like [MAE](https://arxiv.org/abs/2111.06377), and [contrastive image-text pre-training strategies](https://github.com/huggingface/transformers/tree/main/examples/pytorch/contrastive-image-text) like [CLIP](https://arxiv.org/abs/2103.00020). These scripts are valuable resources for the research community as well as for practitioners willing to run pre-training from scratch on custom data corpora.\n\nSome tasks are not inherently meant for fine-tuning, though. Examples include zero-shot image classification (such as [CLIP](https://huggingface.co/docs/transformers/main/en/model_doc/clip)), zero-shot object detection (such as [OWL-ViT](https://huggingface.co/docs/transformers/main/en/model_doc/owlvit)), and zero-shot segmentation (such as [CLIPSeg](https://huggingface.co/docs/transformers/model_doc/clipseg)). We\u2019ll revisit these models in this post.\n\n## Integrations with Datasets\n\n[Datasets](https://huggingface.co/docs/datasets) provides easy access to thousands of datasets of different modalities. As mentioned earlier, the Hub has over 100 datasets for computer vision. Some examples worth noting here: [ImageNet-1k](https://huggingface.co/datasets/imagenet-1k), [Scene Parsing](https://huggingface.co/datasets/scene_parse_150), [NYU Depth V2](https://huggingface.co/datasets/sayakpaul/nyu_depth_v2), [COYO-700M](https://huggingface.co/datasets/kakaobrain/coyo-700m), and [LAION-400M](https://huggingface.co/datasets/laion/laion400m). With these datasets being on the Hub, one can easily load them with just two lines of code:\n\n```py\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"scene_parse_150\")\n```\n\nBesides these datasets, we provide integration support with augmentation libraries like [albumentations](https://github.com/huggingface/notebooks/blob/main/examples/image_classification_albumentations.ipynb) and [Kornia](https://github.com/huggingface/notebooks/blob/main/examples/image_classification_kornia.ipynb). The community can take advantage of the flexibility and performance of Datasets and powerful augmentation transformations provided by these libraries. In addition to these, we also provide [dedicated data-loading guides](https://huggingface.co/docs/datasets/image_load) for core vision tasks: image classification, image segmentation, object detection, and depth estimation.\n\n## \ud83e\udd17 \ud83e\udd1d timm\n\n`timm`, also known as [pytorch-image-models](https://github.com/rwightman/pytorch-image-models), is an open-source collection of state-of-the-art PyTorch image models, pre-trained weights, and utility scripts for training, inference, and validation.\n\nWe have over 200 models from `timm` on the Hub and more are on the way. Check out the [documentation](https://huggingface.co/docs/timm/index) to know more about this integration.\n\n## \ud83e\udde8 Diffusers\n\n[Diffusers](https://huggingface.co/docs/diffusers) provides pre-trained vision and audio diffusion models, and serves as a modular toolbox for inference and training. With this library, you can generate plausible images from natural language inputs amongst other creative use cases. Here is an example:\n\n```py\nfrom diffusers import DiffusionPipeline\n\ngenerator = DiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\")\ngenerator.to(\u201ccuda\u201d)\n\nimage = generator(\"An image of a squirrel in Picasso style\").images[0]\n```\n\n<div align=\"center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/cv_state/sd_output.png\"/>\n</div>\n\nThis type of technology can empower a new generation of creative applications and also aid artists coming from different backgrounds. To know more about Diffusers and the different use cases, check out the [official documentation](https://huggingface.co/docs/diffusers).\n\nThe literature on Diffusion-based models is developing at a rapid pace which is why we partnered with [Jonathan Whitaker](https://github.com/johnowhitaker) to develop a course on it. The course is free, and you can check it out [here](https://github.com/huggingface/diffusion-models-class).\n\n## Support for third-party libraries\n\nCentral to the Hugging Face ecosystem is the [Hugging Face Hub](https://huggingface.co/docs/hub), which lets people collaborate effectively on Machine Learning. As mentioned earlier, we not only support models from \ud83e\udd17 Transformers on the Hub but also models from other third-party libraries. To this end, we provide [several utilities](https://huggingface.co/docs/hub/models-adding-libraries) so that you can integrate your own library with the Hub. One of the primary advantages of doing this is that it becomes very easy to share artifacts (such as models and datasets) with the community, thereby making it easier for your users to try out your models.\n\nWhen you have your models hosted on the H...", "--\ntitle: The State of Computer Vision at Hugging Face \ud83e\udd17\nthumbnail: /blog/assets/cv_state/thumbnail.png\nauthors:\n- user: sayakpaul\n---\n\n# The State of Computer Vision at Hugging Face \ud83e\udd17\n\n\nAt Hugging Face, we pride ourselves on democratizing the field of artificial intelligence together with the community. As a part of that mission, we began focusing our efforts on computer vision over the last year. What started as a [PR for having Vision Transformers (ViT) in \ud83e\udd17 Transformers](https://github.com/huggingface/transformers/pull/10950) has now grown into something much bigger \u2013 8 core vision tasks, over 3000 models, and over 100 datasets on the Hugging Face Hub.\n\nA lot of exciting things have happened since ViTs joined the Hub. In this blog post, we\u2019ll summarize what went down and what\u2019s coming to support the continuous progress of Computer Vision from the \ud83e\udd17 ecosystem.\n\nHere is a list of things we\u2019ll cover:\n\n- [Supported vision tasks and Pipelines](#support-for-pipelines)\n- [Training your own vision models](#training-your-own-models)\n- [Integration with `timm`](#\ud83e\udd17-\ud83e\udd1d-timm)\n- [Diffusers](#\ud83e\udde8-diffusers)\n- [Support for third-party libraries](#support-for-third-party-libraries)\n- [Deployment](#deployment)\n- and much more!\n\n## Enabling the community: One task at a time \ud83d\udc41\n\nThe Hugging Face Hub is home to over 100,000 public models for different tasks such as next-word prediction, mask filling, token classification, sequence classification, and so on. As of today, we support [8 core vision tasks](https://huggingface.co/tasks) providing many model checkpoints:\n\n- Image classification\n- Image segmentation\n- (Zero-shot) object detection\n- Video classification\n- Depth estimation\n- Image-to-image synthesis\n- Unconditional image generation\n- Zero-shot image classification\n\nEach of these tasks comes with at least 10 model checkpoints on the Hub for you to explore. Furthermore, we support [tasks](https://huggingface.co/tasks) that lie at the intersection of vision and language such as:\n\n- Image-to-text (image captioning, OCR)\n- Text-to-image\n- Document question-answering\n- Visual question-answering\n\nThese tasks entail not only state-of-the-art Transformer-based architectures such as [ViT](https://huggingface.co/docs/transformers/model_doc/vit), [Swin](https://huggingface.co/docs/transformers/model_doc/swin), [DETR](https://huggingface.co/docs/transformers/model_doc/detr) but also *pure convolutional* architectures like [ConvNeXt](https://huggingface.co/docs/transformers/model_doc/convnext), [ResNet](https://huggingface.co/docs/transformers/model_doc/resnet), [RegNet](https://huggingface.co/docs/transformers/model_doc/regnet), and more! Architectures like ResNets are still very much relevant for a myriad of industrial use cases and hence the support of these non-Transformer architectures in \ud83e\udd17 Transformers.\n\nIt\u2019s also important to note that the models on the Hub are not just from the Transformers library but also from other third-party libraries. For example, even though we support tasks like unconditional image generation on the Hub, we don\u2019t have any models supporting that task in Transformers yet (such as [this](https://huggingface.co/ceyda/butterfly_cropped_uniq1K_512)). Supporting all ML tasks, whether they are solved with Transformers or a third-party library is a part of our mission to foster a collaborative open-source Machine Learning ecosystem.\n\n## Support for Pipelines\n\nWe developed [Pipelines](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines) to equip practitioners with the tools they need to easily incorporate machine learning into their toolbox. They provide an easy way to perform inference on a given input with respect to a task. We have support for [seven vision tasks](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#computer-vision) in Pipelines. Here is an example of using Pipelines for depth estimation:\n\n```py\nfrom transformers import pipeline\n\ndepth_estimator = pipeline(task=\"depth-estimation\", model=\"Intel/dpt-large\")\noutput = depth_estimator(\"http://images.cocodataset.org/val2017/000000039769.jpg\")\n\n# This is a tensor with the values being the depth expressed\n# in meters for each pixel\noutput[\"depth\"]\n```\n\n<div align=\"center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/cv_state/depth_estimation_output.png\"/>\n</div>\n\nThe interface remains the same even for tasks like visual question-answering:\n\n```py\nfrom transformers import pipeline\n\noracle = pipeline(model=\"dandelin/vilt-b32-finetuned-vqa\")\nimage_url = \"https://huggingface.co/datasets/mishig/sample_images/resolve/main/tiger.jpg\"\n\noracle(question=\"What's the animal doing?\", image=image_url, top_k=1)\n# [{'score': 0.778620, 'answer': 'laying down'}]\n```\n\n## Training your own models\n\nWhile being able to use a model for off-the-shelf inference is a great way to get started, fine-tuning is where the community gets the most benefits. This is especially true when your ..."], "test_settings": "chunk_7000_embeddings_openai-text-embedding-3-small_reader-model_gpt-4", "eval_score_GPT4": "1", "eval_feedback_GPT4": "The response does not provide the correct answer to the question, which is \"Image Classification.\" Instead, it states that the information is not available in the provided documents. This indicates a lack of accuracy and completeness in the response. The task was to identify the latest task added to Hugging Face AutoTrain for Computer Vision, and the response fails to do so. Therefore, it does not meet the criteria for being correct, accurate, or factual as per the rubric."}, {"question": "What is the default repository type created by the `create_repo` function on Hugging Face Hub?\n", "true_answer": "model", "source_doc": "huggingface/huggingface_hub/blob/main/docs/source/en/guides/repository.md", "generated_answer": "The default repository type created by the `create_repo` function on Hugging Face Hub is a model repository.", "retrieved_docs": ["Hugging Face Hub Client library\n\n## Download files from the Hub\n\nThe `hf_hub_download()` function is the main function to download files from the Hub. One\nadvantage of using it is that files are cached locally, so you won't have to\ndownload the files multiple times. If there are changes in the repository, the\nfiles will be automatically downloaded again.\n\n\n### `hf_hub_download`\n\nThe function takes the following parameters, downloads the remote file,\nstores it to disk (in a version-aware way) and returns its local file path.\n\nParameters:\n- a `repo_id` (a user or organization name and a repo name, separated by `/`, like `julien-c/EsperBERTo-small`)\n- a `filename` (like `pytorch_model.bin`)\n- an optional Git revision id (can be a branch name, a tag, or a commit hash)\n- a `cache_dir` which you can specify if you want to control where on disk the\n  files are cached.\n\n```python\nfrom huggingface_hub import hf_hub_download\nhf_hub_download(\"lysandre/arxiv-nlp\", filename=\"config.json\")\n```\n\n### `snapshot_download`\n\nUsing `hf_hub_download()` works well when you know which files you want to download;\nfor example a model file alongside a configuration file, both with static names.\nThere are cases in which you will prefer to download all the files of the remote\nrepository at a specified revision. That's what `snapshot_download()` does. It\ndownloads and stores a remote repository to disk (in a versioning-aware way) and\nreturns its local file path.\n\nParameters:\n- a `repo_id` in the format `namespace/repository`\n- a `revision` on which the repository will be downloaded\n- a `cache_dir` which you can specify if you want to control where on disk the\n  files are cached\n\n### `hf_hub_url`\n\nInternally, the library uses `hf_hub_url()` to return the URL to download the actual files:\n`https://huggingface.co/julien-c/EsperBERTo-small/resolve/main/pytorch_model.bin`\n\n\nParameters:\n- a `repo_id` (a user or organization name and a repo name seperated by a `/`, like `julien-c/EsperBERTo-small`)\n- a `filename` (like `pytorch_model.bin`)\n- an optional `subfolder`, corresponding to a folder inside the model repo\n- an optional `repo_type`, such as `dataset` or `space`\n- an optional Git revision id (can be a branch name, a tag, or a commit hash)\n\nIf you check out this URL's headers with a `HEAD` http request (which you can do\nfrom the command line with `curl -I`) for a few different files, you'll see\nthat:\n- small files are returned directly\n- large files (i.e. the ones stored through\n  [git-lfs](https://git-lfs.github.com/)) are returned via a redirect to a\n  Cloudfront URL. Cloudfront is a Content Delivery Network, or CDN, that ensures\n  that downloads are as fast as possible from anywhere on the globe.\n\n<br>\n\n## Publish files to the Hub\n\nIf you've used Git before, this will be very easy since Git is used to manage\nfiles in the Hub. You can find a step-by-step guide on how to upload your model\nto the Hub: https://huggingface.co/docs/hub/adding-a-model. \n\n\n### API utilities in `hf_api.py`\n\nYou don't need them for the standard publishing workflow (ie. using git command line), however, if you need a\nprogrammatic way of creating a repo, deleting it (`\u26a0\ufe0f caution`), pushing a\nsingle file to a repo or listing models from the Hub, you'll find helpers in\n`hf_api.py`. Some example functionality available with the `HfApi` class:\n\n* `whoami()`\n* `create_repo()`\n* `list_repo_files()`\n* `list_repo_objects()`\n* `delete_repo()`\n* `update_repo_visibility()`\n* `create_commit()`\n* `upload_file()`\n* `delete_file()`\n* `delete_folder()`\n\nThose API utilities are also exposed through the `huggingface-cli` CLI:\n\n```bash\nhuggingface-cli login\nhuggingface-cli logout\nhuggingface-cli whoami\nhuggingface-cli repo create\n```\n\nWith the `HfApi` class there are methods to query models, datasets, and metrics by specific tags (e.g. if you want to list models compatible with your library):\n- **Models**:\n  - `list_models()`\n  - `model_info()`\n  - `get_model_tags()`\n- **Datasets**:\n  - `list_datasets()`\n  - `dataset_info()`\n  - `get_dataset_tags()`\n- **Spaces**:\n  - `list_spaces()`\n  - `space_info()`\n\nThese lightly wrap around the API Endpoints. Documentation for valid parameters and descriptions can be found [here](https://huggingface.co/docs/hub/endpoints).\n  \n\n### Advanced programmatic repository management \n\nThe `Repository` class helps manage both offline Git repositories and Hugging\nFace Hub repositories. Using the `Repository` class requires `git` and `git-lfs`\nto be installed.\n\nInstantiate a `Repository` object by calling it with a path to a local Git\nclone/repository:\n\n```python\n>>> from huggingface_hub import Repository\n>>> repo = Repository(\"<path>/<to>/<folder>\")\n```\n\nThe `Repository` takes a `clone_from` string as parameter. This can stay as\n`None` for offline management, but can also be set to any URL pointing to a Git\nrepo to clone that repository in the specified directory:\n\n```python\n>>> repo = Repository(\"huggingface-hub\", clone_from=\"https://github.com/huggingfac...", "Repositories\n\nModels, Spaces, and Datasets are hosted on the Hugging Face Hub as [Git repositories](https://git-scm.com/about), which means that version control and collaboration are core elements of the Hub. In a nutshell, a repository (also known as a **repo**) is a place where code and assets can be stored to back up your work, share it with the community, and work in a team. \n\nIn these pages, you will go over the basics of getting started with Git and interacting with repositories on the Hub. Once you get the hang of it, you can explore the best practices and next steps that we've compiled for effective repository usage.\n\n## Contents\n\n- [Getting Started with Repositories](./repositories-getting-started)\n- [Settings](./repositories-settings)\n- [Pull Requests & Discussions](./repositories-pull-requests-discussions)\n  - [Pull Requests advanced usage](./repositories-pull-requests-discussions#pull-requests-advanced-usage)\n- [Webhooks](./webhooks)\n- [Notifications](./notifications)\n- [Collections](./collections)\n- [Repository size recommendations](./repositories-recommendations)\n- [Next Steps](./repositories-next-steps)\n- [Licenses](./repositories-licenses)", "!--\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n-->\n\n# Create and manage a repository\n\nThe Hugging Face Hub is a collection of git repositories. [Git](https://git-scm.com/) is a widely used tool in software\ndevelopment to easily version projects when working collaboratively. This guide will show you how to interact with the\nrepositories on the Hub, especially:\n\n- Create and delete a repository.\n- Manage branches and tags. \n- Rename your repository.\n- Update your repository visibility.\n- Manage a local copy of your repository.\n\n<Tip warning={true}>\n\nIf you are used to working with platforms such as GitLab/GitHub/Bitbucket, your first instinct\nmight be to use `git` CLI to clone your repo (`git clone`), commit changes (`git add, git commit`) and push them\n(`git push`). This is valid when using the Hugging Face Hub. However, software engineering and machine learning do\nnot share the same requirements and workflows. Model repositories might maintain large model weight files for different\nframeworks and tools, so cloning the repository can lead to you maintaining large local folders with massive sizes. As\na result, it may be more efficient to use our custom HTTP methods. You can read our [Git vs HTTP paradigm](../concepts/git_vs_http)\nexplanation page for more details.\n\n</Tip>\n\nIf you want to create and manage a repository on the Hub, your machine must be logged in. If you are not, please refer to\n[this section](../quick-start#authentication). In the rest of this guide, we will assume that your machine is logged in.\n\n## Repo creation and deletion\n\nThe first step is to know how to create and delete repositories. You can only manage repositories that you own (under\nyour username namespace) or from organizations in which you have write permissions.\n\n### Create a repository\n\nCreate an empty repository with [`create_repo`] and give it a name with the `repo_id` parameter. The `repo_id` is your namespace followed by the repository name: `username_or_org/repo_name`.\n\n```py\n>>> from huggingface_hub import create_repo\n>>> create_repo(\"lysandre/test-model\")\n'https://huggingface.co/lysandre/test-model'\n```\n\nBy default, [`create_repo`] creates a model repository. But you can use the `repo_type` parameter to specify another repository type. For example, if you want to create a dataset repository:\n\n```py\n>>> from huggingface_hub import create_repo\n>>> create_repo(\"lysandre/test-dataset\", repo_type=\"dataset\")\n'https://huggingface.co/datasets/lysandre/test-dataset'\n```\n\nWhen you create a repository, you can set your repository visibility with the `private` parameter.\n\n```py\n>>> from huggingface_hub import create_repo\n>>> create_repo(\"lysandre/test-private\", private=True)\n```\n\nIf you want to change the repository visibility at a later time, you can use the [`update_repo_visibility`] function.\n\n### Delete a repository\n\nDelete a repository with [`delete_repo`]. Make sure you want to delete a repository because this is an irreversible process!\n\nSpecify the `repo_id` of the repository you want to delete:\n\n```py\n>>> delete_repo(repo_id=\"lysandre/my-corrupted-dataset\", repo_type=\"dataset\")\n```\n\n### Duplicate a repository (only for Spaces)\n\nIn some cases, you want to copy someone else's repo to adapt it to your use case.\nThis is possible for Spaces using the [`duplicate_space`] method. It will duplicate the whole repository.\nYou will still need to configure your own settings (hardware, sleep-time, storage, variables and secrets). Check out our [Manage your Space](./manage-spaces) guide for more details.\n\n```py\n>>> from huggingface_hub import duplicate_space\n>>> duplicate_space(\"multimodalart/dreambooth-training\", private=False)\nRepoUrl('https://huggingface.co/spaces/nateraw/dreambooth-training',...)\n```\n\n## Upload and download files\n\nNow that you have created your repository, you are interested in pushing changes to it and downloading files from it.\n\nThese 2 topics deserve their own guides. Please refer to the [upload](./upload) and the [download](./download) guides\nto learn how to use your repository.\n\n\n## Branches and tags\n\nGit repositories often make use of branches to store different versions of a same repository.\nTags can also be used to flag a specific state of your repository, for example, when releasing a version.\nMore generally, branches and tags are referred as [git references](https://git-scm.com/book/en/v2/Git-Internals-Git-References).\n\n### Create branches and tags\n\nYou can create new branch and tags using [`create_branch`] and [`create_tag`]:\n\n```py\n>>> from huggingface_hub import create_branch, create_tag\n\n# Create a branch on a Space repo from `main` branch\n>>> create_branch(\"Matthijs/speecht5-tts-demo\", repo_type=\"space\", branch=\"handle-dog-speaker\")\n\n# Create a tag on a Dataset repo from `v0.1-release` branch\n>>> create_branch(\"bigcode/the-stack\", repo_type=\"dataset\", revision=\"v0.1-release\", tag=\"v0.1.1\"..."], "test_settings": "chunk_7000_embeddings_openai-text-embedding-3-small_reader-model_gpt-4", "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response correctly identifies the default repository type created by the `create_repo` function on Hugging Face Hub as a model repository. However, the answer is slightly more verbose than the criteria answer, which is simply \"model.\" Despite this, the response is accurate and factual, aligning with the expected answer. Therefore, it meets the requirements for a high score, though it could be more concise."}, {"question": "How many splits does the \"duorc\" dataset have?\n", "true_answer": "Six", "source_doc": "huggingface/datasets-server/blob/main/docs/source/splits.mdx", "generated_answer": "The \"duorc\" dataset has six splits. [\ubb38\uc11c 0]", "retrieved_docs": ["List splits and configurations\n\nDatasets typically have splits and may also have configurations. A _split_ is a subset of the dataset, like `train` and `test`, that are used during different stages of training and evaluating a model. A _configuration_ is a sub-dataset contained within a larger dataset. Configurations are especially common in multilingual speech datasets where there may be a different configuration for each language. If you're interested in learning more about splits and configurations, check out the [Load a dataset from the Hub tutorial](https://huggingface.co/docs/datasets/main/en/load_hub)!\n\nThis guide shows you how to use Datasets Server's `/splits` endpoint to retrieve a dataset's splits and configurations programmatically. Feel free to also try it out with [Postman](https://www.postman.com/huggingface/workspace/hugging-face-apis/request/23242779-f0cde3b9-c2ee-4062-aaca-65c4cfdd96f8), [RapidAPI](https://rapidapi.com/hugging-face-hugging-face-default/api/hugging-face-datasets-api), or [ReDoc](https://redocly.github.io/redoc/?url=https://datasets-server.huggingface.co/openapi.json#operation/listSplits)\n\nThe `/splits` endpoint accepts the dataset name as its query parameter:\n\n<inferencesnippet>\n<python>\n```python\nimport requests\nheaders = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\nAPI_URL = \"https://datasets-server.huggingface.co/splits?dataset=duorc\"\ndef query():\n    response = requests.get(API_URL, headers=headers)\n    return response.json()\ndata = query()\n```\n</python>\n<js>\n```js\nimport fetch from \"node-fetch\";\nasync function query(data) {\n    const response = await fetch(\n        \"https://datasets-server.huggingface.co/splits?dataset=duorc\",\n        {\n            headers: { Authorization: `Bearer ${API_TOKEN}` },\n            method: \"GET\"\n        }\n    );\n    const result = await response.json();\n    return result;\n}\nquery().then((response) => {\n    console.log(JSON.stringify(response));\n});\n```\n</js>\n<curl>\n```curl\ncurl https://datasets-server.huggingface.co/splits?dataset=duorc \\\n        -X GET \\\n        -H \"Authorization: Bearer ${API_TOKEN}\"\n```\n</curl>\n</inferencesnippet>\n\nThe endpoint response is a JSON containing a list of the dataset's splits and configurations. For example, the [duorc](https://huggingface.co/datasets/duorc) dataset has six splits and two configurations:\n\n```json\n{\n  \"splits\": [\n    { \"dataset\": \"duorc\", \"config\": \"ParaphraseRC\", \"split\": \"train\" },\n    { \"dataset\": \"duorc\", \"config\": \"ParaphraseRC\", \"split\": \"validation\" },\n    { \"dataset\": \"duorc\", \"config\": \"ParaphraseRC\", \"split\": \"test\" },\n    { \"dataset\": \"duorc\", \"config\": \"SelfRC\", \"split\": \"train\" },\n    { \"dataset\": \"duorc\", \"config\": \"SelfRC\", \"split\": \"validation\" },\n    { \"dataset\": \"duorc\", \"config\": \"SelfRC\", \"split\": \"test\" }\n  ],\n  \"pending\": [],\n  \"failed\": []\n}\n```", "Splits and configurations\n\nMachine learning datasets are commonly organized in *splits* and they may also have *configurations*. These internal structures provide the scaffolding for building out a dataset, and determines how a dataset should be split and organized. Understanding a dataset's structure can help you create your own dataset, and know which subset of data you should use when during model training and evaluation.\n\n![split-configs-server](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/split-configs-server.gif)\n\n## Splits\n\nEvery processed and cleaned dataset contains *splits*, specific subsets of data reserved for specific needs. The most common splits are:\n\n* `train`: data used to train a model; this data is exposed to the model\n* `validation`: data reserved for evaluation and improving model hyperparameters; this data is hidden from the model\n* `test`: data reserved for evaluation only; this data is completely hidden from the model and ourselves\n\nThe `validation` and `test` sets are especially important to ensure a model is actually learning instead of *overfitting*, or just memorizing the data.\n\n## Configurations\n\nA *configuration* is a higher-level internal structure than a split, and a configuration contains splits. You can think of a configuration as a sub-dataset contained within a larger dataset. It is a useful structure for adding additional layers of organization to a dataset. For example, if you take a look at the [Multilingual LibriSpeech (MLS)](https://huggingface.co/datasets/facebook/multilingual_librispeech) dataset, you'll notice there are eight different languages. While you can create a dataset containing all eight languages, it's probably neater to create a dataset with each language as a configuration. This way, users can instantly load a dataset with their language of interest instead of preprocessing the dataset to filter for a specific language.\n\nConfigurations are flexible, and can be used to organize a dataset along whatever objective you'd like. For example, the [SceneParse150](https://huggingface.co/datasets/scene_parse_150) dataset uses configurations to organize the dataset by task. One configuration is dedicated to segmenting the whole image, while the other configuration is for instance segmentation.", "Get dataset information\n\nDatasets Server provides an `/info` endpoint for exploring the general information about dataset, including such fields as description, citation, homepage, license and features.\n\nThe `/info` endpoint accepts two query parameters:\n\n- `dataset`: the dataset name\n- `config`: the configuration name\n\n<inferencesnippet>\n<python>\n```python\nimport requests\nheaders = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\nAPI_URL = \"https://datasets-server.huggingface.co/info?dataset=duorc&config=SelfRC\"\ndef query():\n    response = requests.get(API_URL, headers=headers)\n    return response.json()\ndata = query()\n```\n</python>\n<js>\n```js\nimport fetch from \"node-fetch\";\nasync function query(data) {\n    const response = await fetch(\n        \"https://datasets-server.huggingface.co/info?dataset=duorc&config=SelfRC\",\n        {\n            headers: { Authorization: `Bearer ${API_TOKEN}` },\n            method: \"GET\"\n        }\n    );\n    const result = await response.json();\n    return result;\n}\nquery().then((response) => {\n    console.log(JSON.stringify(response));\n});\n```\n</js>\n<curl>\n```curl\ncurl https://datasets-server.huggingface.co/info?dataset=duorc&config=SelfRC \\\n        -X GET \\\n        -H \"Authorization: Bearer ${API_TOKEN}\"\n```\n</curl>\n</inferencesnippet>\n\nThe endpoint response is a JSON with the `dataset_info` key. Its structure and content correspond to [DatasetInfo](https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.DatasetInfo) object of the `datasets` library.\n\n```json\n{\n    \"dataset_info\": {\n        \"description\": \"DuoRC contains 186,089 unique question-answer pairs created from a collection of 7680 pairs of movie plots where each pair in the collection reflects two versions of the same movie.\\n\",\n        \"citation\": \"@inproceedings{DuoRC,\\nauthor = { Amrita Saha and Rahul Aralikatte and Mitesh M. Khapra and Karthik Sankaranarayanan},title = {{DuoRC: Towards Complex Language Understanding with Paraphrased Reading Comprehension}},\\nbooktitle = {Meeting of the Association for Computational Linguistics (ACL)},\\nyear = {2018}\\n}\\n\",\n        \"homepage\": \"https://duorc.github.io/\",\n        \"license\": \"https://raw.githubusercontent.com/duorc/duorc/master/LICENSE\",\n        \"features\": {\n            \"plot_id\": {\n                \"dtype\": \"string\",\n                \"_type\": \"Value\"\n            },\n            \"plot\": {\n                \"dtype\": \"string\",\n                \"_type\": \"Value\"\n            },\n            \"title\": {\n                \"dtype\": \"string\",\n                \"_type\": \"Value\"\n            },\n            \"question_id\": {\n                \"dtype\": \"string\",\n                \"_type\": \"Value\"\n            },\n            \"question\": {\n                \"dtype\": \"string\",\n                \"_type\": \"Value\"\n            },\n            \"answers\": {\n                \"feature\": {\n                    \"dtype\": \"string\",\n                    \"_type\": \"Value\"\n                },\n                \"_type\": \"Sequence\"\n            },\n            \"no_answer\": {\n                \"dtype\": \"bool\",\n                \"_type\": \"Value\"\n            }\n        },\n        \"builder_name\": \"duorc\",\n        \"config_name\": \"SelfRC\",\n        \"version\": {\n            \"version_str\": \"1.0.0\",\n            \"major\": 1,\n            \"minor\": 0,\n            \"patch\": 0\n        },\n        \"splits\": {\n            \"train\": {\n                \"name\": \"train\",\n                \"num_bytes\": 239852729,\n                \"num_examples\": 60721,\n                \"dataset_name\": \"duorc\"\n            },\n            \"validation\": {\n                \"name\": \"validation\",\n                \"num_bytes\": 51662519,\n                \"num_examples\": 12961,\n                \"dataset_name\": \"duorc\"\n            },\n            \"test\": {\n                \"name\": \"test\",\n                \"num_bytes\": 49142710,\n                \"num_examples\": 12559,\n                \"dataset_name\": \"duorc\"\n            }\n        },\n        \"download_checksums\": {\n            \"https://raw.githubusercontent.com/duorc/duorc/master/dataset/SelfRC_train.json\": {\n                \"num_bytes\": 24388192,\n                \"checksum\": null\n            },\n            \"https://raw.githubusercontent.com/duorc/duorc/master/dataset/SelfRC_dev.json\": {\n                \"num_bytes\": 5051240,\n                \"checksum\": null\n            },\n            \"https://raw.githubusercontent.com/duorc/duorc/master/dataset/SelfRC_test.json\": {\n                \"num_bytes\": 5023228,\n                \"checksum\": null\n            }\n        },\n        \"download_size\": 34462660,\n        \"dataset_size\": 340657958,\n        \"size_in_bytes\": 375120618\n    }\n}\n```"], "test_settings": "chunk_7000_embeddings_openai-text-embedding-3-small_reader-model_gpt-4", "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response states that the \"duorc\" dataset has six splits, which aligns with the information provided in the reference answer. However, the inclusion of \"[\ubb38\uc11c 0]\" is unnecessary and does not contribute to the accuracy or factual nature of the response. Despite this, the core information is correct and matches the expected answer. Therefore, the response is mostly accurate and factual, but the extraneous text slightly detracts from its precision."}, {"question": "What is the purpose of Fully Sharded Data Parallel (FSDP) in distributed training?\n", "true_answer": "FSDP is developed for distributed training of large pretrained models up to 1T parameters by sharding the model parameters, gradients, and optimizer states across data parallel processes.", "source_doc": "huggingface/peft/blob/main/docs/source/accelerate/fsdp.md", "generated_answer": "The purpose of Fully Sharded Data Parallel (FSDP) in distributed training is to shard a model's parameters, gradients, and optimizer states across available GPUs, which reduces memory usage and improves GPU memory efficiency. This allows training of much larger models on fewer GPUs by effectively managing resources and potentially offloading to CPUs when necessary [0].", "retrieved_docs": ["!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Fully Sharded Data Parallel\n\n[Fully Sharded Data Parallel (FSDP)](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/) is a data parallel method that shards a model's parameters, gradients and optimizer states across the number of available GPUs (also called workers or *rank*). Unlike [DistributedDataParallel (DDP)](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html), FSDP reduces memory-usage because a model is replicated on each GPU. This improves GPU memory-efficiency and allows you to train much larger models on fewer GPUs. FSDP is integrated with the Accelerate, a library for easily managing training in distributed environments, which means it is available for use from the [`Trainer`] class.\n\nBefore you start, make sure Accelerate is installed and at least PyTorch 2.1.0 or newer.\n\n```bash\npip install accelerate\n```\n\n## FSDP configuration\n\nTo start, run the [`accelerate config`](https://huggingface.co/docs/accelerate/package_reference/cli#accelerate-config) command to create a configuration file for your training environment. Accelerate uses this configuration file to automatically setup the correct training environment based on your selected training options in `accelerate config`.\n\n```bash\naccelerate config\n```\n\nWhen you run `accelerate config`, you'll be prompted with a series of options to configure your training environment. This section covers some of the most important FSDP options. To learn more about the other available FSDP options, take a look at the [fsdp_config](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.fsdp_config) parameters.\n\n### Sharding strategy\n\nFSDP offers a number of sharding strategies to select from:\n\n* `FULL_SHARD` - shards model parameters, gradients and optimizer states across workers; select `1` for this option\n* `SHARD_GRAD_OP`- shard gradients and optimizer states across workers; select `2` for this option\n* `NO_SHARD` - don't shard anything (this is equivalent to DDP); select `3` for this option\n* `HYBRID_SHARD` - shard model parameters, gradients and optimizer states within each worker where each worker also has a full copy; select `4` for this option\n* `HYBRID_SHARD_ZERO2` - shard gradients and optimizer states within each worker where each worker also has a full copy; select `5` for this option\n\nThis is enabled by the `fsdp_sharding_strategy` flag.\n\n### CPU offload\n\nYou could also offload parameters and gradients when they are not in use to the CPU to save even more GPU memory and help you fit large models where even FSDP may not be sufficient. This is enabled by setting `fsdp_offload_params: true` when running `accelerate config`.\n\n### Wrapping policy\n\nFSDP is applied by wrapping each layer in the network. The wrapping is usually applied in a nested way where the full weights are discarded after each forward pass to save memory for use in the next layer. The *auto wrapping* policy is the simplest way to implement this and you don't need to change any code. You should select `fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP` to wrap a Transformer layer and `fsdp_transformer_layer_cls_to_wrap` to specify which layer to wrap (for example `BertLayer`).\n\nOtherwise, you can choose a size-based wrapping policy where FSDP is applied to a layer if it exceeds a certain number of parameters. This is enabled by setting `fsdp_wrap_policy: SIZE_BASED_WRAP` and `min_num_param` to the desired size threshold.\n\n### Checkpointing\n\nIntermediate checkpoints should be saved with `fsdp_state_dict_type: SHARDED_STATE_DICT` because saving the full state dict with CPU offloading on rank 0 takes a lot of time and often results in `NCCL Timeout` errors due to indefinite hanging during broadcasting. You can resume training with the sharded state dicts with the [`~accelerate.Accelerator.load_state`]` method.\n\n```py\n# directory containing checkpoints\naccelerator.load_state(\"ckpt\")\n```\n\nHowever, when training ends, you want to save the full state dict because sharded state dict is only compatible with FSDP.\n\n```py\nif trainer.is_fsdp_enabled:\n    trainer.accelerator.state.fsdp_plugin.set_state_dict_type(\"FULL_STATE_DICT\")\n\ntrainer.save_model(script_args....", "To launch training, run the [`accelerate launch`](https://huggingface.co/docs/accelerate/package_reference/cli#accelerate-launch) command and it'll automatically use the configuration file you previously created with `accelerate config`.\n\n```bash\naccelerate launch my-trainer-script.py\n```\n\n```bash\naccelerate launch --fsdp=\"full shard\" --fsdp_config=\"path/to/fsdp_config/ my-trainer-script.py\n```\n\n## Next steps\n\nFSDP can be a powerful tool for training really large models and you have access to more than one GPU or TPU. By sharding the model parameters, optimizer and gradient states, and even offloading them to the CPU when they're inactive, FSDP can reduce the high cost of large-scale training. If you're interested in learning more, the following may be helpful:\n\n* Follow along with the more in-depth Accelerate guide for [FSDP](https://huggingface.co/docs/accelerate/usage_guides/fsdp).\n* Read the [Introducing PyTorch Fully Sharded Data Parallel (FSDP) API](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/) blog post.\n* Read the [Scaling PyTorch models on Cloud TPUs with FSDP](https://pytorch.org/blog/scaling-pytorch-models-on-cloud-tpus-with-fsdp/) blog post.", "--\ntitle: \"Accelerate Large Model Training using PyTorch Fully Sharded Data Parallel\"\nthumbnail: /blog/assets/62_pytorch_fsdp/fsdp-thumbnail.png\nauthors:\n- user: smangrul\n- user: sgugger\n---\n\n# Accelerate Large Model Training using PyTorch Fully Sharded Data Parallel\n\n\nIn this post we will look at how we can leverage **[Accelerate](https://github.com/huggingface/accelerate)** Library for training large models which enables users to leverage the latest features of **[PyTorch FullyShardedDataParallel (FSDP)](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/)**.\n\n# Motivation \ud83e\udd17\n\n**With the ever increasing scale, size and parameters of the Machine Learning (ML) models, ML practitioners are finding it difficult to train or even load such large models on their hardware.** On one hand, it has been found that large models learn quickly (data and compute efficient) and are significantly more performant when compared to smaller models [1]; on the other hand, it  becomes prohibitive to train such models on most of the available hardware.  \n\nDistributed training is the key to enable training such large ML models. There have been major recent advances in the field of **Distributed Training at Scale**. Few the most notable advances are given below:\n1. Data Parallelism using ZeRO - Zero Redundancy Optimizer [2]\n    1. Stage 1: Shards optimizer states across data parallel workers/GPUs\n    2. Stage 2: Shards optimizer states + gradients across data parallel workers/GPUs\n    3. Stage 3: Shards optimizer states + gradients + model parameters across data parallel workers/GPUs\n    4. CPU Offload: Offloads the gradients + optimizer states to CPU building on top of ZERO Stage 2 [3]\n2. Tensor Parallelism [4]: Form of model parallelism wherein sharding parameters of individual layers with huge number of parameters across accelerators/GPUs is done in a clever manner to achieve parallel computation while avoiding expensive communication synchronization overheads. \n3. Pipeline Parallelism [5]: Form of model parallelism wherein different layers of the model are put across different accelerators/GPUs and pipelining is employed to keep all the accelerators running simultaneously. Here, for instance, the second accelerator/GPU computes on the first micro-batch while the first accelerator/GPU computes on the second micro-batch. \n4. 3D parallelism [3]: Employs Data Parallelism using ZERO + Tensor Parallelism + Pipeline Parallelism to train humongous models in the order of 100s of Billions of parameters. For instance, BigScience 176B parameters Language Model employ this [6].\n\nIn this post we will look at Data Parallelism using ZeRO and more specifically the latest PyTorch feature **[FullyShardedDataParallel (FSDP)](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/)**.  **[DeepSpeed](https://github.com/microsoft/deepspeed)** and **[FairScale](https://github.com/facebookresearch/fairscale/)** have implemented the core ideas of the ZERO paper. These have already been integrated in `transformers` Trainer and accompanied by great blog [Fit More and Train Faster With ZeRO via DeepSpeed and FairScale](https://huggingface.co/blog/zero-deepspeed-fairscale) [10]. PyTorch recently upstreamed the Fairscale FSDP into PyTorch Distributed with additional optimizations.\n\n# Accelerate \ud83d\ude80:  Leverage PyTorch FSDP without any code\u00a0changes\n\nWe will look at the task of Causal Language Modelling using GPT-2 Large (762M) and XL (1.5B) model variants.\n\nBelow is the code for pre-training GPT-2 model. It is similar to the official causal language modeling example [here](https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm_no_trainer.py) with the addition of 2 arguments `n_train` (2000) and `n_val` (500) to prevent preprocessing/training on entire data in order to perform quick proof of concept benchmarks.\n\n<a href=\"./assets/62_pytorch_fsdp/run_clm_no_trainer.py\" target=\"_parent\">run_clm_no_trainer.py</a>\n\nSample FSDP config after running the command `accelerate config`:\n```bash\ncompute_environment: LOCAL_MACHINE\ndeepspeed_config: {}\ndistributed_type: FSDP\nfsdp_config:\n  min_num_params: 2000\n  offload_params: false\n  sharding_strategy: 1\nmachine_rank: 0\nmain_process_ip: null\nmain_process_port: null\nmain_training_function: main\nmixed_precision: 'no'\nnum_machines: 1\nnum_processes: 2\nuse_cpu: false\n```\n\n## Multi-GPU FSDP\n\nHere, we experiment on the Single-Node Multi-GPU setting. We compare the performance of Distributed Data Parallel (DDP) and FSDP in various configurations. First, GPT-2 Large(762M) model is used wherein DDP works with certain batch sizes without throwing Out Of Memory (OOM) errors. Next, GPT-2 XL (1.5B) model is used wherein DDP fails with OOM error even on batch size of 1. We observe that FSDP enables larger batch sizes for GPT-2 Large model and it enables training the GPT-2 XL model with decent batch size unlike DDP.\n\n**Hardware setup**: 2X24GB NVIDIA Ti..."], "test_settings": "chunk_7000_embeddings_openai-text-embedding-3-small_reader-model_gpt-4", "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response accurately describes the purpose of Fully Sharded Data Parallel (FSDP) in distributed training. It correctly states that FSDP shards a model's parameters, gradients, and optimizer states across available GPUs, which helps in reducing memory usage and improving GPU memory efficiency. This explanation aligns well with the concept of FSDP, which is designed to enable the training of larger models on fewer GPUs by efficiently managing resources. However, the response could be improved by mentioning the specific scale of models FSDP is designed for, such as models up to 1 trillion parameters, as noted in the reference answer. Additionally, the mention of potentially offloading to CPUs, while relevant, is not a core aspect of FSDP's primary purpose as described in the reference answer. Overall, the response is mostly accurate and factual but lacks some specific details present in the reference answer."}, {"question": "What file format is used to save and store PyTorch model weights more securely than `.bin` files?\n", "true_answer": "`.safetensors`", "source_doc": "huggingface/safetensors/blob/main/docs/source/convert-weights.md", "generated_answer": "The file format used to save and store PyTorch model weights more securely than `.bin` files is the `.safetensors` format [\ubb38\uc11c 0].", "retrieved_docs": ["Convert weights to safetensors\n\nPyTorch model weights are commonly saved and stored as `.bin` files with Python's [`pickle`](https://docs.python.org/3/library/pickle.html) utility. To save and store your model weights in the more secure `safetensor` format, we recommend converting your weights to `.safetensors`.\n\nThe easiest way to convert your model weights is to use the [Convert Space](https://huggingface.co/spaces/diffusers/convert), given your model weights are already stored on the Hub. The Convert Space downloads the pickled weights, converts them, and opens a Pull Request to upload the newly converted `.safetensors` file to your repository.\n\n<Tip warning={true}>\n\nFor larger models, the Space may be a bit slower because its resources are tied up in converting other models. You can also try running the [convert.py](https://github.com/huggingface/safetensors/blob/main/bindings/python/convert.py) script (this is what the Space is running) locally to convert your weights.\n\nFeel free to ping [@Narsil](https://huggingface.co/Narsil) for any issues with the Space.\n\n</Tip>", "```python\nfrom accelerate import infer_auto_device_map, init_empty_weights\nfrom transformers import AutoConfig, AutoModelForCausalLM\n\nconfig = AutoConfig.from_pretrained(\"facebook/opt-13b\")\nwith init_empty_weights():\n    model = AutoModelForCausalLM.from_config(config)\n\ndevice_map = infer_auto_device_map(model)\n```\n\nThis will return a dictionary mapping modules or weights to a device. On a machine with one Titan RTX for instance, we get the following:\n\n```python out\n{'model.decoder.embed_tokens': 0,\n 'model.decoder.embed_positions': 0,\n 'model.decoder.final_layer_norm': 0,\n 'model.decoder.layers.0': 0,\n 'model.decoder.layers.1': 0,\n ...\n 'model.decoder.layers.9': 0,\n 'model.decoder.layers.10.self_attn': 0,\n 'model.decoder.layers.10.activation_fn': 0,\n 'model.decoder.layers.10.self_attn_layer_norm': 0,\n 'model.decoder.layers.10.fc1': 'cpu',\n 'model.decoder.layers.10.fc2': 'cpu',\n 'model.decoder.layers.10.final_layer_norm': 'cpu',\n 'model.decoder.layers.11': 'cpu',\n ...\n 'model.decoder.layers.17': 'cpu',\n 'model.decoder.layers.18.self_attn': 'cpu',\n 'model.decoder.layers.18.activation_fn': 'cpu',\n 'model.decoder.layers.18.self_attn_layer_norm': 'cpu',\n 'model.decoder.layers.18.fc1': 'disk',\n 'model.decoder.layers.18.fc2': 'disk',\n 'model.decoder.layers.18.final_layer_norm': 'disk',\n 'model.decoder.layers.19': 'disk',\n ...\n 'model.decoder.layers.39': 'disk',\n 'lm_head': 'disk'}\n```\n\nAccelerate evaluated that the embeddings and the decoder up until the 9th block could all fit on the GPU (device 0), then part of the 10th block needs to be on the CPU, as well as the following weights until the 17th layer. Then the 18th layer is split between the CPU and the disk and the following layers must all be offloaded to disk\n\nActually using this device map later on won't work, because the layers composing this model have residual connections (where the input of the block is added to the output of the block) so all of a given layer should be on the same device. We can indicate this to Accelerate by passing a list of module names that shouldn't be split with the `no_split_module_classes` keyword argument:\n\n```python\ndevice_map = infer_auto_device_map(model, no_split_module_classes=[\"OPTDecoderLayer\"])\n```\n\nThis will then return\n\n```python out\n'model.decoder.embed_tokens': 0,\n 'model.decoder.embed_positions': 0,\n 'model.decoder.final_layer_norm': 0,\n 'model.decoder.layers.0': 0,\n 'model.decoder.layers.1': 0,\n ...\n 'model.decoder.layers.9': 0,\n 'model.decoder.layers.10': 'cpu',\n 'model.decoder.layers.11': 'cpu',\n ...\n 'model.decoder.layers.17': 'cpu',\n 'model.decoder.layers.18': 'disk',\n ...\n 'model.decoder.layers.39': 'disk',\n 'lm_head': 'disk'}\n\n\n```\n\nNow, each layer is always on the same device.\n\nIn Transformers, when using `device_map` in the `from_pretrained()` method or in a `pipeline`, those classes of blocks to leave on the same device are automatically provided, so you don't need to worry about them. Note that you have the following options for `device_map` (only relevant when you have more than one GPU):\n- `\"auto\"` or `\"balanced\"`: Accelerate will split the weights so that each GPU is used equally;\n- `\"balanced_low_0\"`: Accelerate will split the weights so that each GPU is used equally except the first one, where it will try to have as little weights as possible (useful when you want to work with the outputs of the model on one GPU, for instance when  using the `generate` function);\n- `\"sequential\"`: Accelerate will fill the GPUs in order (so the last ones might not be used at all).\n\nYou can also pass your own `device_map` as long as it follows the format we saw before (dictionary layer/module names to device).\n\nFinally, note that the results of the `device_map` you receive depend on the selected dtype (as different types of floats take a different amount of space). Providing `dtype=\"float16\"` will give us different results:\n\n```python\ndevice_map = infer_auto_device_map(model, no_split_module_classes=[\"OPTDecoderLayer\"], dtype=\"float16\")\n```\n\nIn this precision, we can fit the model up to layer 21 on the GPU:\n\n```python out\n\n\n{'model.decoder.embed_tokens': 0,\n 'model.decoder.embed_positions': 0,\n 'model.decoder.final_layer_norm': 0,\n 'model.decoder.layers.0': 0,\n 'model.decoder.layers.1': 0,\n ...\n 'model.decoder.layers.21': 0,\n 'model.decoder.layers.22': 'cpu',\n ...\n 'model.decoder.layers.37': 'cpu',\n 'model.decoder.layers.38': 'disk',\n 'model.decoder.layers.39': 'disk',\n 'lm_head': 'disk'}\n```\n\nNow that we know where each weight is supposed to go, we can progressively load the pretrained weights inside the model.\n\n## Sharding state dicts\n\nTraditionally, PyTorch models are saved in a whole file containing a map from parameter name to weight. This map is often called a `state_dict`. Here is an excerpt from the [PyTorch documentation](https://pytorch.org/tutorials/beginner/basics/saveloadrun_tutorial.html) on saving on loading:\n\n```python\n# Save the model weights\ntorch.save(my_model.state_dict(), 'model_weights.pth')\n\n...", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Load safetensors\n\n[[open-in-colab]]\n\n[safetensors](https://github.com/huggingface/safetensors) is a safe and fast file format for storing and loading tensors. Typically, PyTorch model weights are saved or *pickled* into a `.bin` file with Python's [`pickle`](https://docs.python.org/3/library/pickle.html) utility. However, `pickle` is not secure and pickled files may contain malicious code that can be executed. safetensors is a secure alternative to `pickle`, making it ideal for sharing model weights.\n\nThis guide will show you how you load `.safetensor` files, and how to convert Stable Diffusion model weights stored in other formats to `.safetensor`. Before you start, make sure you have safetensors installed:\n\n```py\n# uncomment to install the necessary libraries in Colab\n#!pip install safetensors\n```\n\nIf you look at the [`runwayml/stable-diffusion-v1-5`](https://huggingface.co/runwayml/stable-diffusion-v1-5/tree/main) repository, you'll see weights inside the `text_encoder`, `unet` and `vae` subfolders are stored in the `.safetensors` format. By default, \ud83e\udd17 Diffusers automatically loads these `.safetensors` files from their subfolders if they're available in the model repository.\n\nFor more explicit control, you can optionally set `use_safetensors=True` (if `safetensors` is not installed, you'll get an error message asking you to install it):\n\n```py\nfrom diffusers import DiffusionPipeline\n\npipeline = DiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", use_safetensors=True)\n```\n\nHowever, model weights are not necessarily stored in separate subfolders like in the example above. Sometimes, all the weights are stored in a single `.safetensors` file. In this case, if the weights are Stable Diffusion weights, you can load the file directly with the [`~diffusers.loaders.FromSingleFileMixin.from_single_file`] method:\n\n```py\nfrom diffusers import StableDiffusionPipeline\n\npipeline = StableDiffusionPipeline.from_single_file(\n    \"https://huggingface.co/WarriorMama777/OrangeMixs/blob/main/Models/AbyssOrangeMix/AbyssOrangeMix.safetensors\"\n)\n```\n\n## Convert to safetensors\n\nNot all weights on the Hub are available in the `.safetensors` format, and you may encounter weights stored as `.bin`. In this case, use the [Convert Space](https://huggingface.co/spaces/diffusers/convert) to convert the weights to `.safetensors`. The Convert Space downloads the pickled weights, converts them, and opens a Pull Request to upload the newly converted `.safetensors` file on the Hub. This way, if there is any malicious code contained in the pickled files, they're uploaded to the Hub - which has a [security scanner](https://huggingface.co/docs/hub/security-pickle#hubs-security-scanner) to detect unsafe files and suspicious pickle imports - instead of your computer.\n\nYou can use the model with the new `.safetensors` weights by specifying the reference to the Pull Request in the `revision` parameter (you can also test it in this [Check PR](https://huggingface.co/spaces/diffusers/check_pr) Space on the Hub), for example `refs/pr/22`:\n\n```py\nfrom diffusers import DiffusionPipeline\n\npipeline = DiffusionPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-2-1\", revision=\"refs/pr/22\", use_safetensors=True\n)\n```\n\n## Why use safetensors?\n\nThere are several reasons for using safetensors:\n\n- Safety is the number one reason for using safetensors. As open-source and model distribution grows, it is important to be able to trust the model weights you downloaded don't contain any malicious code. The current size of the header in safetensors prevents parsing extremely large JSON files.\n- Loading speed between switching models is another reason to use safetensors, which performs zero-copy of the tensors. It is especially fast compared to `pickle` if you're loading the weights to CPU (the default case), and just as fast if not faster when directly loading the weights to GPU. You'll only notice the performance difference if the model is already loaded, and not if you're downloading the weights or loading the model for the first time.\n\n\tThe time it takes to load the entire pipeline:\n\n\t```py\n \tfrom diffusers import StableDiffusionPipeline\n\n \tpipeline = StableDiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1\", use_safetensors=True)\n \t\"Loaded in safetensors 0:00:02.033658\"\n \t\"Loaded in PyTorch 0:00:02.663379\"\n\t```\n\n\tBut the actual time it takes to lo..."], "test_settings": "chunk_7000_embeddings_openai-text-embedding-3-small_reader-model_gpt-4", "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response correctly identifies the `.safetensors` format as a more secure option for saving and storing PyTorch model weights compared to `.bin` files. However, the inclusion of \"[\ubb38\uc11c 0]\" is unnecessary and does not contribute to the accuracy or factual nature of the response. Despite this minor issue, the core information provided is accurate and aligns with the criteria for a correct answer. Therefore, the response is mostly accurate and factual, but the extraneous text slightly detracts from its precision."}, {"question": "What type of security certification does Hugging Face have?\n", "true_answer": "SOC2 Type 2 certified", "source_doc": "huggingface/hub-docs/blob/main/docs/hub/security.md", "generated_answer": "Hugging Face has SOC2 Type 2 certification [\ubb38\uc11c 0, \ubb38\uc11c 1].", "retrieved_docs": ["Security\n\nThe Hugging Face Hub offers several security features to ensure that your code and data are secure. Beyond offering [private repositories](./repositories-settings#private-repositories) for models, datasets, and Spaces, the Hub supports access tokens, commit signatures, and malware scanning.\n\nHugging Face is GDPR compliant. If a contract or specific data storage is something you'll need, we recommend taking a look at our [Expert Acceleration Program](https://huggingface.co/support). Hugging Face can also offer Business Associate Addendums or GDPR data processing agreements through an [Enterprise Plan](https://huggingface.co/pricing). \n\nHugging Face is also [SOC2 Type 2 certified](https://us.aicpa.org/interestareas/frc/assuranceadvisoryservices/aicpasoc2report.html), meaning we provide security certification to our customers and actively monitor and patch any security weaknesses.\n\n<img width=\"150\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-soc-1.jpg\">\n\nFor any other security questions, please feel free to send us an email at security@huggingface.co.\n\n## Contents\n\n- [User Access Tokens](./security-tokens)\n- [Git over SSH](./security-git-ssh)\n- [Signing commits with GPG](./security-gpg)\n- [Single Sign-On (SSO)](./security-sso)\n- [Malware Scanning](./security-malware)\n- [Pickle Scanning](./security-pickle)\n- [Secrets Scanning](./security-secrets)", "Security & Compliance\n\n\ud83e\udd17 Inference Endpoints is built with security and secure inference at its core. Below you can find an overview of the security measures we have in place.\n\n## Data Security/Privacy\n\nHugging Face does not store any customer data in terms of payloads or tokens that are passed to the Inference Endpoint. We are storing logs for 30 days. Every Inference Endpoints uses TLS/SSL to encrypt the data in transit.\n\nWe also recommend using AWS or Azure Private Link for organizations. This allows you to access your Inference Endpoint through a private connection, without exposing it to the internet.\n\nHugging Face also offers Business Associate Addendum or GDPR data processing agreement through the Inference Endpoint enterprise plan.\n\n## Model Security/Privacy:\n\nYou can set a model repository as private if you do not want to publicly expose it. Hugging Face does not own any model or data you upload to the Hugging Face hub. Hugging Face does provide malware and pickle scans over the contents of the model repository as with all items in the Hub.\n\n## Inference Endpoints and Hub Security\n\nThe Hugging Face Hub, which Inference Endpoints is part, is also SOC2 Type 2 certified. The Hugging Face Hub offers Role Based Access Control. For more on hub security: https://huggingface.co/docs/hub/security\n\n<img width=\"150\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-soc-1.jpg\">\n\n## Inference Endpoint Security level\n\nWe currently offer three types of endpoints, in order or increasing security level:\n\n- **Public**: A Public Endpoint is available from the internet, secured with TLS/SSL, and requires no authentication.\n- **Protected**: A Protected Endpoint is available from the internet, secured with TLS/SSL, and requires a valid Hugging Face token for authentication.\n- **Private** A Private Endpoint is only available through an intra-region secured AWS or Azure PrivateLink connection. Private Endpoints are not accessible from the internet.\n\nPublic and Protected Endpoints do not require any additional configuration. For Private Endpoints, you need to provide the AWS account ID of the account which also should have access to \ud83e\udd17 Inference Endpoints.\n\n<img\n  src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/endpoint_types.png\"\n  alt=\"endpoint types\"\n/>\n\nHugging Face Privacy Policy - https://huggingface.co/privacy", "Security Policy\n\n## Reporting a Vulnerability\n\n\ud83e\udd17 We have our bug bounty program set up with HackerOne. Please feel free to submit vulnerability reports to our private program at https://hackerone.com/hugging_face.\nNote that you'll need to be invited to our program, so send us a quick email at security@huggingface.co if you've found a vulnerability."], "test_settings": "chunk_7000_embeddings_openai-text-embedding-3-small_reader-model_gpt-4", "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response accurately identifies the type of security certification that Hugging Face has, which is SOC2 Type 2. The inclusion of \"[\ubb38\uc11c 0, \ubb38\uc11c 1]\" does not detract from the correctness or accuracy of the response, as it does not alter the factual content. The response is fully aligned with the criteria for a 5-point score, as it is entirely correct, accurate, and factual."}, {"question": "What do RAG models combine to generate outputs?\n", "true_answer": "Pretrained dense retrieval (DPR) and sequence-to-sequence models.", "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/rag.md", "generated_answer": "RAG models combine pretrained dense retrieval (DPR) and sequence-to-sequence (seq2seq) models to generate outputs. They retrieve documents, pass them to a seq2seq model, and then marginalize to generate outputs. [\ubb38\uc11c 0]", "retrieved_docs": ["!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# RAG\n\n<div class=\"flex flex-wrap space-x-1\">\n<a href=\"https://huggingface.co/models?filter=rag\">\n<img alt=\"Models\" src=\"https://img.shields.io/badge/All_model_pages-rag-blueviolet\">\n</a>\n</div>\n\n## Overview\n\nRetrieval-augmented generation (\"RAG\") models combine the powers of pretrained dense retrieval (DPR) and\nsequence-to-sequence models. RAG models retrieve documents, pass them to a seq2seq model, then marginalize to generate\noutputs. The retriever and seq2seq modules are initialized from pretrained models, and fine-tuned jointly, allowing\nboth retrieval and generation to adapt to downstream tasks.\n\nIt is based on the paper [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401) by Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir\nKarpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, Sebastian Riedel, Douwe Kiela.\n\nThe abstract from the paper is the following:\n\n*Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve\nstate-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely\nmanipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind\ntask-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge\nremain open research problems. Pre-trained models with a differentiable access mechanism to explicit nonparametric\nmemory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a\ngeneral-purpose fine-tuning recipe for retrieval-augmented generation (RAG) \u2014 models which combine pre-trained\nparametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a\npre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a\npre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages\nacross the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our\nmodels on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks,\noutperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation\ntasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art\nparametric-only seq2seq baseline.*\n\nThis model was contributed by [ola13](https://huggingface.co/ola13).\n\n## Usage tips\n\nRetrieval-augmented generation (\"RAG\") models combine the powers of pretrained dense retrieval (DPR) and Seq2Seq models. \nRAG models retrieve docs, pass them to a seq2seq model, then marginalize to generate outputs. The retriever and seq2seq \nmodules are initialized from pretrained models, and fine-tuned jointly, allowing both retrieval and generation to adapt \nto downstream tasks.\n\n## RagConfig\n\n[[autodoc]] RagConfig\n\n## RagTokenizer\n\n[[autodoc]] RagTokenizer\n\n## Rag specific outputs\n\n[[autodoc]] models.rag.modeling_rag.RetrievAugLMMarginOutput\n\n[[autodoc]] models.rag.modeling_rag.RetrievAugLMOutput\n\n## RagRetriever\n\n[[autodoc]] RagRetriever\n\n<frameworkcontent>\n<pt>\n\n## RagModel\n\n[[autodoc]] RagModel\n    - forward\n\n## RagSequenceForGeneration\n\n[[autodoc]] RagSequenceForGeneration\n    - forward\n    - generate\n\n## RagTokenForGeneration\n\n[[autodoc]] RagTokenForGeneration\n    - forward\n    - generate\n\n</pt>\n<tf>\n\n## TFRagModel\n\n[[autodoc]] TFRagModel\n    - call\n\n## TFRagSequenceForGeneration\n\n[[autodoc]] TFRagSequenceForGeneration\n    - call\n    - generate\n\n## TFRagTokenForGeneration\n\n[[autodoc]] TFRagTokenForGeneration\n    - call\n    - generate\n\n</tf>\n</frameworkcontent>", "Intro\n\nAuthors: @patrickvonplaten and @lhoestq\n\nAimed at tackling the knowledge-intensive NLP tasks (think tasks a human wouldn't be expected to solve without access to external knowledge sources), RAG models are seq2seq models with access to a retrieval mechanism providing relevant context documents at training and evaluation time.\n\nA RAG model encapsulates two core components: a question encoder and a generator.\nDuring a forward pass, we encode the input with the question encoder and pass it\nto the retriever to extract relevant context documents. The documents are then prepended to the input.\nSuch contextualized inputs are passed to the generator.\n\nRead more about RAG  at https://arxiv.org/abs/2005.11401.\n\n# Note\n\n\u26a0\ufe0f This project should be run with pytorch-lightning==1.3.1 which has a potential security vulnerability\n\n# Finetuning\n\nOur finetuning logic is based on scripts from [`examples/legacy/seq2seq`](https://github.com/huggingface/transformers/tree/main/examples/legacy/seq2seq). We accept training data in the same format as specified there - we expect a directory consisting of 6 text files:\n```bash\ntrain.source\ntrain.target\nval.source\nval.target\ntest.source\ntest.target\n```\n\nA sample finetuning command (run ` ./examples/research_projects/rag/finetune_rag.py --help` to list all available options):\n\n```bash\npython examples/research_projects/rag/finetune_rag.py \\\n    --data_dir $DATA_DIR \\\n    --output_dir $OUTPUT_DIR \\\n    --model_name_or_path $MODEL_NAME_OR_PATH \\\n    --model_type rag_sequence \\\n    --fp16 \\\n    --gpus 8\n```\nWe publish two `base` models which can serve as a starting point for finetuning on downstream tasks (use them as `model_name_or_path`):\n- [`facebook/rag-sequence-base`](https://huggingface.co/facebook/rag-sequence-base) - a base for finetuning `RagSequenceForGeneration` models,\n- [`facebook/rag-token-base`](https://huggingface.co/facebook/rag-token-base) - a base for finetuning `RagTokenForGeneration` models.\n\nThe `base` models initialize the question encoder with [`facebook/dpr-question_encoder-single-nq-base`](https://huggingface.co/facebook/dpr-question_encoder-single-nq-base) and the generator with [`facebook/bart-large`](https://huggingface.co/facebook/bart-large).\n\nIf you would like to initialize finetuning with a base model using different question encoder and generator architectures, you can build it with a consolidation script, e.g.:\n```\npython examples/research_projects/rag/consolidate_rag_checkpoint.py \\\n    --model_type rag_sequence \\\n    --generator_name_or_path facebook/bart-large-cnn \\\n    --question_encoder_name_or_path facebook/dpr-question_encoder-single-nq-base \\\n    --dest path/to/checkpoint\n```\nYou will then be able to pass `path/to/checkpoint` as `model_name_or_path` to the `finetune_rag.py` script.\n\n## Document Retrieval\nWhen running distributed fine-tuning, each training worker needs to retrieve contextual documents\nfor its input by querying a index loaded into memory. RAG provides two implementations for document retrieval,\none with [`torch.distributed`](https://pytorch.org/docs/stable/distributed.html) communication package and the other\nwith [`Ray`](https://docs.ray.io/en/master/).\n\nThis option can be configured with the `--distributed_retriever` flag which can either be set to `pytorch` or `ray`.\nBy default this flag is set to `pytorch`.\n\nFor the Pytorch implementation, only training worker 0 loads the index into CPU memory, and a gather/scatter pattern is used\nto collect the inputs from the other training workers and send back the corresponding document embeddings.\n\nFor the Ray implementation, the index is loaded in *separate* process(es). The training workers randomly select which\nretriever worker to query. To use Ray for distributed retrieval, you have to set the `--distributed_retriever` arg to `ray`.\nTo configure the number of retrieval workers (the number of processes that load the index), you can set the `num_retrieval_workers` flag.\nAlso make sure to start the Ray cluster before running fine-tuning.\n\n```bash\n# Start a single-node Ray cluster.\nray start --head\n\npython examples/research_projects/rag/finetune_rag.py \\\n    --data_dir $DATA_DIR \\\n    --output_dir $OUTPUT_DIR \\\n    --model_name_or_path $MODEL_NAME_OR_PATH \\\n    --model_type rag_sequence \\\n    --fp16 \\\n    --gpus 8\n    --distributed_retriever ray \\\n    --num_retrieval_workers 4\n\n# Stop the ray cluster once fine-tuning has finished.\nray stop\n```\n\nUsing Ray can lead to retrieval speedups on multi-GPU settings since multiple processes load the index rather than\njust the rank 0 training worker. Using Ray also allows you to load the index on GPU since the index is loaded on a separate\nprocesses than the model, while with pytorch distributed retrieval, both are loaded in the same process potentially leading to GPU OOM.\n\n# Evaluation\nOur evaluation script enables two modes of evaluation (controlled by the `eval_mode` argument): `e2e` - end2end evaluation, returns EM (exact match) and F1 scores ...", "End-to-End finetuning of RAG (including DPR retriever) for Question Answering.\n\nThis finetuning script is actively maintained by [Shamane Siri](https://github.com/shamanez). Feel free to ask questions on the [Forum](https://discuss.huggingface.co/) or post an issue on [GitHub](https://github.com/huggingface/transformers/issues/new/choose) and tag @shamanez.\n\nOthers that helped out: Patrick von Platen (@patrickvonplaten), Quentin Lhoest (@lhoestq), and Rivindu Weerasekera (@rivinduw)\n\nThe original RAG implementation is able to train the question encoder and generator end-to-end.\nThis extension enables complete end-to-end training of RAG including the context encoder in the retriever component.\nPlease read the [accompanying blog post](https://shamanesiri.medium.com/how-to-finetune-the-entire-rag-architecture-including-dpr-retriever-4b4385322552) for details on this implementation.\n\nThe original RAG code has also been modified to work with the latest versions of pytorch lightning (version 1.2.10) and RAY (version 1.3.0). All other implementation details remain the same as the [original RAG code](https://github.com/huggingface/transformers/tree/main/examples/research_projects/rag).\nRead more about RAG  at https://arxiv.org/abs/2005.11401.\n\nThis code can be modified to experiment with other research on retrival augmented models which include training of the retriever (e.g. [REALM](https://arxiv.org/abs/2002.08909) and [MARGE](https://arxiv.org/abs/2006.15020)).\n\nTo start training, use the bash script (finetune_rag_ray_end2end.sh) in this folder. This script also includes descriptions on each command-line argument used.\n\n# Latest Update\n\n\u26a0\ufe0f Updated the rag-end2end-retriever to be compatible with PL==1.6.4 and RAY==1.13.0 (latest versions to the date 2022-June-11)\n\n# Note\n\n\u26a0\ufe0f This project should be run with pytorch-lightning==1.3.1 which has a potential security vulnerability\n\n# Testing\n\nThe following two bash scripts can be used to quickly test the implementation.\n1. sh ./test_run/test_finetune.sh script\n    - Tests the full end-to-end fine-tuning ability with a dummy knowlendge-base and dummy training dataset (check test_dir directory).\n    - Users can replace the dummy dataset and knowledge-base with their own to do their own finetuning.\n    - Please read the comments in the test_finetune.sh file.\n2. sh ./test_run/test_rag_new_features.sh\n    - Tests the newly added functions (set_context_encoder and set_context_encoder_tokenizer) related to modeling rag.\n    - This is sufficient to check the model's ability to use the set functions correctly.\n\n\n\n# Comparison of end2end RAG (including DPR finetuning)  VS original-RAG\n\nWe conducted a simple experiment to investigate the effectiveness of this end2end training extension using the SQuAD dataset. Please execute the following steps to reproduce the results.\n\n-   Create a knowledge-base using all the context passages in the SQuAD dataset with their respective titles.\n-   Use the question-answer pairs as training data.\n-   Train the system for 10 epochs.\n-   Test the Exact Match (EM) score with the SQuAD dataset's validation set.\n-   Training dataset, the knowledge-base, and hyperparameters used in experiments can be accessed from [here](https://drive.google.com/drive/folders/1qyzV-PaEARWvaU_jjpnU_NUS3U_dSjtG?usp=sharing).\n\n# Results\n\n- We train both models for 10 epochs.\n\n| Model Type          | EM-Score|\n| --------------------| --------|\n| RAG-original        | 28.12   |\n| RAG-end2end with DPR| 40.02   |"], "test_settings": "chunk_7000_embeddings_openai-text-embedding-3-small_reader-model_gpt-4", "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response accurately identifies the two main components of RAG models: pretrained dense retrieval (DPR) and sequence-to-sequence (seq2seq) models. It also provides additional context by explaining the process of retrieving documents, passing them to a seq2seq model, and marginalizing to generate outputs. This additional detail, while not necessary for a perfect score, does not detract from the accuracy or factual nature of the response. The response is entirely correct and aligns well with the criteria for a 5-point score."}, {"question": "What library does MarkupLMFeatureExtractor use to extract data from HTML and XML files?\n", "true_answer": "Beautiful Soup", "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/markuplm.md", "generated_answer": "MarkupLMFeatureExtractor uses Beautiful Soup to extract data from HTML and XML files. [\ubb38\uc11c 0]", "retrieved_docs": ["!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# MarkupLM\n\n## Overview\n\nThe MarkupLM model was proposed in [MarkupLM: Pre-training of Text and Markup Language for Visually-rich Document\nUnderstanding](https://arxiv.org/abs/2110.08518) by Junlong Li, Yiheng Xu, Lei Cui, Furu Wei. MarkupLM is BERT, but\napplied to HTML pages instead of raw text documents. The model incorporates additional embedding layers to improve\nperformance, similar to [LayoutLM](layoutlm).\n\nThe model can be used for tasks like question answering on web pages or information extraction from web pages. It obtains\nstate-of-the-art results on 2 important benchmarks:\n- [WebSRC](https://x-lance.github.io/WebSRC/), a dataset for Web-Based Structural Reading Comprehension (a bit like SQuAD but for web pages)\n- [SWDE](https://www.researchgate.net/publication/221299838_From_one_tree_to_a_forest_a_unified_solution_for_structured_web_data_extraction), a dataset\nfor information extraction from web pages (basically named-entity recogntion on web pages)\n\nThe abstract from the paper is the following:\n\n*Multimodal pre-training with text, layout, and image has made significant progress for Visually-rich Document\nUnderstanding (VrDU), especially the fixed-layout documents such as scanned document images. While, there are still a\nlarge number of digital documents where the layout information is not fixed and needs to be interactively and\ndynamically rendered for visualization, making existing layout-based pre-training approaches not easy to apply. In this\npaper, we propose MarkupLM for document understanding tasks with markup languages as the backbone such as\nHTML/XML-based documents, where text and markup information is jointly pre-trained. Experiment results show that the\npre-trained MarkupLM significantly outperforms the existing strong baseline models on several document understanding\ntasks. The pre-trained model and code will be publicly available.*\n\nThis model was contributed by [nielsr](https://huggingface.co/nielsr). The original code can be found [here](https://github.com/microsoft/unilm/tree/master/markuplm).\n\n## Usage tips\n\n- In addition to `input_ids`, [`~MarkupLMModel.forward`] expects 2 additional inputs, namely `xpath_tags_seq` and `xpath_subs_seq`.\nThese are the XPATH tags and subscripts respectively for each token in the input sequence.\n- One can use [`MarkupLMProcessor`] to prepare all data for the model. Refer to the [usage guide](#usage-markuplmprocessor) for more info.\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/markuplm_architecture.jpg\"\nalt=\"drawing\" width=\"600\"/> \n\n<small> MarkupLM architecture. Taken from the <a href=\"https://arxiv.org/abs/2110.08518\">original paper.</a> </small>\n\n## Usage: MarkupLMProcessor\n\nThe easiest way to prepare data for the model is to use [`MarkupLMProcessor`], which internally combines a feature extractor\n([`MarkupLMFeatureExtractor`]) and a tokenizer ([`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`]). The feature extractor is\nused to extract all nodes and xpaths from the HTML strings, which are then provided to the tokenizer, which turns them into the\ntoken-level inputs of the model (`input_ids` etc.). Note that you can still use the feature extractor and tokenizer separately,\nif you only want to handle one of the two tasks.\n\n```python\nfrom transformers import MarkupLMFeatureExtractor, MarkupLMTokenizerFast, MarkupLMProcessor\n\nfeature_extractor = MarkupLMFeatureExtractor()\ntokenizer = MarkupLMTokenizerFast.from_pretrained(\"microsoft/markuplm-base\")\nprocessor = MarkupLMProcessor(feature_extractor, tokenizer)\n```\n\nIn short, one can provide HTML strings (and possibly additional data) to [`MarkupLMProcessor`],\nand it will create the inputs expected by the model. Internally, the processor first uses\n[`MarkupLMFeatureExtractor`] to get a list of nodes and corresponding xpaths. The nodes and\nxpaths are then provided to [`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`], which converts them\nto token-level `input_ids`, `attention_mask`, `token_type_ids`, `xpath_subs_seq`, `xpath_tags_seq`.\nOptionally, one can provide node labels to the processor, which are turned into token-level `labels`.\n\n[`MarkupLMFeatureExtractor`] uses [Beautiful Soup](https://www.crum...", "!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Feature Extractor\n\nA feature extractor is in charge of preparing input features for audio or vision models. This includes feature extraction from sequences, e.g., pre-processing audio files to generate Log-Mel Spectrogram features, feature extraction from images, e.g., cropping image files, but also padding, normalization, and conversion to NumPy, PyTorch, and TensorFlow tensors.\n\n\n## FeatureExtractionMixin\n\n[[autodoc]] feature_extraction_utils.FeatureExtractionMixin\n    - from_pretrained\n    - save_pretrained\n\n## SequenceFeatureExtractor\n\n[[autodoc]] SequenceFeatureExtractor\n    - pad\n\n## BatchFeature\n\n[[autodoc]] BatchFeature\n\n## ImageFeatureExtractionMixin\n\n[[autodoc]] image_utils.ImageFeatureExtractionMixin", "**Use case 2: web page classification (training, inference) + token classification (inference), parse_html=False**\n\nIn case one already has obtained all nodes and xpaths, one doesn't need the feature extractor. In that case, one should\nprovide the nodes and corresponding xpaths themselves to the processor, and make sure to set `parse_html` to `False`.\n\n```python\n>>> from transformers import MarkupLMProcessor\n\n>>> processor = MarkupLMProcessor.from_pretrained(\"microsoft/markuplm-base\")\n>>> processor.parse_html = False\n\n>>> nodes = [\"hello\", \"world\", \"how\", \"are\"]\n>>> xpaths = [\"/html/body/div/li[1]/div/span\", \"/html/body/div/li[1]/div/span\", \"html/body\", \"html/body/div\"]\n>>> encoding = processor(nodes=nodes, xpaths=xpaths, return_tensors=\"pt\")\n>>> print(encoding.keys())\ndict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'xpath_tags_seq', 'xpath_subs_seq'])\n```\n\n**Use case 3: token classification (training), parse_html=False**\n\nFor token classification tasks (such as [SWDE](https://paperswithcode.com/dataset/swde)), one can also provide the\ncorresponding node labels in order to train a model. The processor will then convert these into token-level `labels`.\nBy default, it will only label the first wordpiece of a word, and label the remaining wordpieces with -100, which is the\n`ignore_index` of PyTorch's CrossEntropyLoss. In case you want all wordpieces of a word to be labeled, you can\ninitialize the tokenizer with `only_label_first_subword` set to `False`.\n\n```python\n>>> from transformers import MarkupLMProcessor\n\n>>> processor = MarkupLMProcessor.from_pretrained(\"microsoft/markuplm-base\")\n>>> processor.parse_html = False\n\n>>> nodes = [\"hello\", \"world\", \"how\", \"are\"]\n>>> xpaths = [\"/html/body/div/li[1]/div/span\", \"/html/body/div/li[1]/div/span\", \"html/body\", \"html/body/div\"]\n>>> node_labels = [1, 2, 2, 1]\n>>> encoding = processor(nodes=nodes, xpaths=xpaths, node_labels=node_labels, return_tensors=\"pt\")\n>>> print(encoding.keys())\ndict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'xpath_tags_seq', 'xpath_subs_seq', 'labels'])\n```\n\n**Use case 4: web page question answering (inference), parse_html=True**\n\nFor question answering tasks on web pages, you can provide a question to the processor. By default, the\nprocessor will use the feature extractor to get all nodes and xpaths, and create [CLS] question tokens [SEP] word tokens [SEP].\n\n```python\n>>> from transformers import MarkupLMProcessor\n\n>>> processor = MarkupLMProcessor.from_pretrained(\"microsoft/markuplm-base\")\n\n>>> html_string = \"\"\"\n...  <!DOCTYPE html>\n...  <html>\n...  <head>\n...  <title>Hello world</title>\n...  </head>\n...  <body>\n...  <h1>Welcome</h1>\n...  <p>My name is Niels.</p>\n...  </body>\n...  </html>\"\"\"\n\n>>> question = \"What's his name?\"\n>>> encoding = processor(html_string, questions=question, return_tensors=\"pt\")\n>>> print(encoding.keys())\ndict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'xpath_tags_seq', 'xpath_subs_seq'])\n```\n\n**Use case 5: web page question answering (inference), parse_html=False**\n\nFor question answering tasks (such as WebSRC), you can provide a question to the processor. If you have extracted\nall nodes and xpaths yourself, you can provide them directly to the processor. Make sure to set `parse_html` to `False`.\n\n```python\n>>> from transformers import MarkupLMProcessor\n\n>>> processor = MarkupLMProcessor.from_pretrained(\"microsoft/markuplm-base\")\n>>> processor.parse_html = False\n\n>>> nodes = [\"hello\", \"world\", \"how\", \"are\"]\n>>> xpaths = [\"/html/body/div/li[1]/div/span\", \"/html/body/div/li[1]/div/span\", \"html/body\", \"html/body/div\"]\n>>> question = \"What's his name?\"\n>>> encoding = processor(nodes=nodes, xpaths=xpaths, questions=question, return_tensors=\"pt\")\n>>> print(encoding.keys())\ndict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'xpath_tags_seq', 'xpath_subs_seq'])\n```\n\n## Resources\n\n- [Demo notebooks](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/MarkupLM)\n- [Text classification task guide](../tasks/sequence_classification)\n- [Token classification task guide](../tasks/token_classification)\n- [Question answering task guide](../tasks/question_answering)\n\n## MarkupLMConfig\n\n[[autodoc]] MarkupLMConfig\n    - all\n\n## MarkupLMFeatureExtractor\n\n[[autodoc]] MarkupLMFeatureExtractor\n    - __call__\n\n## MarkupLMTokenizer\n\n[[autodoc]] MarkupLMTokenizer\n    - build_inputs_with_special_tokens\n    - get_special_tokens_mask\n    - create_token_type_ids_from_sequences\n    - save_vocabulary\n\n## MarkupLMTokenizerFast\n\n[[autodoc]] MarkupLMTokenizerFast\n    - all\n\n## MarkupLMProcessor\n\n[[autodoc]] MarkupLMProcessor\n    - __call__\n\n## MarkupLMModel\n\n[[autodoc]] MarkupLMModel\n    - forward\n\n## MarkupLMForSequenceClassification\n\n[[autodoc]] MarkupLMForSequenceClassification\n    - forward\n\n## MarkupLMForTokenClassification\n\n[[autodoc]] MarkupLMForTokenClassification\n    - forward\n\n## MarkupLMForQuestionAnswering\n\n[[autodoc]] MarkupLMForQuestionAnswering\n    - forward"], "test_settings": "chunk_7000_embeddings_openai-text-embedding-3-small_reader-model_gpt-4", "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response correctly identifies Beautiful Soup as the library used by MarkupLMFeatureExtractor to extract data from HTML and XML files. However, the additional text \"[\ubb38\uc11c 0]\" is unnecessary and does not contribute to the accuracy or factual nature of the response. Despite this, the core information provided is accurate and aligns with the criteria for a correct answer. Therefore, the response is mostly accurate and factual, but the extraneous text slightly detracts from its precision."}, {"question": "What is the file size limit for syncing to HF Spaces without using Git-LFS?\n", "true_answer": "10MB", "source_doc": "huggingface/hub-docs/blob/main/docs/hub/spaces-github-actions.md", "generated_answer": "The file size limit for syncing to HF Spaces without using Git-LFS is 10MB. [\ubb38\uc11c 0]", "retrieved_docs": ["Managing Spaces with Github Actions\n\nYou can keep your app in sync with your GitHub repository with **Github Actions**. Remember that for files larger than 10MB, Spaces requires Git-LFS. If you don't want to use Git-LFS, you may need to review your files and check your history. Use a tool like [BFG Repo-Cleaner](https://rtyley.github.io/bfg-repo-cleaner/) to remove any large files from your history. BFG Repo-Cleaner will keep a local copy of your repository as a backup.\n\nFirst, you should set up your GitHub repository and Spaces app together. Add your Spaces app as an additional remote to your existing Git repository.\n\n```bash\ngit remote add space https://huggingface.co/spaces/HF_USERNAME/SPACE_NAME\n```\n\nThen force push to sync everything for the first time:\n\n```bash\ngit push --force space main\n```\n\nNext, set up a GitHub Action to push your main branch to Spaces. In the example below:\n\n* Replace `HF_USERNAME` with your username and `SPACE_NAME` with your Space name. \n* Create a [Github secret](https://docs.github.com/en/actions/security-guides/encrypted-secrets#creating-encrypted-secrets-for-an-environment) with your `HF_TOKEN`. You can find your Hugging Face API token under **API Tokens** on your Hugging Face profile.\n\n```yaml\nname: Sync to Hugging Face hub\non:\n  push:\n    branches: [main]\n\n  # to run this workflow manually from the Actions tab\n  workflow_dispatch:\n\njobs:\n  sync-to-hub:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n        with:\n          fetch-depth: 0\n          lfs: true\n      - name: Push to hub\n        env:\n          HF_TOKEN: ${{ secrets.HF_TOKEN }}\n        run: git push https://HF_USERNAME:$HF_TOKEN@huggingface.co/spaces/HF_USERNAME/SPACE_NAME main\n```\n\nFinally, create an Action that automatically checks the file size of any new pull request:\n\n\n```yaml\nname: Check file size\non:               # or directly `on: [push]` to run the action on every push on any branch\n  pull_request:\n    branches: [main]\n\n  # to run this workflow manually from the Actions tab\n  workflow_dispatch:\n\njobs:\n  sync-to-hub:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Check large files\n        uses: ActionsDesk/lfs-warning@v2.0\n        with:\n          filesizelimit: 10485760 # this is 10MB so we can sync to HF Spaces\n```", "Repository limitations and recommendations\n\nThere are some limitations to be aware of when dealing with a large amount of data in your repo. Given the time it takes to stream the data,\ngetting an upload/push to fail at the end of the process or encountering a degraded experience, be it on hf.co or when working locally, can be very annoying.\n\n## Recommendations\n\nWe gathered a list of tips and recommendations for structuring your repo. If you are looking for more practical tips, check out [this guide](https://huggingface.co/docs/huggingface_hub/main/en/guides/upload#practical-tips) on how to upload large amount of data using the Python library.\n\n\n| Characteristic     | Recommended        | Tips                                                   |\n| ----------------   | ------------------ | ------------------------------------------------------ |\n| Repo size          | -                  | contact us for large repos (TBs of data)               |\n| Files per repo     | <100k              | merge data into fewer files                            |\n| Entries per folder | <10k               | use subdirectories in repo                             |\n| File size          | <5GB               | split data into chunked files                          |\n| Commit size        | <100 files*        | upload files in multiple commits                       |\n| Commits per repo   | -                  | upload multiple files per commit and/or squash history |\n\n_* Not relevant when using `git` CLI directly_\n\nPlease read the next section to understand better those limits and how to deal with them.\n\n## Explanations\n\nWhat are we talking about when we say \"large uploads\", and what are their associated limitations? Large uploads can be\nvery diverse, from repositories with a few huge files (e.g. model weights) to repositories with thousands of small files\n(e.g. an image dataset).\n\nUnder the hood, the Hub uses Git to version the data, which has structural implications on what you can do in your repo.\nIf your repo is crossing some of the numbers mentioned in the previous section, **we strongly encourage you to check out [`git-sizer`](https://github.com/github/git-sizer)**,\nwhich has very detailed documentation about the different factors that will impact your experience. Here is a TL;DR of factors to consider:\n\n- **Repository size**: The total size of the data you're planning to upload. There is no hard limit on a Hub repository size. However, if you plan to upload hundreds of GBs or even TBs of data, we would appreciate it if you could let us know in advance so we can better help you if you have any questions during the process. You can contact us at datasets@huggingface.co or on [our Discord](http://hf.co/join/discord).\n- **Number of files**:\n    - For optimal experience, we recommend keeping the total number of files under 100k. Try merging the data into fewer files if you have more.\n      For example, json files can be merged into a single jsonl file, or large datasets can be exported as Parquet files.\n    - The maximum number of files per folder cannot exceed 10k files per folder. A simple solution is to\n      create a repository structure that uses subdirectories. For example, a repo with 1k folders from `000/` to `999/`, each containing at most 1000 files, is already enough.\n- **File size**: In the case of uploading large files (e.g. model weights), we strongly recommend splitting them **into chunks of around 5GB each**.\nThere are a few reasons for this:\n    - Uploading and downloading smaller files is much easier both for you and the other users. Connection issues can always\n      happen when streaming data and smaller files avoid resuming from the beginning in case of errors.\n    - Files are served to the users using CloudFront. From our experience, huge files are not cached by this service\n      leading to a slower download speed.\nIn all cases no single LFS file will be able to be >50GB. I.e. 50GB is the hard limit for single file size.\n- **Number of commits**: There is no hard limit for the total number of commits on your repo history. However, from\nour experience, the user experience on the Hub starts to degrade after a few thousand commits. We are constantly working to\nimprove the service, but one must always remember that a git repository is not meant to work as a database with a lot of\nwrites. If your repo's history gets very large, it is always possible to squash all the commits to get a\nfresh start using `huggingface_hub`'s [`super_squash_history`](https://huggingface.co/docs/huggingface_hub/main/en/package_reference/hf_api#huggingface_hub.HfApi.super_squash_history). Be aware that this is a non-revertible operation.\n- **Number of operations per commit**: Once again, there is no hard limit here. When a commit is uploaded on the Hub, each\ngit operation (addition or delete) is checked by the server. When a hundred LFS files are committed at once,\neach file is checked individually to ensure it's been correctly uploaded. When...", "Managing Spaces with CircleCI Workflows\n\nYou can keep your app in sync with your GitHub repository with a **CircleCI workflow**. \n\n[CircleCI](https://circleci.com) is a continuous integration and continuous delivery (CI/CD) platform that helps automate the software development process. A [CircleCI workflow](https://circleci.com/docs/workflows/) is a set of automated tasks defined in a configuration file, orchestrated by CircleCI, to streamline the process of building, testing, and deploying software applications.\n\n*Note: For files larger than 10MB, Spaces requires Git-LFS. If you don't want to use Git-LFS, you may need to review your files and check your history. Use a tool like [BFG Repo-Cleaner](https://rtyley.github.io/bfg-repo-cleaner/) to remove any large files from your history. BFG Repo-Cleaner will keep a local copy of your repository as a backup.*\n\nFirst, set up your GitHub repository and Spaces app together. Add your Spaces app as an additional remote to your existing Git repository.\n\n```bash\ngit remote add space https://huggingface.co/spaces/HF_USERNAME/SPACE_NAME\n```\n\nThen force push to sync everything for the first time:\n\n```bash\ngit push --force space main\n```\n\nNext, set up a [CircleCI workflow](https://circleci.com/docs/workflows/) to push your `main` git branch to Spaces. \n\nIn the example below:\n\n* Replace `HF_USERNAME` with your username and `SPACE_NAME` with your Space name. \n* [Create a context in CircleCI](https://circleci.com/docs/contexts/) and add an env variable into it called *HF_PERSONAL_TOKEN* (you can give it any name, use the key you create in place of HF_PERSONAL_TOKEN) and the value as your Hugging Face API token. You can find your Hugging Face API token under **API Tokens** on [your Hugging Face profile](https://huggingface.co/settings/tokens).\n\n```yaml\nversion: 2.1\n\nworkflows:\n  main:\n    jobs:\n      - sync-to-huggingface:\n          context:\n            - HuggingFace\n          filters:\n            branches:\n              only:\n                - main\n\njobs:\n  sync-to-huggingface:\n    docker:\n      - image: alpine\n    resource_class: small\n    steps:\n      - run: \n          name: install git\n          command: apk update && apk add openssh-client git\n      - checkout\n      - run:\n          name: push to Huggingface hub\n          command: |\n                  git config user.email \"<your-email@here>\" \n                  git config user.name \"<your-identifier>\" \n                  git push -f https://HF_USERNAME:${HF_PERSONAL_TOKEN}@huggingface.co/spaces/HF_USERNAME/SPACE_NAME main\n```"], "test_settings": "chunk_7000_embeddings_openai-text-embedding-3-small_reader-model_gpt-4", "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response correctly states the file size limit for syncing to HF Spaces without using Git-LFS as 10MB, which matches the information provided in the criteria for a perfect answer. However, the inclusion of \"[\ubb38\uc11c 0]\" is unnecessary and does not contribute to the accuracy or clarity of the response. Despite this minor issue, the core information is accurate and factual."}, {"question": "What is the title of the paper introducing the ByT5 model?\n", "true_answer": "ByT5: Towards a token-free future with pre-trained byte-to-byte models", "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/byt5.md", "generated_answer": "The title of the paper introducing the ByT5 model is \"ByT5: Towards a token-free future with pre-trained byte-to-byte models\" [0].", "retrieved_docs": ["!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# ByT5\n\n## Overview\n\nThe ByT5 model was presented in [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626) by Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir\nKale, Adam Roberts, Colin Raffel.\n\nThe abstract from the paper is the following:\n\n*Most widely-used pre-trained language models operate on sequences of tokens corresponding to word or subword units.\nEncoding text as a sequence of tokens requires a tokenizer, which is typically created as an independent artifact from\nthe model. Token-free models that instead operate directly on raw text (bytes or characters) have many benefits: they\ncan process text in any language out of the box, they are more robust to noise, and they minimize technical debt by\nremoving complex and error-prone text preprocessing pipelines. Since byte or character sequences are longer than token\nsequences, past work on token-free models has often introduced new model architectures designed to amortize the cost of\noperating directly on raw text. In this paper, we show that a standard Transformer architecture can be used with\nminimal modifications to process byte sequences. We carefully characterize the trade-offs in terms of parameter count,\ntraining FLOPs, and inference speed, and show that byte-level models are competitive with their token-level\ncounterparts. We also demonstrate that byte-level models are significantly more robust to noise and perform better on\ntasks that are sensitive to spelling and pronunciation. As part of our contribution, we release a new set of\npre-trained byte-level Transformer models based on the T5 architecture, as well as all code and data used in our\nexperiments.*\n\nThis model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten). The original code can be\nfound [here](https://github.com/google-research/byt5).\n\n<Tip>\n\nByT5's architecture is based on the T5v1.1 model, refer to [T5v1.1's documentation page](t5v1.1) for the API reference. They\nonly differ in how inputs should be prepared for the model, see the code examples below.\n\n</Tip>\n\nSince ByT5 was pre-trained unsupervisedly, there's no real advantage to using a task prefix during single-task\nfine-tuning. If you are doing multi-task fine-tuning, you should use a prefix.\n\n\n## Usage example\n\nByT5 works on raw UTF-8 bytes, so it can be used without a tokenizer:\n\n```python\n>>> from transformers import T5ForConditionalGeneration\n>>> import torch\n\n>>> model = T5ForConditionalGeneration.from_pretrained(\"google/byt5-small\")\n\n>>> num_special_tokens = 3\n>>> # Model has 3 special tokens which take up the input ids 0,1,2 of ByT5.\n>>> # => Need to shift utf-8 character encodings by 3 before passing ids to model.\n\n>>> input_ids = torch.tensor([list(\"Life is like a box of chocolates.\".encode(\"utf-8\"))]) + num_special_tokens\n\n>>> labels = torch.tensor([list(\"La vie est comme une bo\u00eete de chocolat.\".encode(\"utf-8\"))]) + num_special_tokens\n\n>>> loss = model(input_ids, labels=labels).loss\n>>> loss.item()\n2.66\n```\n\nFor batched inference and training it is however recommended to make use of the tokenizer:\n\n```python\n>>> from transformers import T5ForConditionalGeneration, AutoTokenizer\n\n>>> model = T5ForConditionalGeneration.from_pretrained(\"google/byt5-small\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/byt5-small\")\n\n>>> model_inputs = tokenizer(\n...     [\"Life is like a box of chocolates.\", \"Today is Monday.\"], padding=\"longest\", return_tensors=\"pt\"\n... )\n>>> labels_dict = tokenizer(\n...     [\"La vie est comme une bo\u00eete de chocolat.\", \"Aujourd'hui c'est lundi.\"], padding=\"longest\", return_tensors=\"pt\"\n... )\n>>> labels = labels_dict.input_ids\n\n>>> loss = model(**model_inputs, labels=labels).loss\n>>> loss.item()\n17.9\n```\n\nSimilar to [T5](t5), ByT5 was trained on the span-mask denoising task. However, \nsince the model works directly on characters, the pretraining task is a bit \ndifferent. Let's corrupt some characters of the \ninput sentence `\"The dog chases a ball in the park.\"` and ask ByT5 to predict them \nfor us.\n\n```python\n>>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/byt5-base\")\n>...", "!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# T5v1.1\n\n## Overview\n\nT5v1.1 was released in the [google-research/text-to-text-transfer-transformer](https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md#t511)\nrepository by Colin Raffel et al. It's an improved version of the original T5 model.\nThis model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten). The original code can be\nfound [here](https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md#t511).\n\n## Usage tips\n\nOne can directly plug in the weights of T5v1.1 into a T5 model, like so:\n\n```python\n>>> from transformers import T5ForConditionalGeneration\n\n>>> model = T5ForConditionalGeneration.from_pretrained(\"google/t5-v1_1-base\")\n```\n\nT5 Version 1.1 includes the following improvements compared to the original T5 model:\n\n- GEGLU activation in the feed-forward hidden layer, rather than ReLU. See [this paper](https://arxiv.org/abs/2002.05202).\n\n- Dropout was turned off in pre-training (quality win). Dropout should be re-enabled during fine-tuning.\n\n- Pre-trained on C4 only without mixing in the downstream tasks.\n\n- No parameter sharing between the embedding and classifier layer.\n\n- \"xl\" and \"xxl\" replace \"3B\" and \"11B\". The model shapes are a bit different - larger `d_model` and smaller\n  `num_heads` and `d_ff`.\n\nNote: T5 Version 1.1 was only pre-trained on [C4](https://huggingface.co/datasets/c4) excluding any supervised\ntraining. Therefore, this model has to be fine-tuned before it is usable on a downstream task, unlike the original T5\nmodel. Since t5v1.1 was pre-trained unsupervisedly, there's no real advantage to using a task prefix during single-task\nfine-tuning. If you are doing multi-task fine-tuning, you should use a prefix.\n\nGoogle has released the following variants:\n\n- [google/t5-v1_1-small](https://huggingface.co/google/t5-v1_1-small)\n\n- [google/t5-v1_1-base](https://huggingface.co/google/t5-v1_1-base)\n\n- [google/t5-v1_1-large](https://huggingface.co/google/t5-v1_1-large)\n\n- [google/t5-v1_1-xl](https://huggingface.co/google/t5-v1_1-xl)\n\n- [google/t5-v1_1-xxl](https://huggingface.co/google/t5-v1_1-xxl).\n\n\n<Tip>\n\nRefer to [T5's documentation page](t5) for all API reference, tips, code examples and notebooks.\n\n</Tip>", "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# LongT5\n\n## Overview\n\nThe LongT5 model was proposed in [LongT5: Efficient Text-To-Text Transformer for Long Sequences](https://arxiv.org/abs/2112.07916)\nby Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung and Yinfei Yang. It's an\nencoder-decoder transformer pre-trained in a text-to-text denoising generative setting. LongT5 model is an extension of\nT5 model, and it enables using one of the two different efficient attention mechanisms - (1) Local attention, or (2)\nTransient-Global attention.\n\n\nThe abstract from the paper is the following:\n\n*Recent work has shown that either (1) increasing the input length or (2) increasing model size can improve the\nperformance of Transformer-based neural models. In this paper, we present a new model, called LongT5, with which we\nexplore the effects of scaling both the input length and model size at the same time. Specifically, we integrated\nattention ideas from long-input transformers (ETC), and adopted pre-training strategies from summarization pre-training\n(PEGASUS) into the scalable T5 architecture. The result is a new attention mechanism we call {\\em Transient Global}\n(TGlobal), which mimics ETC's local/global attention mechanism, but without requiring additional side-inputs. We are\nable to achieve state-of-the-art results on several summarization tasks and outperform the original T5 models on\nquestion answering tasks.*\n\nThis model was contributed by [stancld](https://huggingface.co/stancld).\nThe original code can be found [here](https://github.com/google-research/longt5).\n\n## Usage tips\n\n- [`LongT5ForConditionalGeneration`] is an extension of [`T5ForConditionalGeneration`] exchanging the traditional\nencoder *self-attention* layer with efficient either *local* attention or *transient-global* (*tglobal*) attention.\n- Unlike the T5 model, LongT5 does not use a task prefix. Furthermore, it uses a different pre-training objective\ninspired by the pre-training of [`PegasusForConditionalGeneration`].\n- LongT5 model is designed to work efficiently and very well on long-range *sequence-to-sequence* tasks where the\ninput sequence exceeds commonly used 512 tokens. It is capable of handling input sequences of a length up to 16,384 tokens.\n- For *Local Attention*, the sparse sliding-window local attention operation allows a given token to attend only `r`\ntokens to the left and right of it (with `r=127` by default). *Local Attention* does not introduce any new parameters\nto the model. The complexity of the mechanism is linear in input sequence length `l`: `O(l*r)`.\n- *Transient Global Attention* is an extension of the *Local Attention*. It, furthermore, allows each input token to\ninteract with all other tokens in the layer. This is achieved via splitting an input sequence into blocks of a fixed\nlength `k` (with a default `k=16`). Then, a global token for such a block is obtained via summing and normalizing the embeddings of every token\nin the block. Thanks to this, the attention allows each token to attend to both nearby tokens like in Local attention, and\nalso every global token like in the case of standard global attention (*transient* represents the fact the global tokens\nare constructed dynamically within each attention operation).  As a consequence, *TGlobal* attention introduces\na few new parameters -- global relative position biases and a layer normalization for global token's embedding.\nThe complexity of this mechanism is `O(l(r + l/k))`.\n- An example showing how to evaluate a fine-tuned LongT5 model on the [pubmed dataset](https://huggingface.co/datasets/scientific_papers) is below.\n\n```python\n>>> import evaluate\n>>> from datasets import load_dataset\n>>> from transformers import AutoTokenizer, LongT5ForConditionalGeneration\n\n>>> dataset = load_dataset(\"scientific_papers\", \"pubmed\", split=\"validation\")\n>>> model = (\n...     LongT5ForConditionalGeneration.from_pretrained(\"Stancld/longt5-tglobal-large-16384-pubmed-3k_steps\")\n...     .to(\"cuda\")\n...     .half()\n... )\n>>> tokenizer = AutoTokenizer.from_pretrained(\"Stancld/longt5-tglobal-large-16384-pubmed-3k_steps\")\n\n\n>>> def generate_answers(batch):\n...     inputs_dict = tokenizer(\n...         batch[\"article\"], max_length=16384, padding=\"max_length\", truncat..."], "test_settings": "chunk_7000_embeddings_openai-text-embedding-3-small_reader-model_gpt-4", "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response accurately provides the title of the paper introducing the ByT5 model, which matches the criterion answer exactly. The inclusion of the reference \"[0]\" is unnecessary but does not detract from the correctness or accuracy of the response. Overall, the response is entirely correct, precise, and factual."}, {"question": "What is the dimension of the feature vector for the base BERT model?\n", "true_answer": "768", "source_doc": "huggingface/course/blob/main/subtitles/en/raw/chapter1/05_encoders.md", "generated_answer": "The dimension of the feature vector for the base BERT model is 768.", "retrieved_docs": ["**Fun Fact:** Masking has been around a long time - [1953 Paper on Cloze procedure (or \u2018Masking\u2019)](https://psycnet.apa.org/record/1955-00850-001). \n\n### 2.3 What is Next Sentence Prediction?\n\nNSP (Next Sentence Prediction) is used to help BERT learn about relationships between sentences by predicting if a given sentence follows the previous sentence or not. \n\n**Next Sentence Prediction Example:**\n\n1. Paul went shopping. He bought a new shirt. (correct sentence pair)\n2. Ramona made coffee. Vanilla ice cream cones for sale. (incorrect sentence pair)\n\nIn training, 50% correct sentence pairs are mixed in with 50% random sentence pairs to help BERT increase next sentence prediction accuracy.\n\n**Fun Fact:** BERT is trained on both MLM (50%) and NSP (50%) at the same time.\n\n### 2.4 Transformers\n\nThe Transformer architecture makes it possible to parallelize ML training extremely efficiently. Massive parallelization thus makes it feasible to train BERT on large amounts of data in a relatively short period of time. \n\nTransformers use an attention mechanism to observe relationships between words. A concept originally proposed in the popular [2017 Attention Is All You Need](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) paper sparked the use of Transformers in NLP models all around the world.\n\n\n<p align=\"center\">\n\n>Since their introduction in 2017, Transformers have rapidly become the state-of-the-art approach to tackle tasks in many domains such as natural language processing, speech recognition, and computer vision. In short, if you\u2019re doing deep learning, then you need Transformers!\n\n<p class=\"text-center px-6\">Lewis Tunstall, Hugging Face ML Engineer & <a href=\"https://www.amazon.com/Natural-Language-Processing-Transformers-Applications/dp/1098103246)\">Author of Natural Language Processing with Transformers</a></p>\n</p>\n\nTimeline of popular Transformer model releases:\n\n<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Transformer model timeline\" src=\"assets/52_bert_101/transformers-timeline.png\"></medium-zoom>\n  <figcaption><a href=\"https://huggingface.co/course/chapter1/4\">Source</a></figcaption>\n</figure>\n\n#### 2.4.1 How do Transformers work?\n\nTransformers work by leveraging attention, a powerful deep-learning algorithm, first seen in computer vision models.\n\n\u2014Not all that different from how we humans process information through attention. We are incredibly good at forgetting/ignoring mundane daily inputs that don\u2019t pose a threat or require a response from us. For example, can you remember everything you saw and heard coming home last Tuesday? Of course not! Our brain\u2019s memory is limited and valuable. Our recall is aided by our ability to forget trivial inputs. \n\nSimilarly, Machine Learning models need to learn how to pay attention only to the things that matter and not waste computational resources processing irrelevant information. Transformers create differential weights signaling which words in a sentence are the most critical to further process.\n\n<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Encoder and Decoder\" src=\"assets/52_bert_101/encoder-and-decoder-transformers-blocks.png\"></medium-zoom>\n</figure>\n\nA transformer does this by successively processing an input through a stack of transformer layers, usually called the encoder. If necessary, another stack of transformer layers - the decoder - can be used to predict a target output. \u2014BERT however, doesn\u2019t use a decoder. Transformers are uniquely suited for unsupervised learning because they can efficiently process millions of data points.\n\nFun Fact: Google has been using your reCAPTCHA selections to label training data since 2011. The entire Google Books archive and 13 million articles from the New York Times catalog have been transcribed/digitized via people entering reCAPTCHA text. Now, reCAPTCHA is asking us to label Google Street View images, vehicles, stoplights, airplanes, etc. Would be neat if Google made us aware of our participation in this effort (as the training data likely has future commercial intent) but I digress..\n\n<p class=\"text-center\">\n    To learn more about Transformers check out our <a href=\"https://huggingface.co/course/chapter1/1\">Hugging Face Transformers Course</a>.\n</p>\n\n## 3. BERT model size & architecture\n\nLet\u2019s break down the architecture for the two original BERT models:\n\n<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Original BERT models architecture\" src=\"assets/52_bert_101/BERT-size-and-architecture.png\"></medium-zoom>\n</figure>\n\n\nML Architecture Glossary:\n\n| ML Architecture Parts | Definition                                                                                                                                                        |\n|-----------------------|-----------------------------------------------------------...", "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# BERT\n\n<div class=\"flex flex-wrap space-x-1\">\n<a href=\"https://huggingface.co/models?filter=bert\">\n<img alt=\"Models\" src=\"https://img.shields.io/badge/All_model_pages-bert-blueviolet\">\n</a>\n<a href=\"https://huggingface.co/spaces/docs-demos/bert-base-uncased\">\n<img alt=\"Spaces\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue\">\n</a>\n</div>\n\n## Overview\n\nThe BERT model was proposed in [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova. It's a\nbidirectional transformer pretrained using a combination of masked language modeling objective and next sentence\nprediction on a large corpus comprising the Toronto Book Corpus and Wikipedia.\n\nThe abstract from the paper is the following:\n\n*We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations\nfrom Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional\nrepresentations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result,\nthe pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models\nfor a wide range of tasks, such as question answering and language inference, without substantial task-specific\narchitecture modifications.*\n\n*BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural\nlanguage processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI\naccuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute\nimprovement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).*\n\nThis model was contributed by [thomwolf](https://huggingface.co/thomwolf). The original code can be found [here](https://github.com/google-research/bert).\n\n## Usage tips\n\n- BERT is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather than\n  the left.\n- BERT was trained with the masked language modeling (MLM) and next sentence prediction (NSP) objectives. It is\n  efficient at predicting masked tokens and at NLU in general, but is not optimal for text generation.\n- Corrupts the inputs by using random masking, more precisely, during pretraining, a given percentage of tokens (usually 15%) is masked by:\n\n    * a special mask token with probability 0.8\n    * a random token different from the one masked with probability 0.1\n    * the same token with probability 0.1\n    \n- The model must predict the original sentence, but has a second objective: inputs are two sentences A and B (with a separation token in between). With probability 50%, the sentences are consecutive in the corpus, in the remaining 50% they are not related. The model has to predict if the sentences are consecutive or not.\n\n## Resources\n\nA list of official Hugging Face and community (indicated by \ud83c\udf0e) resources to help you get started with BERT. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n<PipelineTag pipeline=\"text-classification\"/>\n\n- A blog post on [BERT Text Classification in a different language](https://www.philschmid.de/bert-text-classification-in-a-different-language).\n- A notebook for [Finetuning BERT (and friends) for multi-label text classification](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/BERT/Fine_tuning_BERT_(and_friends)_for_multi_label_text_classification.ipynb).\n- A notebook on how to [Finetune BERT for multi-label classification using PyTorch](https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_multi_label_classification.ipynb). \ud83c\udf0e\n- A notebook on how to [warm-start an EncoderDecoder model with BERT for summarization](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/BERT2BERT_for_CNN_Dailymail.ipynb).\n- [`...", "(query): Linear(in_features=768, out_features=768, bias=True)\n                    (key): Linear(in_features=768, out_features=768, bias=True)\n                    (value): Linear(in_features=768, out_features=768, bias=True)\n                    (dropout): Dropout(p=0.1, inplace=False)\n                  )\n                  (output): BertSelfOutput(\n                    (dense): Linear(in_features=768, out_features=768, bias=True)\n                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                    (dropout): Dropout(p=0.1, inplace=False)\n                  )\n                )\n                (intermediate): BertIntermediate(\n                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n                )\n                (output): BertOutput(\n                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n              )\n            )\n          )\n        )\n        (cls): BertOnlyMLMHead(\n          (predictions): BertLMPredictionHead(\n            (transform): BertPredictionHeadTransform(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            )\n            (decoder): Linear(in_features=768, out_features=30522, bias=True)\n          )\n        )\n      )\n    )\n```"], "test_settings": "chunk_7000_embeddings_openai-text-embedding-3-small_reader-model_gpt-4", "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response accurately states the dimension of the feature vector for the base BERT model as 768, which is correct and factual. The answer is clear and directly addresses the question without any unnecessary information. It aligns perfectly with the criteria for a 5-point response, as it is entirely correct and precise."}, {"question": "What special identifier does the WordPiece Model use for continuing subwords?\n", "true_answer": "##", "source_doc": "huggingface/tokenizers/blob/main/docs/source-doc-builder/components.mdx", "generated_answer": "The WordPiece model uses the prefix \"##\" as a special identifier for continuing subwords.", "retrieved_docs": ["et's see together what is the training strategy of the WordPiece algorithm and how it performs the tokenization of a text once trained WordPiece is a tokenization algorithm introduced by Google. It is used for example by Bert. To our knowledge, the code of Word Pieces has not been open sourced, so we base our explanations on our own interpretation of the published literature. What is the training strategy of WordPiece? Similarly to the BPE algorithm, WordPiece starts by establishing an initial vocabulary composed of elementary units and then increases this vocabulary to the desired size. To build the initial vocabulary, we divide each word in the training corpus into the sequence of letters that make it up. As you can see, there is a small subtlety: we add a 2 hashtags in front of the letters that do not start a word. By keeping only one occurrence per elementary unit we now have our initial vocabulary. We will list all the existing pairs in our corpus. Once we have this list, we will calculate a score for each of these pairs. As for the BPE algorithm, we will select the pair with the highest score. Taking for example the first pair composed of H and U. The score of a pair is simply equal to the frequency of appearance of the pair divided by the product of the frequency of appearance of the first token by the frequency of appearance of the second token. Thus at a fixed frequency of appearance of the pair, if the subparts of the pair are very frequent in the corpus then this score will be decreased. In our example, the pair \"hu\" appears 4 times, the letter \"h\" 4 times and the letter u 4 times. This gives us a score of 0.25. Now that we know how to calculate this score, we can do it for all pairs. We can now add to the vocabulary the pair with the highest score, after merging it of course! And now we can apply this same fusion to our split corpus. As you can imagine, we just have to repeat the same operations until we have the vocabulary at the desired size! Let's look at a few more steps to see the evolution of our vocabulary and the length of the splits getting shorter. Now that we are happy with our vocabulary, you are probably wondering how to use it to tokenize a text.  Let's say we want to tokenize the word \"huggingface\".  WordPiece follows these rules: We will look for the longest possible token at the beginning of our word. Then we start again on the remaining part of our word. And so on until we reach the end! And that's it, huggingface is divided into 4 sub-tokens. \u00dfThis video is about to end, I hope it helped you to understand better what is behind the word WordPiece!", "<a id='wordpiece'></a>\n\n### WordPiece\n\nWordPiece is the subword tokenization algorithm used for [BERT](model_doc/bert), [DistilBERT](model_doc/distilbert), and [Electra](model_doc/electra). The algorithm was outlined in [Japanese and Korean\nVoice Search (Schuster et al., 2012)](https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf) and is very similar to\nBPE. WordPiece first initializes the vocabulary to include every character present in the training data and\nprogressively learns a given number of merge rules. In contrast to BPE, WordPiece does not choose the most frequent\nsymbol pair, but the one that maximizes the likelihood of the training data once added to the vocabulary.\n\nSo what does this mean exactly? Referring to the previous example, maximizing the likelihood of the training data is\nequivalent to finding the symbol pair, whose probability divided by the probabilities of its first symbol followed by\nits second symbol is the greatest among all symbol pairs. *E.g.* `\"u\"`, followed by `\"g\"` would have only been\nmerged if the probability of `\"ug\"` divided by `\"u\"`, `\"g\"` would have been greater than for any other symbol\npair. Intuitively, WordPiece is slightly different to BPE in that it evaluates what it _loses_ by merging two symbols\nto ensure it's _worth it_.\n\n<a id='unigram'></a>\n\n### Unigram\n\nUnigram is a subword tokenization algorithm introduced in [Subword Regularization: Improving Neural Network Translation\nModels with Multiple Subword Candidates (Kudo, 2018)](https://arxiv.org/pdf/1804.10959.pdf). In contrast to BPE or\nWordPiece, Unigram initializes its base vocabulary to a large number of symbols and progressively trims down each\nsymbol to obtain a smaller vocabulary. The base vocabulary could for instance correspond to all pre-tokenized words and\nthe most common substrings. Unigram is not used directly for any of the models in the transformers, but it's used in\nconjunction with [SentencePiece](#sentencepiece).\n\nAt each training step, the Unigram algorithm defines a loss (often defined as the log-likelihood) over the training\ndata given the current vocabulary and a unigram language model. Then, for each symbol in the vocabulary, the algorithm\ncomputes how much the overall loss would increase if the symbol was to be removed from the vocabulary. Unigram then\nremoves p (with p usually being 10% or 20%) percent of the symbols whose loss increase is the lowest, *i.e.* those\nsymbols that least affect the overall loss over the training data. This process is repeated until the vocabulary has\nreached the desired size. The Unigram algorithm always keeps the base characters so that any word can be tokenized.\n\nBecause Unigram is not based on merge rules (in contrast to BPE and WordPiece), the algorithm has several ways of\ntokenizing new text after training. As an example, if a trained Unigram tokenizer exhibits the vocabulary:\n\n```\n[\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\", \"ug\", \"un\", \"hug\"],\n```\n\n`\"hugs\"` could be tokenized both as `[\"hug\", \"s\"]`, `[\"h\", \"ug\", \"s\"]` or `[\"h\", \"u\", \"g\", \"s\"]`. So which one\nto choose? Unigram saves the probability of each token in the training corpus on top of saving the vocabulary so that\nthe probability of each possible tokenization can be computed after training. The algorithm simply picks the most\nlikely tokenization in practice, but also offers the possibility to sample a possible tokenization according to their\nprobabilities.\n\nThose probabilities are defined by the loss the tokenizer is trained on. Assuming that the training data consists of\nthe words \\\\(x_{1}, \\dots, x_{N}\\\\) and that the set of all possible tokenizations for a word \\\\(x_{i}\\\\) is\ndefined as \\\\(S(x_{i})\\\\), then the overall loss is defined as\n\n$$\\mathcal{L} = -\\sum_{i=1}^{N} \\log \\left ( \\sum_{x \\in S(x_{i})} p(x) \\right )$$\n\n<a id='sentencepiece'></a>\n\n### SentencePiece\n\nAll tokenization algorithms described so far have the same problem: It is assumed that the input text uses spaces to\nseparate words. However, not all languages use spaces to separate words. One possible solution is to use language\nspecific pre-tokenizers, *e.g.* [XLM](model_doc/xlm) uses a specific Chinese, Japanese, and Thai pre-tokenizer).\nTo solve this problem more generally, [SentencePiece: A simple and language independent subword tokenizer and\ndetokenizer for Neural Text Processing (Kudo et al., 2018)](https://arxiv.org/pdf/1808.06226.pdf) treats the input\nas a raw input stream, thus including the space in the set of characters to use. It then uses the BPE or unigram\nalgorithm to construct the appropriate vocabulary.\n\nThe [`XLNetTokenizer`] uses SentencePiece for example, which is also why in the example earlier the\n`\"\u2581\"` character was included in the vocabulary. Decoding with SentencePiece is very easy since all tokens can just be\nconcatenated and `\"\u2581\"` is replaced by a space.\n\nAll transformers models in the library that use SentencePiece use it in combination with unigram. Examples of models\n...", "WordPiece tokenization[[wordpiece-tokenization]]\n\n<CourseFloatingBanner chapter={6}\n  classNames=\"absolute z-10 right-0 top-0\"\n  notebooks={[\n    {label: \"Google Colab\", value: \"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section6.ipynb\"},\n    {label: \"Aws Studio\", value: \"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter6/section6.ipynb\"},\n]} />\n\nWordPiece is the tokenization algorithm Google developed to pretrain BERT. It has since been reused in quite a few Transformer models based on BERT, such as DistilBERT, MobileBERT, Funnel Transformers, and MPNET. It's very similar to BPE in terms of the training, but the actual tokenization is done differently.\n\n<Youtube id=\"qpv6ms_t_1A\"/>\n\n<Tip>\n\n\ud83d\udca1 This section covers WordPiece in depth, going as far as showing a full implementation. You can skip to the end if you just want a general overview of the tokenization algorithm.\n\n</Tip>\n\n## Training algorithm[[training-algorithm]]\n\n<Tip warning={true}>\n\n\u26a0\ufe0f Google never open-sourced its implementation of the training algorithm of WordPiece, so what follows is our best guess based on the published literature. It may not be 100% accurate.\n\n</Tip>\n\nLike BPE, WordPiece starts from a small vocabulary including the special tokens used by the model and the initial alphabet. Since it identifies subwords by adding a prefix (like `##` for BERT), each word is initially split by adding that prefix to all the characters inside the word. So, for instance, `\"word\"` gets split like this:\n\n```\nw ##o ##r ##d\n```\n\nThus, the initial alphabet contains all the characters present at the beginning of a word and the characters present inside a word preceded by the WordPiece prefix.\n\nThen, again like BPE, WordPiece learns merge rules. The main difference is the way the pair to be merged is selected. Instead of selecting the most frequent pair, WordPiece computes a score for each pair, using the following formula:\n\n$$\\mathrm{score} = (\\mathrm{freq\\_of\\_pair}) / (\\mathrm{freq\\_of\\_first\\_element} \\times \\mathrm{freq\\_of\\_second\\_element})$$\n\nBy dividing the frequency of the pair by the product of the frequencies of each of its parts, the algorithm prioritizes the merging of pairs where the individual parts are less frequent in the vocabulary. For instance, it won't necessarily merge `(\"un\", \"##able\")` even if that pair occurs very frequently in the vocabulary, because the two pairs `\"un\"` and `\"##able\"` will likely each appear in a lot of other words and have a high frequency. In contrast, a pair like `(\"hu\", \"##gging\")` will probably be merged faster (assuming the word \"hugging\" appears often in the vocabulary) since `\"hu\"` and `\"##gging\"` are likely to be less frequent individually.\n\nLet's look at the same vocabulary we used in the BPE training example:\n\n```\n(\"hug\", 10), (\"pug\", 5), (\"pun\", 12), (\"bun\", 4), (\"hugs\", 5)\n```\n\nThe splits here will be:\n\n```\n(\"h\" \"##u\" \"##g\", 10), (\"p\" \"##u\" \"##g\", 5), (\"p\" \"##u\" \"##n\", 12), (\"b\" \"##u\" \"##n\", 4), (\"h\" \"##u\" \"##g\" \"##s\", 5)\n```\n\nso the initial vocabulary will be `[\"b\", \"h\", \"p\", \"##g\", \"##n\", \"##s\", \"##u\"]` (if we forget about special tokens for now). The most frequent pair is `(\"##u\", \"##g\")` (present 20 times), but the individual frequency of `\"##u\"` is very high, so its score is not the highest (it's 1 / 36). All pairs with a `\"##u\"` actually have that same score (1 / 36), so the best score goes to the pair `(\"##g\", \"##s\")` -- the only one without a `\"##u\"` -- at 1 / 20, and the first merge learned is `(\"##g\", \"##s\") -> (\"##gs\")`.\n\nNote that when we merge, we remove the `##` between the two tokens, so we add `\"##gs\"` to the vocabulary and apply the merge in the words of the corpus:\n\n```\nVocabulary: [\"b\", \"h\", \"p\", \"##g\", \"##n\", \"##s\", \"##u\", \"##gs\"]\nCorpus: (\"h\" \"##u\" \"##g\", 10), (\"p\" \"##u\" \"##g\", 5), (\"p\" \"##u\" \"##n\", 12), (\"b\" \"##u\" \"##n\", 4), (\"h\" \"##u\" \"##gs\", 5)\n```\n\nAt this point, `\"##u\"` is in all the possible pairs, so they all end up with the same score. Let's say that in this case, the first pair is merged, so `(\"h\", \"##u\") -> \"hu\"`. This takes us to:\n\n```\nVocabulary: [\"b\", \"h\", \"p\", \"##g\", \"##n\", \"##s\", \"##u\", \"##gs\", \"hu\"]\nCorpus: (\"hu\" \"##g\", 10), (\"p\" \"##u\" \"##g\", 5), (\"p\" \"##u\" \"##n\", 12), (\"b\" \"##u\" \"##n\", 4), (\"hu\" \"##gs\", 5)\n```\n\nThen the next best score is shared by `(\"hu\", \"##g\")` and `(\"hu\", \"##gs\")` (with 1/15, compared to 1/21 for all the other pairs), so the first pair with the biggest score is merged:\n\n```\nVocabulary: [\"b\", \"h\", \"p\", \"##g\", \"##n\", \"##s\", \"##u\", \"##gs\", \"hu\", \"hug\"]\nCorpus: (\"hug\", 10), (\"p\" \"##u\" \"##g\", 5), (\"p\" \"##u\" \"##n\", 12), (\"b\" \"##u\" \"##n\", 4), (\"hu\" \"##gs\", 5)\n```\n\nand we continue like this until we reach the desired vocabulary size.\n\n<Tip>\n\n\u270f\ufe0f **Now your turn!** What will the next merge rule be?\n\n</Tip>\n\n## Tokenization algorithm[[tokenization-algorithm]]\n\nTokenization differs in WordPiece and BPE in that WordPiece only saves the final vocabulary, not ..."], "test_settings": "chunk_7000_embeddings_openai-text-embedding-3-small_reader-model_gpt-4", "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response is entirely correct, accurate, and factual, providing the expected identifier used by the WordPiece model for continuing subwords."}, {"question": "What is the purpose of the \ud83e\udde8 Diffusers tutorials?\n", "true_answer": "To provide a gentle introduction to diffusion models and help understand the library fundamentals.", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/tutorials/tutorial_overview.md", "generated_answer": "The purpose of the \ud83e\udde8 Diffusers tutorials is to provide a gentle introduction to diffusion models, helping users understand the library fundamentals, learn how to use a pipeline for inference, deconstruct the pipeline to use the library as a modular toolbox, and eventually train their own diffusion model. The tutorials aim to equip users with the necessary skills to explore the library for their own projects and applications [\ubb38\uc11c 0].", "retrieved_docs": ["!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Overview\n\nWelcome to \ud83e\udde8 Diffusers! If you're new to diffusion models and generative AI, and want to learn more, then you've come to the right place. These beginner-friendly tutorials are designed to provide a gentle introduction to diffusion models and help you understand the library fundamentals - the core components and how \ud83e\udde8 Diffusers is meant to be used.\n\nYou'll learn how to use a pipeline for inference to rapidly generate things, and then deconstruct that pipeline to really understand how to use the library as a modular toolbox for building your own diffusion systems. In the next lesson, you'll learn how to train your own diffusion model to generate what you want.\n\nAfter completing the tutorials, you'll have gained the necessary skills to start exploring the library on your own and see how to use it for your own projects and applications.\n\nFeel free to join our community on [Discord](https://discord.com/invite/JfAtkvEtRb) or the [forums](https://discuss.huggingface.co/c/discussion-related-to-httpsgithubcomhuggingfacediffusers/63) to connect and collaborate with other users and developers!\n\nLet's start diffusing! \ud83e\udde8", "\ud83e\udde8 Diffusers Experimental\n\nWe are adding experimental code to support novel applications and usages of the Diffusers library.\nCurrently, the following experiments are supported:\n* Reinforcement learning via an implementation of the [Diffuser](https://arxiv.org/abs/2205.09991) model.", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n<p align=\"center\">\n    <br>\n    <img src=\"https://raw.githubusercontent.com/huggingface/diffusers/77aadfee6a891ab9fcfb780f87c693f7a5beeb8e/docs/source/imgs/diffusers_library.jpg\" width=\"400\"/>\n    <br>\n</p>\n\n# Diffusers\n\n\ud83e\udd17 Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you're looking for a simple inference solution or want to train your own diffusion model, \ud83e\udd17 Diffusers is a modular toolbox that supports both. Our library is designed with a focus on [usability over performance](conceptual/philosophy#usability-over-performance), [simple over easy](conceptual/philosophy#simple-over-easy), and [customizability over abstractions](conceptual/philosophy#tweakable-contributorfriendly-over-abstraction).\n\nThe library has three main components:\n\n- State-of-the-art diffusion pipelines for inference with just a few lines of code. There are many pipelines in \ud83e\udd17 Diffusers, check out the table in the pipeline [overview](api/pipelines/overview) for a complete list of available pipelines and the task they solve.\n- Interchangeable [noise schedulers](api/schedulers/overview) for balancing trade-offs between generation speed and quality.\n- Pretrained [models](api/models) that can be used as building blocks, and combined with schedulers, for creating your own end-to-end diffusion systems.\n\n<div class=\"mt-10\">\n  <div class=\"w-full flex flex-col space-y-4 md:space-y-0 md:grid md:grid-cols-2 md:gap-y-4 md:gap-x-5\">\n    <a class=\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\" href=\"./tutorials/tutorial_overview\"\n      ><div class=\"w-full text-center bg-gradient-to-br from-blue-400 to-blue-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed\">Tutorials</div>\n      <p class=\"text-gray-700\">Learn the fundamental skills you need to start generating outputs, build your own diffusion system, and train a diffusion model. We recommend starting here if you're using \ud83e\udd17 Diffusers for the first time!</p>\n    </a>\n    <a class=\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\" href=\"./using-diffusers/loading_overview\"\n      ><div class=\"w-full text-center bg-gradient-to-br from-indigo-400 to-indigo-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed\">How-to guides</div>\n      <p class=\"text-gray-700\">Practical guides for helping you load pipelines, models, and schedulers. You'll also learn how to use pipelines for specific tasks, control how outputs are generated, optimize for inference speed, and different training techniques.</p>\n    </a>\n    <a class=\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\" href=\"./conceptual/philosophy\"\n      ><div class=\"w-full text-center bg-gradient-to-br from-pink-400 to-pink-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed\">Conceptual guides</div>\n      <p class=\"text-gray-700\">Understand why the library was designed the way it was, and learn more about the ethical guidelines and safety implementations for using the library.</p>\n   </a>\n    <a class=\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\" href=\"./api/models/overview\"\n      ><div class=\"w-full text-center bg-gradient-to-br from-purple-400 to-purple-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed\">Reference</div>\n      <p class=\"text-gray-700\">Technical descriptions of how \ud83e\udd17 Diffusers classes and methods work.</p>\n    </a>\n  </div>\n</div>"], "test_settings": "chunk_7000_embeddings_openai-text-embedding-3-small_reader-model_gpt-4", "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response provides a detailed explanation of the purpose of the \ud83e\udde8 Diffusers tutorials. It accurately describes the tutorials' goals, including introducing diffusion models, understanding library fundamentals, using and deconstructing pipelines, and training diffusion models. The response goes beyond the briefness of the criteria answer by elaborating on the skills users will gain, which aligns with the purpose of the tutorials. However, the inclusion of \"[\ubb38\uc11c 0]\" is unnecessary and slightly detracts from the clarity of the response. Despite this minor issue, the response is comprehensive and aligns well with the criteria for a full understanding of the tutorials' purpose."}, {"question": "What is the default setting for the `allow_flagging` parameter in Gradio's `Interface`?\n", "true_answer": "\"manual\"", "source_doc": "gradio-app/gradio/blob/main/guides/cn/07_other-tutorials/using-flagging.md", "generated_answer": "The default setting for the `allow_flagging` parameter in Gradio's `Interface` is `\"manual\"`. This means users will see a button to flag, and samples are only flagged when the button is clicked.", "retrieved_docs": ["Using Flagging\n\nRelated spaces: https://huggingface.co/spaces/gradio/calculator-flagging-crowdsourced, https://huggingface.co/spaces/gradio/calculator-flagging-options, https://huggingface.co/spaces/gradio/calculator-flag-basic\nTags: FLAGGING, DATA\n\n## Introduction\n\nWhen you demo a machine learning model, you might want to collect data from users who try the model, particularly data points in which the model is not behaving as expected. Capturing these \"hard\" data points is valuable because it allows you to improve your machine learning model and make it more reliable and robust.\n\nGradio simplifies the collection of this data by including a **Flag** button with every `Interface`. This allows a user or tester to easily send data back to the machine where the demo is running. In this Guide, we discuss more about how to use the flagging feature, both with `gradio.Interface` as well as with `gradio.Blocks`.\n\n## The **Flag** button in `gradio.Interface`\n\nFlagging with Gradio's `Interface` is especially easy. By default, underneath the output components, there is a button marked **Flag**. When a user testing your model sees input with interesting output, they can click the flag button to send the input and output data back to the machine where the demo is running. The sample is saved to a CSV log file (by default). If the demo involves images, audio, video, or other types of files, these are saved separately in a parallel directory and the paths to these files are saved in the CSV file.\n\nThere are [four parameters](https://gradio.app/docs/#interface-header) in `gradio.Interface` that control how flagging works. We will go over them in greater detail.\n\n- `allow_flagging`: this parameter can be set to either `\"manual\"` (default), `\"auto\"`, or `\"never\"`.\n  - `manual`: users will see a button to flag, and samples are only flagged when the button is clicked.\n  - `auto`: users will not see a button to flag, but every sample will be flagged automatically.\n  - `never`: users will not see a button to flag, and no sample will be flagged.\n- `flagging_options`: this parameter can be either `None` (default) or a list of strings.\n  - If `None`, then the user simply clicks on the **Flag** button and no additional options are shown.\n  - If a list of strings are provided, then the user sees several buttons, corresponding to each of the strings that are provided. For example, if the value of this parameter is `[\"Incorrect\", \"Ambiguous\"]`, then buttons labeled **Flag as Incorrect** and **Flag as Ambiguous** appear. This only applies if `allow_flagging` is `\"manual\"`.\n  - The chosen option is then logged along with the input and output.\n- `flagging_dir`: this parameter takes a string.\n  - It represents what to name the directory where flagged data is stored.\n- `flagging_callback`: this parameter takes an instance of a subclass of the `FlaggingCallback` class\n  - Using this parameter allows you to write custom code that gets run when the flag button is clicked\n  - By default, this is set to an instance of `gr.CSVLogger`\n  - One example is setting it to an instance of `gr.HuggingFaceDatasetSaver` which can allow you to pipe any flagged data into a HuggingFace Dataset. (See more below.)\n\n## What happens to flagged data?\n\nWithin the directory provided by the `flagging_dir` argument, a CSV file will log the flagged data.\n\nHere's an example: The code below creates the calculator interface embedded below it:\n\n```python\nimport gradio as gr\n\n\ndef calculator(num1, operation, num2):\n    if operation == \"add\":\n        return num1 + num2\n    elif operation == \"subtract\":\n        return num1 - num2\n    elif operation == \"multiply\":\n        return num1 * num2\n    elif operation == \"divide\":\n        return num1 / num2\n\n\niface = gr.Interface(\n    calculator,\n    [\"number\", gr.Radio([\"add\", \"subtract\", \"multiply\", \"divide\"]), \"number\"],\n    \"number\",\n    allow_flagging=\"manual\"\n)\n\niface.launch()\n```\n\n<gradio-app space=\"gradio/calculator-flag-basic/\"></gradio-app>\n\nWhen you click the flag button above, the directory where the interface was launched will include a new flagged subfolder, with a csv file inside it. This csv file includes all the data that was flagged.\n\n```directory\n+-- flagged/\n|   +-- logs.csv\n```\n\n_flagged/logs.csv_\n\n```csv\nnum1,operation,num2,Output,timestamp\n5,add,7,12,2022-01-31 11:40:51.093412\n6,subtract,1.5,4.5,2022-01-31 03:25:32.023542\n```\n\nIf the interface involves file data, such as for Image and Audio components, folders will be created to store those flagged data as well. For example an `image` input to `image` output interface will create the following structure.\n\n```directory\n+-- flagged/\n|   +-- logs.csv\n|   +-- image/\n|   |   +-- 0.png\n|   |   +-- 1.png\n|   +-- Output/\n|   |   +-- 0.png\n|   |   +-- 1.png\n```\n\n_flagged/logs.csv_\n\n```csv\nim,Output timestamp\nim/0.png,Output/0.png,2022-02-04 19:49:58.026963\nim/1.png,Output/1.png,2022-02-02 10:40:51.093412\n```\n\nIf you wish for the user to provide a reason for flagging, you ca...", "We've made this super easy with the `flagging_callback` parameter.\n\nFor example, below we're going to pipe flagged data from our calculator example into a Hugging Face Dataset, e.g. so that we can build a \"crowd-sourced\" dataset:\n\n```python\nimport os\n\nHF_TOKEN = os.getenv('HF_TOKEN')\nhf_writer = gr.HuggingFaceDatasetSaver(HF_TOKEN, \"crowdsourced-calculator-demo\")\n\niface = gr.Interface(\n    calculator,\n    [\"number\", gr.Radio([\"add\", \"subtract\", \"multiply\", \"divide\"]), \"number\"],\n    \"number\",\n    description=\"Check out the crowd-sourced dataset at: [https://huggingface.co/datasets/aliabd/crowdsourced-calculator-demo](https://huggingface.co/datasets/aliabd/crowdsourced-calculator-demo)\",\n    allow_flagging=\"manual\",\n    flagging_options=[\"wrong sign\", \"off by one\", \"other\"],\n    flagging_callback=hf_writer\n)\n\niface.launch()\n```\n\nNotice that we define our own\ninstance of `gradio.HuggingFaceDatasetSaver` using our Hugging Face token and\nthe name of a dataset we'd like to save samples to. In addition, we also set `allow_flagging=\"manual\"`\nbecause on Hugging Face Spaces, `allow_flagging` is set to `\"never\"` by default. Here's our demo:\n\n<gradio-app space=\"gradio/calculator-flagging-crowdsourced/\"></gradio-app>\n\nYou can now see all the examples flagged above in this [public Hugging Face dataset](https://huggingface.co/datasets/aliabd/crowdsourced-calculator-demo).\n\n![flagging callback hf](https://github.com/gradio-app/gradio/blob/main/guides/assets/flagging-callback-hf.png?raw=true)\n\nWe created the `gradio.HuggingFaceDatasetSaver` class, but you can pass your own custom class as long as it inherits from `FLaggingCallback` defined in [this file](https://github.com/gradio-app/gradio/blob/master/gradio/flagging.py). If you create a cool callback, contribute it to the repo!\n\n## Flagging with Blocks\n\nWhat about if you are using `gradio.Blocks`? On one hand, you have even more flexibility\nwith Blocks -- you can write whatever Python code you want to run when a button is clicked,\nand assign that using the built-in events in Blocks.\n\nAt the same time, you might want to use an existing `FlaggingCallback` to avoid writing extra code.\nThis requires two steps:\n\n1. You have to run your callback's `.setup()` somewhere in the code prior to the\n   first time you flag data\n2. When the flagging button is clicked, then you trigger the callback's `.flag()` method,\n   making sure to collect the arguments correctly and disabling the typical preprocessing.\n\nHere is an example with an image sepia filter Blocks demo that lets you flag\ndata using the default `CSVLogger`:\n\n$code_blocks_flag\n$demo_blocks_flag\n\n## Privacy\n\nImportant Note: please make sure your users understand when the data they submit is being saved, and what you plan on doing with it. This is especially important when you use `allow_flagging=auto` (when all of the data submitted through the demo is being flagged)\n\n### That's all! Happy building :)", "The `Interface` class\n\nAs mentioned in the [Quickstart](/main/guides/quickstart), the `gr.Interface` class is a high-level abstraction in Gradio that allows you to quickly create a demo for any Python function simply by specifying the input types and the output types. Revisiting our first demo:\n\n$code_hello_world_4\n\n\nWe see that the `Interface` class is initialized with three required parameters:\n\n- `fn`: the function to wrap a user interface (UI) around\n- `inputs`: which Gradio component(s) to use for the input. The number of components should match the number of arguments in your function.\n- `outputs`: which Gradio component(s) to use for the output. The number of components should match the number of return values from your function.\n\nLet's take a closer look at these components used to provide input and output.\n\n## Components Attributes\n\nWe used the default versions of the `gr.Textbox` and `gr.Slider`, but what if you want to change how the UI components look or behave?\n\nLet's say you want to customize the slider to have values from 1 to 10, with a default of 2. And you wanted to customize the output text field \u2014 you want it to be larger and have a label.\n\nIf you use the actual class for `gr.Textbox` and `gr.Slider` instead of using the string shortcut, you have access to much more customizability through component attributes.\n\n$code_hello_world_2\n$demo_hello_world_2\n\n## Multiple Input and Output Components\n\nSuppose you had a more complex function, with multiple outputs as well. In the example below, we define a function that takes a string, boolean, and number, and returns a string and number. \n\n$code_hello_world_3\n$demo_hello_world_3\n\nJust as each component in the `inputs` list corresponds to one of the parameters of the function, in order, each component in the `outputs` list corresponds to one of the values returned by the function, in order.\n\n## An Image Example\n\nGradio supports many types of components, such as `Image`, `DataFrame`, `Video`, or `Label`. Let's try an image-to-image function to get a feel for these!\n\n$code_sepia_filter\n$demo_sepia_filter\n\nWhen using the `Image` component as input, your function will receive a NumPy array with the shape `(height, width, 3)`, where the last dimension represents the RGB values. We'll return an image as well in the form of a NumPy array.\n\nYou can also set the datatype used by the component with the `type=` keyword argument. For example, if you wanted your function to take a file path to an image instead of a NumPy array, the input `Image` component could be written as:\n\n```python\ngr.Image(type=\"filepath\", shape=...)\n```\n\nAlso note that our input `Image` component comes with an edit button \ud83d\udd89, which allows for cropping and zooming into images. Manipulating images in this way can help reveal biases or hidden flaws in a machine learning model!\n\nYou can read more about the many components and how to use them in the [Gradio docs](https://gradio.app/docs).\n\n## Example Inputs\n\nYou can provide example data that a user can easily load into `Interface`. This can be helpful to demonstrate the types of inputs the model expects, as well as to provide a way to explore your dataset in conjunction with your model. To load example data, you can provide a **nested list** to the `examples=` keyword argument of the Interface constructor. Each sublist within the outer list represents a data sample, and each element within the sublist represents an input for each input component. The format of example data for each component is specified in the [Docs](https://gradio.app/docs#components).\n\n$code_calculator\n$demo_calculator\n\nYou can load a large dataset into the examples to browse and interact with the dataset through Gradio. The examples will be automatically paginated (you can configure this through the `examples_per_page` argument of `Interface`).\n\nContinue learning about examples in the [More On Examples](https://gradio.app/guides/more-on-examples) guide.\n\n## Descriptive Content\n\nIn the previous example, you may have noticed the `title=` and `description=` keyword arguments in the `Interface` constructor that helps users understand your app.\n\nThere are three arguments in the `Interface` constructor to specify where this content should go:\n\n- `title`: which accepts text and can display it at the very top of interface, and also becomes the page title.\n- `description`: which accepts text, markdown or HTML and places it right under the title.\n- `article`: which also accepts text, markdown or HTML and places it below the interface.\n\n![annotated](https://github.com/gradio-app/gradio/blob/main/guides/assets/annotated.png?raw=true)\n\nIf you're using the `Blocks` API instead, you can insert text, markdown, or HTML anywhere using the `gr.Markdown(...)` or `gr.HTML(...)` components, with descriptive content inside the `Component` constructor.\n\nAnother useful keyword argument is `label=`, which is present in every `Component`. This modifies the label text at the top of each `Component`...."], "test_settings": "chunk_7000_embeddings_openai-text-embedding-3-small_reader-model_gpt-4", "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response correctly identifies the default setting for the `allow_flagging` parameter in Gradio's `Interface` as `\"manual\"`, which matches the criterion answer. Additionally, the response provides a brief explanation of what the `\"manual\"` setting entails, enhancing the understanding of the default behavior. This additional context is accurate and aligns with the functionality of the parameter, making the response not only correct but also informative. Therefore, the response is entirely accurate and factual."}, {"question": "Where can the full code for the Stable Diffusion demo be found?\n", "true_answer": "https://hf.co/spaces/stabilityai/stable-diffusion/tree/main", "source_doc": "gradio-app/gradio/blob/main/demo/stable-diffusion/DESCRIPTION.md", "generated_answer": "The full code for the Stable Diffusion demo can be found at: https://hf.co/spaces/stabilityai/stable-diffusion/tree/main [\ubb38\uc11c 0].", "retrieved_docs": ["ote: This is a simplified version of the code needed to create the Stable Diffusion demo. See full code here: https://hf.co/spaces/stabilityai/stable-diffusion/tree/main", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Stable Diffusion pipelines\n\nStable Diffusion is a text-to-image latent diffusion model created by the researchers and engineers from [CompVis](https://github.com/CompVis), [Stability AI](https://stability.ai/) and [LAION](https://laion.ai/). Latent diffusion applies the diffusion process over a lower dimensional latent space to reduce memory and compute complexity. This specific type of diffusion model was proposed in [High-Resolution Image Synthesis with Latent Diffusion Models](https://huggingface.co/papers/2112.10752) by Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Bj\u00f6rn Ommer.\n\nStable Diffusion is trained on 512x512 images from a subset of the LAION-5B dataset. This model uses a frozen CLIP ViT-L/14 text encoder to condition the model on text prompts. With its 860M UNet and 123M text encoder, the model is relatively lightweight and can run on consumer GPUs.\n\nFor more details about how Stable Diffusion works and how it differs from the base latent diffusion model, take a look at the Stability AI [announcement](https://stability.ai/blog/stable-diffusion-announcement) and our own [blog post](https://huggingface.co/blog/stable_diffusion#how-does-stable-diffusion-work) for more technical details.\n\nYou can find the original codebase for Stable Diffusion v1.0 at [CompVis/stable-diffusion](https://github.com/CompVis/stable-diffusion) and Stable Diffusion v2.0 at [Stability-AI/stablediffusion](https://github.com/Stability-AI/stablediffusion) as well as their original scripts for various tasks. Additional official checkpoints for the different Stable Diffusion versions and tasks can be found on the [CompVis](https://huggingface.co/CompVis), [Runway](https://huggingface.co/runwayml), and [Stability AI](https://huggingface.co/stabilityai) Hub organizations. Explore these organizations to find the best checkpoint for your use-case!\n\nThe table below summarizes the available Stable Diffusion pipelines, their supported tasks, and an interactive demo:", "| Multilingual Stable Diffusion                                                                                                         | Stable Diffusion Pipeline that supports prompts in 50 different languages.                                                                                                                                                                                                                                                                                                                                                                                                                               | [Multilingual Stable Diffusion](#multilingual-stable-diffusion-pipeline)                  | -                                                                                                                                                                                                                  |          [Juan Carlos Pi\u00f1eros](https://github.com/juancopi81) |\n| Image to Image Inpainting Stable Diffusion                                                                                            | Stable Diffusion Pipeline that enables the overlaying of two images and subsequent inpainting                                                                                                                                                                                                                                                                                                                                                                                                            | [Image to Image Inpainting Stable Diffusion](#image-to-image-inpainting-stable-diffusion) | -                                                                                                                                                                                                                  |                    [Alex McKinney](https://github.com/vvvm23) |\n| Text Based Inpainting Stable Diffusion                                                                                                | Stable Diffusion Inpainting Pipeline that enables passing a text prompt to generate the mask for inpainting                                                                                                                                                                                                                                                                                                                                                                                              | [Text Based Inpainting Stable Diffusion](#image-to-image-inpainting-stable-diffusion)     | -                                                                                                                                                                                                                  |                   [Dhruv Karan](https://github.com/unography) |\n| Bit Diffusion                                                                                                                         | Diffusion on discrete data                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | [Bit Diffusion](#bit-diffusion)                                                           | -  |                       [Stuti R.](https://github.com/kingstut) |\n| K-Diffusion Stable Diffusion                                                                                                          | Run Stable Diffusion with any of [K-Diffusion's samplers](https://github.com/crowsonkb/k-diffusion/blob/master/k_diffusion/sampling.py)                                                                                                                                                                                                                                                                                                                                                                  | [Stable Diffusion with K Diffusion](#stable-diffusion-with-k-diffusion)                   | -  |    [Patrick von Platen](https://github.com/patrickvonplaten/) |\n| Checkpoint Merger Pipeline                                                                                                            | Diffusion Pipeline that enables merging of saved model checkpoints                                                                                                                                                                                                                              ..."], "test_settings": "chunk_7000_embeddings_openai-text-embedding-3-small_reader-model_gpt-4", "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response provides the correct URL for the Stable Diffusion demo code, which matches the URL given in the 5-point criteria. However, the additional text \"[\ubb38\uc11c 0]\" is unnecessary and does not contribute to the accuracy or relevance of the response. Despite this minor issue, the core information is accurate and factual. Therefore, the response is mostly correct and precise, but the extraneous text slightly detracts from its conciseness."}, {"question": "What transformation does the FNet model use to replace the self-attention layer in a BERT model?\n", "true_answer": "Fourier transform", "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/fnet.md", "generated_answer": "The FNet model uses a Fourier Transform to replace the self-attention layer in a BERT model. The model returns only the real parts of the transform, making it significantly faster and more memory efficient than traditional BERT models [0].", "retrieved_docs": ["!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# FNet\n\n## Overview\n\nThe FNet model was proposed in [FNet: Mixing Tokens with Fourier Transforms](https://arxiv.org/abs/2105.03824) by\nJames Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon. The model replaces the self-attention layer in a BERT\nmodel with a fourier transform which returns only the real parts of the transform. The model is significantly faster\nthan the BERT model because it has fewer parameters and is more memory efficient. The model achieves about 92-97%\naccuracy of BERT counterparts on GLUE benchmark, and trains much faster than the BERT model. The abstract from the\npaper is the following:\n\n*We show that Transformer encoder architectures can be sped up, with limited accuracy costs, by replacing the\nself-attention sublayers with simple linear transformations that \"mix\" input tokens. These linear mixers, along with\nstandard nonlinearities in feed-forward layers, prove competent at modeling semantic relationships in several text\nclassification tasks. Most surprisingly, we find that replacing the self-attention sublayer in a Transformer encoder\nwith a standard, unparameterized Fourier Transform achieves 92-97% of the accuracy of BERT counterparts on the GLUE\nbenchmark, but trains 80% faster on GPUs and 70% faster on TPUs at standard 512 input lengths. At longer input lengths,\nour FNet model is significantly faster: when compared to the \"efficient\" Transformers on the Long Range Arena\nbenchmark, FNet matches the accuracy of the most accurate models, while outpacing the fastest models across all\nsequence lengths on GPUs (and across relatively shorter lengths on TPUs). Finally, FNet has a light memory footprint\nand is particularly efficient at smaller model sizes; for a fixed speed and accuracy budget, small FNet models\noutperform Transformer counterparts.*\n\nThis model was contributed by [gchhablani](https://huggingface.co/gchhablani). The original code can be found [here](https://github.com/google-research/google-research/tree/master/f_net).\n\n## Usage tips\n\nThe model was trained without an attention mask as it is based on Fourier Transform. The model was trained with \nmaximum sequence length 512 which includes pad tokens. Hence, it is highly recommended to use the same maximum \nsequence length for fine-tuning and inference.\n\n## Resources\n\n- [Text classification task guide](../tasks/sequence_classification)\n- [Token classification task guide](../tasks/token_classification)\n- [Question answering task guide](../tasks/question_answering)\n- [Masked language modeling task guide](../tasks/masked_language_modeling)\n- [Multiple choice task guide](../tasks/multiple_choice)\n\n## FNetConfig\n\n[[autodoc]] FNetConfig\n\n## FNetTokenizer\n\n[[autodoc]] FNetTokenizer\n    - build_inputs_with_special_tokens\n    - get_special_tokens_mask\n    - create_token_type_ids_from_sequences\n    - save_vocabulary\n\n## FNetTokenizerFast\n\n[[autodoc]] FNetTokenizerFast\n\n## FNetModel\n\n[[autodoc]] FNetModel\n    - forward\n\n## FNetForPreTraining\n\n[[autodoc]] FNetForPreTraining\n    - forward\n\n## FNetForMaskedLM\n\n[[autodoc]] FNetForMaskedLM\n    - forward\n\n## FNetForNextSentencePrediction\n\n[[autodoc]] FNetForNextSentencePrediction\n    - forward\n\n## FNetForSequenceClassification\n\n[[autodoc]] FNetForSequenceClassification\n    - forward\n\n## FNetForMultipleChoice\n\n[[autodoc]] FNetForMultipleChoice\n    - forward\n\n## FNetForTokenClassification\n\n[[autodoc]] FNetForTokenClassification\n    - forward\n\n## FNetForQuestionAnswering\n\n[[autodoc]] FNetForQuestionAnswering\n    - forward", "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n# Adding BetterTransformer support for new architectures\n\nYou want to add a new model for `Better Transformer`, the fast path of PyTorch Transformer API? Check this guideline!\n\n## Models that should be supported\n\nIn theory, any model that has a transformer encoder layer, similar to the classic encoder described in the [\"Attention Is All You Need\"](https://arxiv.org/abs/1706.03762) paper should be supported. \nMore specifically, a model that has an encoder block with a MultiHead-Attention module (with pre or post-attention layer norm) should be convertible to its `BetterTransformer` equivalent. The conditions can be summarized as follows:\n\n- Use classic Multi Head attention module (for example, [DeBERTa](https://arxiv.org/abs/2006.03654) cannot be supported)\n- Use either `gelu` or `relu` activation function   \n- Have an even number of attention heads\n- Do not use any attention bias (for eg `T5` uses attention bias, therefore cannot be supported)\n- `eps` must be equal between the first and second layer norms for each layer\n\n## How to convert a model into its `BetterTransformer` format?\n\n### Step 1: Identifying the source layer to change\n\nFirst, go to `optimum/bettertransformer/__init__.py` and you'll see the dictionary `BetterTransformerManager.MODEL_MAPPING`. This should contain the mapping between a model type, and the `Tuple[str, BetterTransformerBaseLayer]` composed of the name of the `nn.Module` that can be converted to its `BetterTransformer` equivalent, and effectively the equivalent `BetterTransformer` layer class.\n\nLet us try to do it step by step for `Bert`, first we need to identify the layers that needs to be replaced:\n```python\n>>> from transformers import AutoModel\n\n>>> model = AutoModel.from_pretrained(\"bert-base-uncased\")\n>>> print(model)  # doctest: +IGNORE_RESULT\n...\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (11): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (pooler): BertPooler(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (activation): Tanh()\n  )\n)\n```\n\nYou can clearly see that the layers that need to be replaced are the `BertLayer` modules since they contain the whole encoder layer module.\n\n### Step 2: Building the `xxxLayerBetterTransformer` module\n\nCheck that the identified module is not already copied from another module (by inspecting the source code in [`transformers`](https://github.com/huggingface/transformers) and checking that the class definition does not start with `# Copied from ...`) - and if not, create a class in `bettertransformer/models/encoder_model.py`.\nStart with those lines:\n```python\nimport torch\nimport torch.nn as nn\n\nfrom ..base import BetterTransformerBaseLayer\n\n\nclass BertLayerBetterTransformer(BetterTransformerBaseLayer):\n    def __init__(self, bert_layer, config):\n...\n```\n\nNow, make sure to fill all the necessary attributes, the list of attributes are:\n\n- `in_proj_weight`\n- `in_proj_bias`\n- `out_proj_weight`\n- `out_proj_bias`\n- `linear1_weight`\n- `linear1_bias`\n- `linear2_weight`\n- `linear2_bias`\n- `norm1_eps`\n- `norm1_weight`\n- `norm1_bias`\n- `norm2_weight`\n- `norm2_bias` \n- `num_heads`\n- `embed_dim`\n\nNote that these attributes correspond to all the components that are necessary to run a Transformer Encoder module, check the figure 1 on the [\"Attenti...", "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# FocalNet\n\n## Overview\n\nThe FocalNet model was proposed in [Focal Modulation Networks](https://arxiv.org/abs/2203.11926) by Jianwei Yang, Chunyuan Li, Xiyang Dai, Lu Yuan, Jianfeng Gao.\nFocalNets completely replace self-attention (used in models like [ViT](vit) and [Swin](swin)) by a focal modulation mechanism for modeling token interactions in vision.\nThe authors claim that FocalNets outperform self-attention based models with similar computational costs on the tasks of image classification, object detection, and segmentation.\n\nThe abstract from the paper is the following:\n\n*We propose focal modulation networks (FocalNets in short), where self-attention (SA) is completely replaced by a focal modulation mechanism for modeling token interactions in vision. Focal modulation comprises three components: (i) hierarchical contextualization, implemented using a stack of depth-wise convolutional layers, to encode visual contexts from short to long ranges, (ii) gated aggregation to selectively gather contexts for each query token based on its\ncontent, and (iii) element-wise modulation or affine transformation to inject the aggregated context into the query. Extensive experiments show FocalNets outperform the state-of-the-art SA counterparts (e.g., Swin and Focal Transformers) with similar computational costs on the tasks of image classification, object detection, and segmentation. Specifically, FocalNets with tiny and base size achieve 82.3% and 83.9% top-1 accuracy on ImageNet-1K. After pretrained on ImageNet-22K in 224 resolution, it attains 86.5% and 87.3% top-1 accuracy when finetuned with resolution 224 and 384, respectively. When transferred to downstream tasks, FocalNets exhibit clear superiority. For object detection with Mask R-CNN, FocalNet base trained with 1\\times outperforms the Swin counterpart by 2.1 points and already surpasses Swin trained with 3\\times schedule (49.0 v.s. 48.5). For semantic segmentation with UPerNet, FocalNet base at single-scale outperforms Swin by 2.4, and beats Swin at multi-scale (50.5 v.s. 49.7). Using large FocalNet and Mask2former, we achieve 58.5 mIoU for ADE20K semantic segmentation, and 57.9 PQ for COCO Panoptic Segmentation. Using huge FocalNet and DINO, we achieved 64.3 and 64.4 mAP on COCO minival and test-dev, respectively, establishing new SoTA on top of much larger attention-based models like Swinv2-G and BEIT-3.*\n\nThis model was contributed by [nielsr](https://huggingface.co/nielsr).\nThe original code can be found [here](https://github.com/microsoft/FocalNet).\n\n## FocalNetConfig\n\n[[autodoc]] FocalNetConfig\n\n## FocalNetModel\n\n[[autodoc]] FocalNetModel\n    - forward\n\n## FocalNetForMaskedImageModeling\n\n[[autodoc]] FocalNetForMaskedImageModeling\n    - forward\n\n## FocalNetForImageClassification\n\n[[autodoc]] FocalNetForImageClassification\n    - forward"], "test_settings": "chunk_7000_embeddings_openai-text-embedding-3-small_reader-model_gpt-4", "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response accurately identifies the transformation used in the FNet model and provides additional context about its efficiency, which is both correct and informative."}, {"question": "What type of test should typically accompany a bug fix in Gradio's testing strategy?\n", "true_answer": "Dynamic code test", "source_doc": "gradio-app/gradio/blob/main/test-strategy.md", "generated_answer": "A bug fix in Gradio's testing strategy should typically be accompanied by tests wherever it is reasonably possible. This aligns with the objective of preventing regressions, ensuring that once a bug is fixed, it stays fixed [Document 0].", "retrieved_docs": ["Test Strategy\n\nVery brief, mildly aspirational test strategy document. This isn't where we are but it is where we want to get to.\n\nThis document does not detail how to setup an environment or how to run the tests locally nor does it contain any best practices that we try to follow when writing tests, that information exists in the [contributing guide](https://github.com/gradio-app/gradio/blob/main/CONTRIBUTING.md).\n\n## Objectives\n\nThe purposes of all testing activities on Gradio fit one of the following objectives:\n\n1. Ensure that the Gradio library functions as we expect it to.\n2. Enable the maintenance team to quickly identify both the presence and source of defects.\n3. Prevent regressions, i.e. if we fix something it should stay fixed.\n4. Improve the quality of the codebase in order to ease maintenance efforts.\n5. Reduce the amount of manual testing required.\n\n## Scope\n\nTesting is always a tradeoff. We can't cover everything unless we want to spend all of our time writing and running tests. We should focus on a few keys areas.\n\nWe should not focus on code coverage but on test coverage following the below criteria:\n\n- The documented Gradio API (that's the bit that users interact with via python) should be tested thoroughly. (1)\n- Additional gradio elements that are both publicly available and used internally (such as the Python and JS client libraries) should be tested thoroughly. (1)\n- Additional gradio elements that are publicly available should be tested as thoroughly as is reasonable (this could be things like demos/the gradio CLI/ other tooling). The importance of each individual component, and the appropriate investment of effort, needs to be assessed on a case-by-case basis. (1)\n- Element boundaries should be tested where there is reasonable cause to do so (e.g. config generation) (1)\n- Implementation details should only be tested where there is sufficient complexity to warrant it. (1)\n- Bug fixes should be accompanied by tests wherever is reasonably possible. (3)\n\n## Types of testing\n\nOur tests will broadly fall into one of three categories:\n\n- Static Quality checks\n- Dynamic 'Code' tests\n- Dynamic Functional tests\n\n### Static Quality checks\n\nStatic quality checks are generally very fast to run and do not require building the code base. They also provide the least value. These tests would be things like linting, typechecking, and formatting.\n\nWhile they offer little in terms of testing functionality they align very closely with objective (4, 5) as they generally help to keep the codebase in good shape and offer very fast feedback. Such check are almost free from an authoring point of view as fixes can be mostly automated (either via scripts or editor integrations).\n\n### Dynamic code tests\n\nThese tests generally test either isolated pieces of code or test the relationship between parts of the code base. They sometimes test functionality or give indications of working functionality but never offer enough confidence to rely on them solely.\n\nThese test are usually either unit or integration tests. They are generally pretty quick to write (especially unit tests) and run and offer a moderate amount of confidence. They align closely with Objectives 2 and 3 and a little bit of 1.\n\nThese kind of tests should probably make up the bulk of our handwritten tests.\n\n### Dynamic functional tests\n\nThese tests give by far the most confidence as they are testing only the functionality of the software and do so by running the entire software itself, exactly as a user would.\n\nThis aligns very closely with objective 1 but significantly impacts objective 5, as these tests are costly to both write and run. Despite the value, due to the downside we should try to get as much out of other tests types as we can, reserving functional testing for complex use cases and end-to-end journey.\n\nTests in this category could be browser-based end-to-end tests, accessibility tests, or performance tests. They are sometimes called acceptance tests.\n\n## Testing tools\n\nWe currently use the following tools:\n\n### Static quality checks\n\n- Python type-checking (python)\n- Black linting (python)\n- ruff formatting (python)\n- prettier formatting (javascript/svelte)\n- TypeScript type-checking (javascript/svelte)\n- eslint linting (javascript/svelte) [in progress]\n\n### Dynamic code tests\n\n- pytest (python unit and integration tests)\n- vitest (node-based unit and integration tests)\n- playwright (browser-based unit and integration tests)\n\n### Functional/acceptance tests\n\n- playwright (full end to end testing)\n- chromatic (visual testing) [in progress]\n- Accessibility testing [to do]\n\n## Supported environments and versions\n\nAll operating systems refer to the current runner variants supported by GitHub actions.\n\nAll unspecified version segments (`x`) refer to latest.\n\n| Software | Version(s)            | Operating System(s)               |\n| -------- | --------------------- | --------------------------------- |\n| Python   | `3.8.x`               | `ubuntu...", "Backend Testing Guidelines\n\n- All the tests should test Backend functionalities. Frontend functionalities and e2e tests are done in Frontend.\n- Make use of pytest fixtures whenever it is possible. With fixtures, objects with high initialize durations are reused within tests, ex. a client session.\n- All test*data resides within \\_gradio/test_data* and all test_files resides within test/test_files.\n- When doing network operations do not forget to make use of async to make tests faster.\n- Have clear class and function naming within the tests.\n- Short descriptions within test functions are great.\n- Library function docstrings is expected to contain an example, please add missing docstrings to the library while you are writing tests the related function.\n- Library docstring examples and descriptions are expected to align with tests, please fix divergent tests and library docstrings.", "Contributing to Gradio\n\n![GitHub issues by-label](https://img.shields.io/github/issues/gradio-app/gradio/good%20first%20issue?color=fe7c01&link=https%3A%2F%2Fgithub.com%2Fgradio-app%2Fgradio%2Fissues%3Fq%3Dis%253Aopen%2Bis%253Aissue%2Blabel%253A%2522good%2Bfirst%2Bissue%2522)\n\n\nMore than 200 awesome developers have contributed to the `gradio` library, and we'd be thrilled if you would like to be the next contributor! \n\nPrerequisites:\n\n- [Python 3.8+](https://www.python.org/downloads/)\n- [Node.js v16.14+](https://nodejs.dev/en/download/package-manager/) (only needed if you are making changes to the frontend)\n- [pnpm 8.1+](https://pnpm.io/8.x/installation) (only needed if you are making changes to the frontend)\n \n## \ud83c\udfe1 Setup for local development\n\nThere are a few ways to install and run Gradio.\n\n### \ud83d\udee0\ufe0f Install Gradio locally from `main`\n\n- Clone this repo\n- Navigate to the repo directory and (from the root directory) run\n\n```bash\nbash scripts/install_gradio.sh\n```\n\n- Build the front end\n\n```\nbash scripts/build_frontend.sh\n```\n\n-  Install development requirements\n\n(Note that it is highly recommended to use a virtual environment running **Python 3.9** since the versions of Gradio's dependencies are pinned)\n\n```\nbash scripts/install_test_requirements.sh\n```\n\nIf you have a different Python version and conflicting packages during the installation, please first run:\n\n```\nbash scripts/create_test_requirements.sh\n```\n\n### \ud83d\udce6 Using dev containers\n\nYou can alternatively use dev containers. This is supported on all platforms (macOS/Windows/Linux), as well as on GitHub Codespaces. \n\nPrerequisites:\n\n- An editor which supports dev containers, like VS Code\n- Docker support on the host computer:\n  - macOS: [Docker Desktop 2.0+](https://www.docker.com/products/docker-desktop/)\n  - Windows: [Docker Desktop 2.0+](https://www.docker.com/products/docker-desktop/)\n  - Linux: [Docker CE/EE 18.06+](https://docs.docker.com/get-docker/) and [Docker Compose 1.21+](https://docs.docker.com/compose/install/)\n- If using VS Code, the [Dev Containers](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers) extension\n\nSteps:\n\n- Clone repository\n- Open it in your editor\n- For VS Code, execute `Dev Containers: Reopen in container` command\n\nFor detailed instructions, please see the [Dev Containers tutorial](https://code.visualstudio.com/docs/devcontainers/tutorial).\n\n## \ud83e\uddf1 Structure of the Repository\n\nIf you're a newcomer to Gradio, we recommend getting familiar with the overall structure of the repository so that you can focus on the part of the source code you'd like to contribute to.\n\n- `/gradio`: contains the Python source code for the library\n  - `/gradio/interface.py`: contains the Python source code for the core `Interface` class\n  - `/gradio/blocks.py`: contains the Python source code for the core `Blocks` class\n  - `/gradio/components/`: the directory that contains the Python source code for all of the Gradio components.\n- `/js`: contains the HTML/JS/CSS source code for the library ([start here for frontend changes](/js/README.md))\n  - `/js/_website`: contains the code for the Gradio website (www.gradio.app). See the README in the `/js/_website` folder for more details\n- `/test`: contains Python unit tests for the library\n- `/demo`: contains demos that are used in the documentation, you can find `Gradio` examples over here.\n\n\n## \ud83d\ude80 Run a Gradio app\n\nYou can get started by creating an `app.py` file in the root:\n\n```\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n   gr.Button()\n    \nif __name__ == \"__main__\":\n   demo.launch()\n```\n\nthen run:\n\n```\ngradio app.py\n```\n\nThis will start the backend server in reload mode, which will watch for changes in the `gradio` folder and reload the app if changes are made. By default, Gradio will launch on port 7860. You can also just use `python app.py`, but this won't automatically trigger updates. \n\n\nIf you're making frontend changes, start the frontend server:\n\n```\npnpm dev\n```\nThis will open a separate browser tab. By default, Gradio will launch this on port 9876. Any changes to the frontend will also reload automatically in the browser. For more information about developing in the frontend, you can refer to [js/README.md](js/README.md).\n\nWe also have demos of all our components in the `/gradio/demo` directory. To get our simple gradio Chatbot running locally:\n\n```\ngradio demo/chatbot_simple/run.py\n```\n\n\n## \ud83e\uddea Testing\n\nWe use Pytest, Playwright and Vitest to test our code. \n\n- The Python tests are located in `/test`. To run these tests:\n\n```\nbash scripts/run_all_tests.sh\n```\n\n- The frontend unit tests are any defined with the filename `*.test.ts`. To run them:\n\n```\npnpm test\n```\n\n- Browser tests are located in `js/app/test` and are defined as `*spec.ts` files. To run browser tests:\n\n```\npnpm test:browser\n```\n\nTo build the frontend code before running browser tests:\n\n```\npnpm test:browser:full\n```\n\nYou can also run browser tests in the UI mode by adding the `--ui` flag: \n\n```\n..."], "test_settings": "chunk_7000_embeddings_openai-text-embedding-3-small_reader-model_gpt-4", "eval_score_GPT4": "3", "eval_feedback_GPT4": "The response provides a general understanding of the importance of testing in preventing regressions but fails to specify the type of test required for a bug fix in Gradio's testing strategy, which is \"Dynamic code test.\" To improve, the response should directly mention the specific type of test."}, {"question": "How can you force mixed precision training when initializing the Accelerator in \ud83e\udd17 Accelerate?\n", "true_answer": "By passing `fp16=True` to the Accelerator init.", "source_doc": "huggingface/blog/blob/main/accelerate-library.md", "generated_answer": "To force mixed precision training when initializing the Accelerator in \ud83e\udd17 Accelerate, you can pass the argument `mixed_precision='fp16'` or `mixed_precision='bf16'` to the `Accelerator` constructor, depending on the precision you want to use.", "retrieved_docs": ["upercharge your Pytorch training loop with Hugging Face Accelerate. There are multiple setups on which you can run your training: it could be on CPU, GPUs, TPUs. Distributed on one machine with several devices, or several machines (often called nodes) each with multiple devices. On top of that there are new tweaks to make your training faster or more memory efficient, like mixed precision and DeepSpeed. Each of those setups or training tweaks, requires you to change the code of your training loop in one way or another and to learn a new API. All those setups are handled by the Trainer API, and there are several third-party libraries that can also help you with that. The problem with those is that they can feel like a black box and that it might not be easy to implement the tweak to the training loop you need. Accelerate has been designed specifically to let you retain full control over your training loop and be as non-intrusive as possible. With just four lines to add to your training loop (here shown on the code of the training loop from the \"Raw training loop\" video), Accelerate will handle all the setups and training tweaks mentioned on the first slide. It's only one API to learn and master instead of ten different ones. More specifically, you have to import and instantiate an accelerator object, that will handle all the necessary code for your  specific setup. Then you have to send it the model, optimizer and dataloaders you are using in the prepare method, which is the main method to remember. Accelerate handles device placement, so you don't need to put your batch on the specific device you are using. Finally, you have to replace the loss.backward line by accelerate.backward(loss), and that's all you need! Accelerate also handles distributed evaluation. You can still use a classic evaluation loop such as the one we saw in the \"Raw training loop\" video, in which case all processes will each perform the full evaluation. To use a distributed evaluation, you just have to adapt your evaluation loop like this: pass along the evaluation dataloader to the accelerator.prepare method, like for training. Then you can dismiss the line that places the batch on the proper device, and just before passing your predictions and labels to your metric, use accelerator.gather to gather together the predictions and labels from each process. A distributed training script has to be launched several times on different processes (for instance one per GPU you are using). You can use the PyTorch tools if you are familiar with them, but Accelerate also provides an easy API to configure your setup and launch your training script. In a terminal, run accelerate config and answer the small questionnaire to generate a configuration file with all the relevant information, then you can just run accelerate launch, followed by the path to your training script. In a notebook, you can use the notebook_launcher function to launch your training function.", "You can enable gradient accumulation by adding the `gradient_accumulation_steps` argument to  [`TrainingArguments`]: \n\n```py\ntraining_args = TrainingArguments(per_device_train_batch_size=1, gradient_accumulation_steps=4, **default_args)\n```\n\nIn the above example, your effective batch size becomes 4. \n\nAlternatively, use \ud83e\udd17 Accelerate to gain full control over the training loop. Find the \ud83e\udd17 Accelerate example \n[further down in this guide](#using-accelerate).\n\nWhile it is advised to max out GPU usage as much as possible, a high number of gradient accumulation steps can \nresult in a more pronounced training slowdown. Consider the following example. Let's say, the `per_device_train_batch_size=4` \nwithout gradient accumulation hits the GPU's limit. If you would like to train with batches of size 64, do not set the \n`per_device_train_batch_size` to 1 and `gradient_accumulation_steps` to 64. Instead, keep `per_device_train_batch_size=4` \nand set `gradient_accumulation_steps=16`. This results in the same effective batch size while making better use of \nthe available GPU resources.\n\nFor additional information, please refer to batch size and gradient accumulation benchmarks for [RTX-3090](https://github.com/huggingface/transformers/issues/14608#issuecomment-1004392537)\nand [A100](https://github.com/huggingface/transformers/issues/15026#issuecomment-1005033957).\n\n## Gradient Checkpointing\n\nSome large models may still face memory issues even when the batch size is set to 1 and gradient accumulation is used. \nThis is because there are other components that also require memory storage.\n\nSaving all activations from the forward pass in order to compute the gradients during the backward pass can result in \nsignificant memory overhead. The alternative approach of discarding the activations and recalculating them when needed \nduring the backward pass, would introduce a considerable computational overhead and slow down the training process.\n\n**Gradient checkpointing** offers a compromise between these two approaches and saves strategically selected activations \nthroughout the computational graph so only a fraction of the activations need to be re-computed for the gradients. For \nan in-depth explanation of gradient checkpointing, refer to [this great article](https://medium.com/tensorflow/fitting-larger-networks-into-memory-583e3c758ff9).\n\nTo enable gradient checkpointing in the [`Trainer`], pass the corresponding a flag to [`TrainingArguments`]:\n\n```py\ntraining_args = TrainingArguments(\n    per_device_train_batch_size=1, gradient_accumulation_steps=4, gradient_checkpointing=True, **default_args\n)\n```\n\nAlternatively, use \ud83e\udd17 Accelerate - find the \ud83e\udd17 Accelerate example [further in this guide](#using-accelerate). \n\n<Tip>\n\nWhile gradient checkpointing may improve memory efficiency, it slows training by approximately 20%.\n\n</Tip>\n\n## Mixed precision training\n\n**Mixed precision training** is a technique that aims to optimize the computational efficiency of training models by \nutilizing lower-precision numerical formats for certain variables. Traditionally, most models use 32-bit floating point \nprecision (fp32 or float32) to represent and process variables. However, not all variables require this high precision \nlevel to achieve accurate results. By reducing the precision of certain variables to lower numerical formats like 16-bit \nfloating point (fp16 or float16), we can speed up the computations. Because in this approach some computations are performed \nin half-precision, while some are still in full precision, the approach is called mixed precision training.\n\nMost commonly mixed precision training is achieved by using fp16 (float16) data types, however, some GPU architectures \n(such as the Ampere architecture) offer bf16 and tf32 (CUDA internal data type) data types. Check \nout the [NVIDIA Blog](https://developer.nvidia.com/blog/accelerating-ai-training-with-tf32-tensor-cores/) to learn more about \nthe differences between these data types.\n\n### fp16\n\nThe main advantage of mixed precision training comes from saving the activations in half precision (fp16). \nAlthough the gradients are also computed in half precision they are converted back to full precision for the optimization \nstep so no memory is saved here. \nWhile mixed precision training results in faster computations, it can also lead to more GPU memory being utilized, especially for small batch sizes.\nThis is because the model is now present on the GPU in both 16-bit and 32-bit precision (1.5x the original model on the GPU).\n\nTo enable mixed precision training, set the `fp16` flag to `True`:\n\n```py\ntraining_args = TrainingArguments(per_device_train_batch_size=4, fp16=True, **default_args)\n```\n\nIf you prefer to use \ud83e\udd17 Accelerate, find the \ud83e\udd17 Accelerate example [further in this guide](#using-accelerate). \n\n### BF16\n\nIf you have access to an Ampere or newer hardware you can use bf16 for mixed precision training and evaluation. While \nbf16 has a worse precision than fp16, it has a much...", "--\ntitle: \"Introducing \ud83e\udd17 Accelerate\"\nthumbnail: /blog/assets/20_accelerate_library/accelerate_diff.png\nauthors:\n- user: sgugger\n---\n\n# Introducing \ud83e\udd17 Accelerate\n\n\n## \ud83e\udd17 Accelerate\n\nRun your **raw** PyTorch training scripts on any kind of device.\n\nMost high-level libraries above PyTorch provide support for distributed training and mixed precision, but the abstraction they introduce require a user to learn a new API if they want to customize the underlying training loop. \ud83e\udd17 Accelerate was created for PyTorch users who like to have full control over their training loops but are reluctant to write (and maintain) the boilerplate code needed to use distributed training (for multi-GPU on one or several nodes, TPUs, ...) or mixed precision training. Plans forward include support for fairscale, deepseed, AWS SageMaker specific data-parallelism and model parallelism.\n\nIt provides two things: a simple and consistent API that abstracts that boilerplate code and a launcher command to easily run those scripts on various setups.\n\n### Easy integration!\n\nLet's first have a look at an example:\n\n```diff\n  import torch\n  import torch.nn.functional as F\n  from datasets import load_dataset\n+ from accelerate import Accelerator\n\n+ accelerator = Accelerator()\n- device = 'cpu'\n+ device = accelerator.device\n\n  model = torch.nn.Transformer().to(device)\n  optim = torch.optim.Adam(model.parameters())\n\n  dataset = load_dataset('my_dataset')\n  data = torch.utils.data.DataLoader(dataset, shuffle=True)\n\n+ model, optim, data = accelerator.prepare(model, optim, data)\n\n  model.train()\n  for epoch in range(10):\n      for source, targets in data:\n          source = source.to(device)\n          targets = targets.to(device)\n\n          optimizer.zero_grad()\n\n          output = model(source)\n          loss = F.cross_entropy(output, targets)\n\n-         loss.backward()\n+         accelerator.backward(loss)\n\n          optimizer.step()\n```\n\nBy just adding five lines of code to any standard PyTorch training script, you can now run said script on any kind of distributed setting, as well as with or without mixed precision. \ud83e\udd17 Accelerate even handles the device placement for you, so you can simplify the training loop above even further:\n\n```diff\n  import torch\n  import torch.nn.functional as F\n  from datasets import load_dataset\n+ from accelerate import Accelerator\n\n+ accelerator = Accelerator()\n- device = 'cpu'\n\n- model = torch.nn.Transformer().to(device)\n+ model = torch.nn.Transformer()\n  optim = torch.optim.Adam(model.parameters())\n\n  dataset = load_dataset('my_dataset')\n  data = torch.utils.data.DataLoader(dataset, shuffle=True)\n\n+ model, optim, data = accelerator.prepare(model, optim, data)\n\n  model.train()\n  for epoch in range(10):\n      for source, targets in data:\n-         source = source.to(device)\n-         targets = targets.to(device)\n\n          optimizer.zero_grad()\n\n          output = model(source)\n          loss = F.cross_entropy(output, targets)\n\n-         loss.backward()\n+         accelerator.backward(loss)\n\n          optimizer.step()\n```\n\nIn contrast, here are the changes needed to have this code run with distributed training are the followings:\n\n```diff\n+ import os\n  import torch\n  import torch.nn.functional as F\n  from datasets import load_dataset\n+ from torch.utils.data import DistributedSampler\n+ from torch.nn.parallel import DistributedDataParallel\n\n+ local_rank = int(os.environ.get(\"LOCAL_RANK\", -1))\n- device = 'cpu'\n+ device = device = torch.device(\"cuda\", local_rank)\n\n  model = torch.nn.Transformer().to(device)\n+ model = DistributedDataParallel(model)  \n  optim = torch.optim.Adam(model.parameters())\n\n  dataset = load_dataset('my_dataset')\n+ sampler = DistributedSampler(dataset)\n- data = torch.utils.data.DataLoader(dataset, shuffle=True)\n+ data = torch.utils.data.DataLoader(dataset, sampler=sampler)\n\n  model.train()\n  for epoch in range(10):\n+     sampler.set_epoch(epoch)  \n      for source, targets in data:\n          source = source.to(device)\n          targets = targets.to(device)\n\n          optimizer.zero_grad()\n\n          output = model(source)\n          loss = F.cross_entropy(output, targets)\n\n          loss.backward()\n\n          optimizer.step()\n```\n\nThese changes will make your training script work for multiple GPUs, but your script will then stop working on CPU or one GPU (unless you start adding if statements everywhere). Even more annoying, if you wanted to test your script on TPUs you would need to change different lines of codes. Same for mixed precision training. The promise of \ud83e\udd17 Accelerate is:\n- to keep the changes to your training loop to the bare minimum so you have to learn as little as possible.\n- to have the same functions work for any distributed setup, so only have to learn one API.\n\n### How does it work?\n\nTo see how the library works in practice, let's have a look at each line of code we need to add to a training loop.\n\n```python\naccelerator = Accelerator()\n```\n\nOn top of giving the main object that you will use, thi..."], "test_settings": "chunk_7000_embeddings_openai-text-embedding-3-small_reader-model_gpt-4", "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response is mostly correct and provides a valid method for enabling mixed precision training in the \ud83e\udd17 Accelerate library. It mentions using the `mixed_precision` argument with values `'fp16'` or `'bf16'`, which are indeed valid options for specifying the desired precision. However, the reference answer suggests using `fp16=True`, which is a different approach. The response could be improved by aligning more closely with the reference answer or by clarifying the difference between the two methods. Overall, the response is accurate but slightly deviates from the expected answer format."}, {"question": "What is the purpose of tokenizers in the NLP pipeline?\n", "true_answer": "To translate text into data that can be processed by the model.", "source_doc": "huggingface/course/blob/main/chapters/en/chapter2/4.mdx", "generated_answer": "The purpose of tokenizers in the NLP pipeline is to translate raw text into numbers, which machine learning models can process. Tokenizers achieve this by converting the text into a meaningful numerical representation through various tokenization algorithms. This step is essential because machine learning models cannot directly read and understand raw text. [\ubb38\uc11c 0]", "retrieved_docs": ["n these few videos, we'll take a look at the tokenizers. In Natural Language Processing, most of the data that we handle consists of raw text. However, machine learning models cannot read and understand text in its raw form they can only work with numbers. The tokenizer's objective will be to translate the text into numbers. There are several possible approaches to this conversion, and the objective is to find the most meaningful representation. We'll take a look at three distinct tokenization algorithms. We compare them one to one, so we recommend you look at the videos in the following order: Word-based, Character-based, and Subword-based.", "The tokenization pipeline\n\nWhen calling `Tokenizer.encode` or\n`Tokenizer.encode_batch`, the input\ntext(s) go through the following pipeline:\n\n-   `normalization`\n-   `pre-tokenization`\n-   `model`\n-   `post-processing`\n\nWe'll see in details what happens during each of those steps in detail,\nas well as when you want to `decode <decoding>` some token ids, and how the \ud83e\udd17 Tokenizers library allows you\nto customize each of those steps to your needs. If you're already\nfamiliar with those steps and want to learn by seeing some code, jump to\n`our BERT from scratch example <example>`.\n\nFor the examples that require a `Tokenizer` we will use the tokenizer we trained in the\n`quicktour`, which you can load with:\n\n<tokenizerslangcontent>\n<python>\n<literalinclude>\n{\"path\": \"../../bindings/python/tests/documentation/test_pipeline.py\",\n\"language\": \"python\",\n\"start-after\": \"START reload_tokenizer\",\n\"end-before\": \"END reload_tokenizer\",\n\"dedent\": 12}\n</literalinclude>\n</python>\n<rust>\n<literalinclude>\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\n\"language\": \"rust\",\n\"start-after\": \"START pipeline_reload_tokenizer\",\n\"end-before\": \"END pipeline_reload_tokenizer\",\n\"dedent\": 4}\n</literalinclude>\n</rust>\n<node>\n<literalinclude>\n{\"path\": \"../../bindings/node/examples/documentation/pipeline.test.ts\",\n\"language\": \"js\",\n\"start-after\": \"START reload_tokenizer\",\n\"end-before\": \"END reload_tokenizer\",\n\"dedent\": 8}\n</literalinclude>\n</node>\n</tokenizerslangcontent>\n\n## Normalization\n\nNormalization is, in a nutshell, a set of operations you apply to a raw\nstring to make it less random or \"cleaner\". Common operations include\nstripping whitespace, removing accented characters or lowercasing all\ntext. If you're familiar with [Unicode\nnormalization](https://unicode.org/reports/tr15), it is also a very\ncommon normalization operation applied in most tokenizers.\n\nEach normalization operation is represented in the \ud83e\udd17 Tokenizers library\nby a `Normalizer`, and you can combine\nseveral of those by using a `normalizers.Sequence`. Here is a normalizer applying NFD Unicode normalization\nand removing accents as an example:\n\n<tokenizerslangcontent>\n<python>\n<literalinclude>\n{\"path\": \"../../bindings/python/tests/documentation/test_pipeline.py\",\n\"language\": \"python\",\n\"start-after\": \"START setup_normalizer\",\n\"end-before\": \"END setup_normalizer\",\n\"dedent\": 8}\n</literalinclude>\n</python>\n<rust>\n<literalinclude>\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\n\"language\": \"rust\",\n\"start-after\": \"START pipeline_setup_normalizer\",\n\"end-before\": \"END pipeline_setup_normalizer\",\n\"dedent\": 4}\n</literalinclude>\n</rust>\n<node>\n<literalinclude>\n{\"path\": \"../../bindings/node/examples/documentation/pipeline.test.ts\",\n\"language\": \"js\",\n\"start-after\": \"START setup_normalizer\",\n\"end-before\": \"END setup_normalizer\",\n\"dedent\": 8}\n</literalinclude>\n</node>\n</tokenizerslangcontent>\n\nYou can manually test that normalizer by applying it to any string:\n\n<tokenizerslangcontent>\n<python>\n<literalinclude>\n{\"path\": \"../../bindings/python/tests/documentation/test_pipeline.py\",\n\"language\": \"python\",\n\"start-after\": \"START test_normalizer\",\n\"end-before\": \"END test_normalizer\",\n\"dedent\": 8}\n</literalinclude>\n</python>\n<rust>\n<literalinclude>\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\n\"language\": \"rust\",\n\"start-after\": \"START pipeline_test_normalizer\",\n\"end-before\": \"END pipeline_test_normalizer\",\n\"dedent\": 4}\n</literalinclude>\n</rust>\n<node>\n<literalinclude>\n{\"path\": \"../../bindings/node/examples/documentation/pipeline.test.ts\",\n\"language\": \"js\",\n\"start-after\": \"START test_normalizer\",\n\"end-before\": \"END test_normalizer\",\n\"dedent\": 8}\n</literalinclude>\n</node>\n</tokenizerslangcontent>\n\nWhen building a `Tokenizer`, you can\ncustomize its normalizer by just changing the corresponding attribute:\n\n<tokenizerslangcontent>\n<python>\n<literalinclude>\n{\"path\": \"../../bindings/python/tests/documentation/test_pipeline.py\",\n\"language\": \"python\",\n\"start-after\": \"START replace_normalizer\",\n\"end-before\": \"END replace_normalizer\",\n\"dedent\": 8}\n</literalinclude>\n</python>\n<rust>\n<literalinclude>\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\n\"language\": \"rust\",\n\"start-after\": \"START pipeline_replace_normalizer\",\n\"end-before\": \"END pipeline_replace_normalizer\",\n\"dedent\": 4}\n</literalinclude>\n</rust>\n<node>\n<literalinclude>\n{\"path\": \"../../bindings/node/examples/documentation/pipeline.test.ts\",\n\"language\": \"js\",\n\"start-after\": \"START replace_normalizer\",\n\"end-before\": \"END replace_normalizer\",\n\"dedent\": 8}\n</literalinclude>\n</node>\n</tokenizerslangcontent>\n\nOf course, if you change the way a tokenizer applies normalization, you\nshould probably retrain it from scratch afterward.\n\n## Pre-Tokenization\n\nPre-tokenization is the act of splitting a text into smaller objects\nthat give an upper bound to what your tokens will be at the end of\ntraining. A good way to think of this is that the pre-tokenizer will\nsplit your text into \"words\" and then, your final tokens will be parts\nof those words.\n\nAn...", "Components\n\nWhen building a Tokenizer, you can attach various types of components to\nthis Tokenizer in order to customize its behavior. This page lists most\nprovided components.\n\n## Normalizers\n\nA `Normalizer` is in charge of pre-processing the input string in order\nto normalize it as relevant for a given use case. Some common examples\nof normalization are the Unicode normalization algorithms (NFD, NFKD,\nNFC & NFKC), lowercasing etc... The specificity of `tokenizers` is that\nwe keep track of the alignment while normalizing. This is essential to\nallow mapping from the generated tokens back to the input text.\n\nThe `Normalizer` is optional.\n\n<tokenizerslangcontent>\n<python>\n| Name | Description | Example |\n| :--- | :--- | :--- |\n| NFD | NFD unicode normalization |  |\n| NFKD | NFKD unicode normalization |  |\n| NFC | NFC unicode normalization |  |\n| NFKC | NFKC unicode normalization |  |\n| Lowercase | Replaces all uppercase to lowercase | Input: `HELLO \u1f48\u0394\u03a5\u03a3\u03a3\u0395\u038e\u03a3` <br> Output: `hello`\u1f40\u03b4\u03c5\u03c3\u03c3\u03b5\u03cd\u03c2`  |\n| Strip | Removes all whitespace characters on the specified sides (left, right or both) of the input | Input: `\"`hi`\"` <br> Output: `\"hi\"`  |\n| StripAccents | Removes all accent symbols in unicode (to be used with NFD for consistency) | Input: `\u00e9` <br> Ouput: `e`  |\n| Replace | Replaces a custom string or regexp and changes it with given content | `Replace(\"a\", \"e\")` will behave like this: <br> Input: `\"banana\"` <br> Ouput: `\"benene\"`  |\n| BertNormalizer | Provides an implementation of the Normalizer used in the original BERT. Options that can be set are: <ul> <li>clean_text</li> <li>handle_chinese_chars</li> <li>strip_accents</li> <li>lowercase</li> </ul>  |  |\n| Sequence | Composes multiple normalizers that will run in the provided order | `Sequence([NFKC(), Lowercase()])` |\n</python>\n<rust>\n| Name | Description | Example |\n| :--- | :--- | :--- |\n| NFD | NFD unicode normalization |  |\n| NFKD | NFKD unicode normalization |  |\n| NFC | NFC unicode normalization |  |\n| NFKC | NFKC unicode normalization |  |\n| Lowercase | Replaces all uppercase to lowercase | Input: `HELLO \u1f48\u0394\u03a5\u03a3\u03a3\u0395\u038e\u03a3` <br> Output: `hello`\u1f40\u03b4\u03c5\u03c3\u03c3\u03b5\u03cd\u03c2`  |\n| Strip | Removes all whitespace characters on the specified sides (left, right or both) of the input | Input: `\"`hi`\"` <br> Output: `\"hi\"`  |\n| StripAccents | Removes all accent symbols in unicode (to be used with NFD for consistency) | Input: `\u00e9` <br> Ouput: `e`  |\n| Replace | Replaces a custom string or regexp and changes it with given content | `Replace(\"a\", \"e\")` will behave like this: <br> Input: `\"banana\"` <br> Ouput: `\"benene\"`  |\n| BertNormalizer | Provides an implementation of the Normalizer used in the original BERT. Options that can be set are: <ul> <li>clean_text</li> <li>handle_chinese_chars</li> <li>strip_accents</li> <li>lowercase</li> </ul>  |  |\n| Sequence | Composes multiple normalizers that will run in the provided order | `Sequence::new(vec![NFKC, Lowercase])` |\n</rust>\n<node>\n| Name | Description | Example |\n| :--- | :--- | :--- |\n| NFD | NFD unicode normalization |  |\n| NFKD | NFKD unicode normalization |  |\n| NFC | NFC unicode normalization |  |\n| NFKC | NFKC unicode normalization |  |\n| Lowercase | Replaces all uppercase to lowercase | Input: `HELLO \u1f48\u0394\u03a5\u03a3\u03a3\u0395\u038e\u03a3` <br> Output: `hello`\u1f40\u03b4\u03c5\u03c3\u03c3\u03b5\u03cd\u03c2`  |\n| Strip | Removes all whitespace characters on the specified sides (left, right or both) of the input | Input: `\"`hi`\"` <br> Output: `\"hi\"`  |\n| StripAccents | Removes all accent symbols in unicode (to be used with NFD for consistency) | Input: `\u00e9` <br> Ouput: `e`  |\n| Replace | Replaces a custom string or regexp and changes it with given content | `Replace(\"a\", \"e\")` will behave like this: <br> Input: `\"banana\"` <br> Ouput: `\"benene\"`  |\n| BertNormalizer | Provides an implementation of the Normalizer used in the original BERT. Options that can be set are: <ul> <li>cleanText</li> <li>handleChineseChars</li> <li>stripAccents</li> <li>lowercase</li> </ul>  |  |\n| Sequence | Composes multiple normalizers that will run in the provided order | |\n</node>\n</tokenizerslangcontent>\n\n## Pre-tokenizers\n\nThe `PreTokenizer` takes care of splitting the input according to a set\nof rules. This pre-processing lets you ensure that the underlying\n`Model` does not build tokens across multiple \"splits\". For example if\nyou don't want to have whitespaces inside a token, then you can have a\n`PreTokenizer` that splits on these whitespaces.\n\nYou can easily combine multiple `PreTokenizer` together using a\n`Sequence` (see below). The `PreTokenizer` is also allowed to modify the\nstring, just like a `Normalizer` does. This is necessary to allow some\ncomplicated algorithms that require to split before normalizing (e.g.\nthe ByteLevel)"], "test_settings": "chunk_7000_embeddings_openai-text-embedding-3-small_reader-model_gpt-4", "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response accurately describes the purpose of tokenizers in the NLP pipeline, which is to convert raw text into a numerical format that machine learning models can process. It correctly highlights the necessity of this step due to the inability of models to directly interpret raw text. The explanation is detailed and aligns well with the criteria for a correct and factual response. However, it could be slightly more concise to match the brevity of the ideal answer. Overall, the response is mostly accurate and factual, fulfilling the requirements of the task."}, {"question": "What is the purpose of the Safety Checker in the Diffusers library?\n", "true_answer": "The Safety Checker checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated to mitigate the risk of generating harmful content.", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/conceptual/ethical_guidelines.md", "generated_answer": "The purpose of the Safety Checker in the Diffusers library is to check and compare the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated. This is done to mitigate inappropriate content generation and ensure safer deployments by preventing the generation of harmful or inappropriate images [\ubb38\uc11c 2].", "retrieved_docs": ["!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n<p align=\"center\">\n    <br>\n    <img src=\"https://raw.githubusercontent.com/huggingface/diffusers/77aadfee6a891ab9fcfb780f87c693f7a5beeb8e/docs/source/imgs/diffusers_library.jpg\" width=\"400\"/>\n    <br>\n</p>\n\n# Diffusers\n\n\ud83e\udd17 Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you're looking for a simple inference solution or want to train your own diffusion model, \ud83e\udd17 Diffusers is a modular toolbox that supports both. Our library is designed with a focus on [usability over performance](conceptual/philosophy#usability-over-performance), [simple over easy](conceptual/philosophy#simple-over-easy), and [customizability over abstractions](conceptual/philosophy#tweakable-contributorfriendly-over-abstraction).\n\nThe library has three main components:\n\n- State-of-the-art diffusion pipelines for inference with just a few lines of code. There are many pipelines in \ud83e\udd17 Diffusers, check out the table in the pipeline [overview](api/pipelines/overview) for a complete list of available pipelines and the task they solve.\n- Interchangeable [noise schedulers](api/schedulers/overview) for balancing trade-offs between generation speed and quality.\n- Pretrained [models](api/models) that can be used as building blocks, and combined with schedulers, for creating your own end-to-end diffusion systems.\n\n<div class=\"mt-10\">\n  <div class=\"w-full flex flex-col space-y-4 md:space-y-0 md:grid md:grid-cols-2 md:gap-y-4 md:gap-x-5\">\n    <a class=\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\" href=\"./tutorials/tutorial_overview\"\n      ><div class=\"w-full text-center bg-gradient-to-br from-blue-400 to-blue-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed\">Tutorials</div>\n      <p class=\"text-gray-700\">Learn the fundamental skills you need to start generating outputs, build your own diffusion system, and train a diffusion model. We recommend starting here if you're using \ud83e\udd17 Diffusers for the first time!</p>\n    </a>\n    <a class=\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\" href=\"./using-diffusers/loading_overview\"\n      ><div class=\"w-full text-center bg-gradient-to-br from-indigo-400 to-indigo-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed\">How-to guides</div>\n      <p class=\"text-gray-700\">Practical guides for helping you load pipelines, models, and schedulers. You'll also learn how to use pipelines for specific tasks, control how outputs are generated, optimize for inference speed, and different training techniques.</p>\n    </a>\n    <a class=\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\" href=\"./conceptual/philosophy\"\n      ><div class=\"w-full text-center bg-gradient-to-br from-pink-400 to-pink-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed\">Conceptual guides</div>\n      <p class=\"text-gray-700\">Understand why the library was designed the way it was, and learn more about the ethical guidelines and safety implementations for using the library.</p>\n   </a>\n    <a class=\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\" href=\"./api/models/overview\"\n      ><div class=\"w-full text-center bg-gradient-to-br from-purple-400 to-purple-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed\">Reference</div>\n      <p class=\"text-gray-700\">Technical descriptions of how \ud83e\udd17 Diffusers classes and methods work.</p>\n    </a>\n  </div>\n</div>", "\ud83e\udde8 Diffusers Experimental\n\nWe are adding experimental code to support novel applications and usages of the Diffusers library.\nCurrently, the following experiments are supported:\n* Reinforcement learning via an implementation of the [Diffuser](https://arxiv.org/abs/2205.09991) model.", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# \ud83e\udde8 Diffusers\u2019 Ethical Guidelines\n\n## Preamble\n\n[Diffusers](https://huggingface.co/docs/diffusers/index) provides pre-trained diffusion models and serves as a modular toolbox for inference and training.\n\nGiven its real case applications in the world and potential negative impacts on society, we think it is important to provide the project with ethical guidelines to guide the development, users\u2019 contributions, and usage of the Diffusers library.\n\nThe risks associated with using this technology are still being examined, but to name a few: copyrights issues for artists; deep-fake exploitation; sexual content generation in inappropriate contexts; non-consensual impersonation; harmful social biases perpetuating the oppression of marginalized groups.\nWe will keep tracking risks and adapt the following guidelines based on the community's responsiveness and valuable feedback.\n\n\n## Scope\n\nThe Diffusers community will apply the following ethical guidelines to the project\u2019s development and help coordinate how the community will integrate the contributions, especially concerning sensitive topics related to ethical concerns.\n\n\n## Ethical guidelines\n\nThe following ethical guidelines apply generally, but we will primarily implement them when dealing with ethically sensitive issues while making a technical choice. Furthermore, we commit to adapting those ethical principles over time following emerging harms related to the state of the art of the technology in question.\n\n- **Transparency**: we are committed to being transparent in managing PRs, explaining our choices to users, and making technical decisions.\n\n- **Consistency**: we are committed to guaranteeing our users the same level of attention in project management, keeping it technically stable and consistent.\n\n- **Simplicity**: with a desire to make it easy to use and exploit the Diffusers library, we are committed to keeping the project\u2019s goals lean and coherent.\n\n- **Accessibility**: the Diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. Doing so makes research artifacts more accessible to the community.\n\n- **Reproducibility**: we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the Diffusers library.\n\n- **Responsibility**: as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology's potential risks and dangers.\n\n\n## Examples of implementations: Safety features and Mechanisms\n\nThe team works daily to make the technical and non-technical tools available to deal with the potential ethical and social risks associated with diffusion technology. Moreover, the community's input is invaluable in ensuring these features' implementation and raising awareness with us.\n\n- [**Community tab**](https://huggingface.co/docs/hub/repositories-pull-requests-discussions): it enables the community to discuss and better collaborate on a project.\n\n- **Bias exploration and evaluation**: the Hugging Face team provides a [space](https://huggingface.co/spaces/society-ethics/DiffusionBiasExplorer) to demonstrate the biases in Stable Diffusion interactively. In this sense, we support and encourage bias explorers and evaluations.\n\n- **Encouraging safety in deployment**\n\n  - [**Safe Stable Diffusion**](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_safe): It mitigates the well-known issue that models, like Stable Diffusion, that are trained on unfiltered, web-crawled datasets tend to suffer from inappropriate degeneration. Related paper: [Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models](https://arxiv.org/abs/2211.05105).\n\n  - [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated. The harmful concepts are intentionally hidden to prevent reverse engineering of the checker.\n\n- **Staged released on the Hub**: in particularly sensitive situations, access to some repositories should be restricted. This staged release is an intermediary step that allows the repository\u2019s authors to have more control over its use.\n\n- **Lice..."], "test_settings": "chunk_7000_embeddings_openai-text-embedding-3-small_reader-model_gpt-4", "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response accurately describes the purpose of the Safety Checker in the Diffusers library. It correctly explains that the Safety Checker compares the class probability of harmful concepts in the embedding space against an image after it has been generated. The response also mentions the goal of mitigating inappropriate content generation and ensuring safer deployments, which aligns with the purpose outlined in the criteria. The inclusion of \"[\ubb38\uc11c 2]\" is unnecessary and does not affect the accuracy of the response. Overall, the response is precise and factual, meeting the criteria for a full score."}, {"question": "What Python class allows you to retrieve Discussions and Pull Requests from a given repository on the Hugging Face Hub?\n", "true_answer": "HfApi", "source_doc": "huggingface/huggingface_hub/blob/main/docs/source/en/guides/community.md", "generated_answer": "The Python class that allows you to retrieve Discussions and Pull Requests from a given repository on the Hugging Face Hub is the `HfApi` class. [\ubb38\uc11c 0]", "retrieved_docs": ["!--\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n-->\n\n# Interact with Discussions and Pull Requests \n\nThe `huggingface_hub` library provides a Python interface to interact with Pull Requests and Discussions on the Hub.\nVisit [the dedicated documentation page](https://huggingface.co/docs/hub/repositories-pull-requests-discussions)\nfor a deeper view of what Discussions and Pull Requests on the Hub are, and how they work under the hood.\n\n## Retrieve Discussions and Pull Requests from the Hub\n\nThe `HfApi` class allows you to retrieve Discussions and Pull Requests on a given repo:\n\n```python\n>>> from huggingface_hub import get_repo_discussions\n>>> for discussion in get_repo_discussions(repo_id=\"bigscience/bloom\"):\n...     print(f\"{discussion.num} - {discussion.title}, pr: {discussion.is_pull_request}\")\n\n# 11 - Add Flax weights, pr: True\n# 10 - Update README.md, pr: True\n# 9 - Training languages in the model card, pr: True\n# 8 - Update tokenizer_config.json, pr: True\n# 7 - Slurm training script, pr: False\n[...]\n```\n\n`HfApi.get_repo_discussions` supports filtering by author, type (Pull Request or Discussion) and status (`open` or `closed`):\n\n```python\n>>> from huggingface_hub import get_repo_discussions\n>>> for discussion in get_repo_discussions(\n...    repo_id=\"bigscience/bloom\",\n...    author=\"ArthurZ\",\n...    discussion_type=\"pull_request\",\n...    discussion_status=\"open\",\n... ):\n...     print(f\"{discussion.num} - {discussion.title} by {discussion.author}, pr: {discussion.is_pull_request}\")\n\n# 19 - Add Flax weights by ArthurZ, pr: True\n```\n\n`HfApi.get_repo_discussions` returns a [generator](https://docs.python.org/3.7/howto/functional.html#generators) that yields\n[`Discussion`] objects. To get all the Discussions in a single list, run:\n\n```python\n>>> from huggingface_hub import get_repo_discussions\n>>> discussions_list = list(get_repo_discussions(repo_id=\"bert-base-uncased\"))\n```\n\nThe [`Discussion`] object returned by [`HfApi.get_repo_discussions`] contains high-level overview of the\nDiscussion or Pull Request. You can also get more detailed information using [`HfApi.get_discussion_details`]:\n\n```python\n>>> from huggingface_hub import get_discussion_details\n\n>>> get_discussion_details(\n...     repo_id=\"bigscience/bloom-1b3\",\n...     discussion_num=2\n... )\nDiscussionWithDetails(\n    num=2,\n    author='cakiki',\n    title='Update VRAM memory for the V100s',\n    status='open',\n    is_pull_request=True,\n    events=[\n        DiscussionComment(type='comment', author='cakiki', ...),\n        DiscussionCommit(type='commit', author='cakiki', summary='Update VRAM memory for the V100s', oid='1256f9d9a33fa8887e1c1bf0e09b4713da96773a', ...),\n    ],\n    conflicting_files=[],\n    target_branch='refs/heads/main',\n    merge_commit_oid=None,\n    diff='diff --git a/README.md b/README.md\\nindex a6ae3b9294edf8d0eda0d67c7780a10241242a7e..3a1814f212bc3f0d3cc8f74bdbd316de4ae7b9e3 100644\\n--- a/README.md\\n+++ b/README.md\\n@@ -132,7 +132,7 [...]',\n)\n```\n\n[`HfApi.get_discussion_details`] returns a [`DiscussionWithDetails`] object, which is a subclass of [`Discussion`]\nwith more detailed information about the Discussion or Pull Request. Information includes all the comments, status changes,\nand renames of the Discussion via [`DiscussionWithDetails.events`].\n\nIn case of a Pull Request, you can retrieve the raw git diff with [`DiscussionWithDetails.diff`]. All the commits of the\nPull Request are listed in [`DiscussionWithDetails.events`].\n\n\n## Create and edit a Discussion or Pull Request programmatically\n\nThe [`HfApi`] class also offers ways to create and edit Discussions and Pull Requests.\nYou will need an [access token](https://huggingface.co/docs/hub/security-tokens) to create and edit Discussions\nor Pull Requests.\n\nThe simplest way to propose changes on a repo on the Hub is via the [`create_commit`] API: just \nset the `create_pr` parameter to `True`. This parameter is also available on other methods that wrap [`create_commit`]:\n\n    * [`upload_file`]\n    * [`upload_folder`]\n    * [`delete_file`]\n    * [`delete_folder`]\n    * [`metadata_update`]\n\n```python\n>>> from huggingface_hub import metadata_update\n\n>>> metadata_update(\n...     repo_id=\"username/repo_name\",\n...     metadata={\"tags\": [\"computer-vision\", \"awesome-model\"]},\n...     create_pr=True,\n... )\n```\n\nYou can also use [`HfApi.create_discussion`] (respectively [`HfApi.create_pull_request`]) to create a Discussion (respectively a Pull Request) on a repo.\nOpening a Pull Request this way can be useful if you need to work on changes locally. Pull Requests opened this way will be in `\"draft\"` mode.\n\n```python\n>>> from huggingface_hub import create_discussion, create_pull_request\n\n>>> create_discussion(\n...     repo_id=\"username/repo-name\",\n...     title=\"Hi from the huggingface_hub library!\",\n...     token=\"<insert your access token here>\",\n... )\nDiscussionWithDetails(...)...", "--\ntitle: Introducing Pull Requests and Discussions \ud83e\udd73\nthumbnail: /blog/assets/76_community_update/thumbnail.png\n---\n\n# Introducing Pull Requests and Discussions \ud83e\udd73\n\n![Pull requests and discussions on the hub](assets/76_community_update/community-update.png)\n\nWe are thrilled to announce the release of our latest collaborative features: pull requests and discussions on the Hugging Face Hub!\n\nPull requests and discussions are available today under the [community tab](https://huggingface.co/gpt2/discussions) for all repository types: models, datasets, and Spaces. Any member of the community can create and participate in discussions and pull requests, facilitating collaborations not only within teams, but also with everyone else in the community!\n\nIt's the biggest update ever done to the Hub, and we can't wait to see the community members start collaborating with it \ud83e\udd29.\n\nThe new \"Community\" tab also aligns with proposals in ethical ML throughout the years. Feedback and iterations have a central place in the development of ethical machine learning software. We really believe having it in the community's toolset will unlock new kinds of positive patterns in ML, collaborations, and progress.\n\nSome example use cases for discussions and pull requests:\n\n- Propose suggestions in model cards to improve disclosures of ethical biases.\n- Let users flag concerning generations of a given Space demo.\n- Provide a venue through which model and dataset authors can have a direct discussion with community members.\n- Allow others to improve your repositories! For example, users might want to provide TensorFlow weights!\n\n## Discussions\n\n![Discussions on the Hugging Face Hub](assets/76_community_update/new-discussion.png)\n\n[Discussions](https://huggingface.co/gpt2/discussions?type=discussion) allow community members ask and answer questions as well as share their ideas and suggestions directly with the repository owners and the community. Anyone can create and participate in discussions in the community tab of a repository.\n\n## Pull requests\n\n![Pull requests on the Hugging Face Hub](assets/76_community_update/new-pr.png)\n\n[Pull requests](https://huggingface.co/gpt2/discussions?type=pull_request) allow community members open, comment, merge, or close pull requests directly from the website. The easiest way to open a pull request is to use the \"Collaborate\" button in the \"Files and versions\" tab. It will let you do single file contributions very easily.\n\nUnder the hood, our Pull requests do not use forks and branches, but instead, custom \"branches\" called `refs` that are stored directly on the source repo. This approach to avoids the need to create a forks for each new version of the model/dataset.\n\n## How is this different from other git hosts\n\nAt a high level, we aim to build a simpler version of other git hosts' (like GitHub's) PRs and Issues:\n\n- no forks are involved: contributors push to a special `ref` branch directly on the source repo\n- no hard distinction between issues and PRs: they are essentially the same so we display them in the same lists\n- streamlined for ML (i.e. models/datasets/Spaces repos), not arbitrary repos\n\n## What's next\n\nOf course, it's only the beginning. We will listen to the community feedback to add new features and improve the community tab in the future. If you have any feedback, you can [join the discussion here](https://huggingface.co/spaces/huggingface/HuggingDiscussions/discussions/1). Today is the best time to join your first discussion and open a PR! \ud83e\udd17", "!--\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n-->\n\n# Interacting with Discussions and Pull Requests\n\nCheck the [`HfApi`] documentation page for the reference of methods enabling\ninteraction with Pull Requests and Discussions on the Hub.\n\n- [`get_repo_discussions`]\n- [`get_discussion_details`]\n- [`create_discussion`]\n- [`create_pull_request`]\n- [`rename_discussion`]\n- [`comment_discussion`]\n- [`edit_discussion_comment`]\n- [`change_discussion_status`]\n- [`merge_pull_request`]\n\n## Data structures\n\n[[autodoc]] Discussion\n\n[[autodoc]] DiscussionWithDetails\n\n[[autodoc]] DiscussionEvent\n\n[[autodoc]] DiscussionComment\n\n[[autodoc]] DiscussionStatusChange\n\n[[autodoc]] DiscussionCommit\n\n[[autodoc]] DiscussionTitleChange"], "test_settings": "chunk_7000_embeddings_openai-text-embedding-3-small_reader-model_gpt-4", "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response correctly identifies the `HfApi` class as the Python class used to retrieve Discussions and Pull Requests from a given repository on the Hugging Face Hub. The inclusion of \"[\ubb38\uc11c 0]\" is unnecessary and does not contribute to the accuracy or completeness of the answer. However, the core information provided is accurate and aligns with the criteria for a correct response. Therefore, the response is mostly accurate and factual, but the extraneous text slightly detracts from its precision."}, {"question": "What is the name of the new library introduced by Hugging Face for hosting scikit-learn models?\n", "true_answer": "Skops", "source_doc": "huggingface/blog/blob/main/skops.md", "generated_answer": "\ubb38\ub9e5\uc5d0\uc11c scikit-learn \ubaa8\ub378\uc744 \ud638\uc2a4\ud305\ud558\uae30 \uc704\ud574 Hugging Face\uc5d0\uc11c \ub3c4\uc785\ud55c \uc0c8\ub85c\uc6b4 \ub77c\uc774\ube0c\ub7ec\ub9ac\uc5d0 \ub300\ud55c \uc5b8\uae09\uc740 \uc5c6\uc2b5\ub2c8\ub2e4.", "retrieved_docs": ["Libraries\n\nThe Hub has support for dozens of libraries in the Open Source ecosystem. Thanks to the `huggingface_hub` Python library, it's easy to enable sharing your models on the Hub. The Hub supports many libraries, and we're working on expanding this support. We're happy to welcome to the Hub a set of Open Source libraries that are pushing Machine Learning forward.\n\nThe table below summarizes the supported libraries and their level of integration. Find all our supported libraries in [the model-libraries.ts file](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/model-libraries.ts).", "--\ntitle: \"Welcome spaCy to the Hugging Face Hub\"\nthumbnail: /blog/assets/23_spacy/thumbnail.png\n\nauthors:\n- user: osanseviero\n- user: ines\n---\n\n# Welcome spaCy to the Hugging Face Hub\n\n\n[spaCy](https://github.com/explosion/spaCy) is a popular library for advanced Natural Language Processing used widely across industry. spaCy makes it easy to use and train pipelines for tasks like named entity recognition, text classification, part of speech tagging and more, and lets you build powerful applications to process and analyze large volumes of text.\n\nHugging Face makes it really easy to share your spaCy pipelines with the community! With a single command, you can upload any pipeline package, with a pretty model card and all required metadata auto-generated for you. The inference API currently supports NER out-of-the-box, and you can try out your pipeline interactively in your browser. You'll also get a live URL for your package that you can `pip install` from anywhere for a smooth path from prototype all the way to production!\n\n### Finding models\n\nOver 60 canonical models can be found in the [spaCy](https://hf.co/spacy) org. These models are from the [latest 3.1 release](https://explosion.ai/blog/spacy-v3-1), so you can try the latest realesed models right now! On top of this, you can find all spaCy models from the community here https://huggingface.co/models?filter=spacy.\n\n\n### Widgets\n\nThis integration includes support for NER widgets, so all models with a NER component will have this out of the box! Coming soon there will be support for text classification and POS.", "The Hugging Face Hub[[the-hugging-face-hub]]\n\n<CourseFloatingBanner\n    chapter={4}\n    classNames=\"absolute z-10 right-0 top-0\"\n/>\n\nThe [Hugging Face Hub](https://huggingface.co/) \u2013- our main website \u2013- is a central platform that enables anyone to discover, use, and contribute new state-of-the-art models and datasets. It hosts a wide variety of models, with more than 10,000 publicly available. We'll focus on the models in this chapter, and take a look at the datasets in Chapter 5.\n\nThe models in the Hub are not limited to \ud83e\udd17 Transformers or even NLP. There are models from [Flair](https://github.com/flairNLP/flair) and [AllenNLP](https://github.com/allenai/allennlp) for NLP, [Asteroid](https://github.com/asteroid-team/asteroid) and [pyannote](https://github.com/pyannote/pyannote-audio) for speech, and [timm](https://github.com/rwightman/pytorch-image-models) for vision, to name a few. \n\nEach of these models is hosted as a Git repository, which allows versioning and reproducibility. Sharing a model on the Hub means opening it up to the community and making it accessible to anyone looking to easily use it, in turn eliminating their need to train a model on their own and simplifying sharing and usage. \n\nAdditionally, sharing a model on the Hub automatically deploys a hosted Inference API for that model. Anyone in the community is free to test it out directly on the model's page, with custom inputs and appropriate widgets.\n\nThe best part is that sharing and using any public model on the Hub is completely free! [Paid plans](https://huggingface.co/pricing) also exist if you wish to share models privately.\n\nThe video below shows how to navigate the Hub.\n\n<Youtube id=\"XvSGPZFEjDY\"/>\n\nHaving a huggingface.co account is required to follow along this part, as we'll be creating and managing repositories on the Hugging Face Hub: [create an account](https://huggingface.co/join)"], "test_settings": "chunk_7000_embeddings_openai-text-embedding-3-small_reader-model_gpt-4", "eval_score_GPT4": "1", "eval_feedback_GPT4": "The response does not provide the correct name of the new library introduced by Hugging Face for hosting scikit-learn models. Instead, it states that there is no mention of such a library, which is incorrect. The correct answer is \"Skops.\" Therefore, the response is entirely inaccurate and does not meet the criteria for correctness, accuracy, or factuality as outlined in the rubric."}, {"question": "What is the purpose of Textual Inversion?\n", "true_answer": "Textual Inversion is a training method for personalizing models by learning new text embeddings from a few example images.", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/loaders/textual_inversion.md", "generated_answer": "The purpose of Textual Inversion is to personalize text-to-image models like Stable Diffusion by learning new concepts from just a few example images, allowing for more control over the generated images and tailoring the model towards specific concepts [\ubb38\uc11c 0, \ubb38\uc11c 1, \ubb38\uc11c 2].", "retrieved_docs": ["!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Textual Inversion\n\nTextual Inversion is a training method for personalizing models by learning new text embeddings from a few example images. The file produced from training is extremely small (a few KBs) and the new embeddings can be loaded into the text encoder.\n\n[`TextualInversionLoaderMixin`] provides a function for loading Textual Inversion embeddings from Diffusers and Automatic1111 into the text encoder and loading a special token to activate the embeddings.\n\n<Tip>\n\nTo learn more about how to load Textual Inversion embeddings, see the [Textual Inversion](../../using-diffusers/loading_adapters#textual-inversion) loading guide.\n\n</Tip>\n\n## TextualInversionLoaderMixin\n\n[[autodoc]] loaders.textual_inversion.TextualInversionLoaderMixin", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Textual inversion\n\n[[open-in-colab]]\n\nThe [`StableDiffusionPipeline`] supports textual inversion, a technique that enables a model like Stable Diffusion to learn a new concept from just a few sample images. This gives you more control over the generated images and allows you to tailor the model towards specific concepts. You can get started quickly with a collection of community created concepts in the [Stable Diffusion Conceptualizer](https://huggingface.co/spaces/sd-concepts-library/stable-diffusion-conceptualizer).\n\nThis guide will show you how to run inference with textual inversion using a pre-learned concept from the Stable Diffusion Conceptualizer. If you're interested in teaching a model new concepts with textual inversion, take a look at the [Textual Inversion](../training/text_inversion) training guide.\n\nImport the necessary libraries:\n\n```py\nimport torch\nfrom diffusers import StableDiffusionPipeline\nfrom diffusers.utils import make_image_grid\n```\n\n## Stable Diffusion 1 and 2\n\nPick a Stable Diffusion checkpoint and a pre-learned concept from the [Stable Diffusion Conceptualizer](https://huggingface.co/spaces/sd-concepts-library/stable-diffusion-conceptualizer):\n\n```py\npretrained_model_name_or_path = \"runwayml/stable-diffusion-v1-5\"\nrepo_id_embeds = \"sd-concepts-library/cat-toy\"\n```\n\nNow you can load a pipeline, and pass the pre-learned concept to it:\n\n```py\npipeline = StableDiffusionPipeline.from_pretrained(\n    pretrained_model_name_or_path, torch_dtype=torch.float16, use_safetensors=True\n).to(\"cuda\")\n\npipeline.load_textual_inversion(repo_id_embeds)\n```\n\nCreate a prompt with the pre-learned concept by using the special placeholder token `<cat-toy>`, and choose the number of samples and rows of images you'd like to generate:\n\n```py\nprompt = \"a grafitti in a favela wall with a <cat-toy> on it\"\n\nnum_samples_per_row = 2\nnum_rows = 2\n```\n\nThen run the pipeline (feel free to adjust the parameters like `num_inference_steps` and `guidance_scale` to see how they affect image quality), save the generated images and visualize them with the helper function you created at the beginning:\n\n```py\nall_images = []\nfor _ in range(num_rows):\n    images = pipeline(prompt, num_images_per_prompt=num_samples_per_row, num_inference_steps=50, guidance_scale=7.5).images\n    all_images.extend(images)\n\ngrid = make_image_grid(all_images, num_rows, num_samples_per_row)\ngrid\n```\n\n<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/textual_inversion_inference.png\">\n</div>\n\n## Stable Diffusion XL\n\nStable Diffusion XL (SDXL) can also use textual inversion vectors for inference. In contrast to Stable Diffusion 1 and 2, SDXL has two text encoders so you'll need two textual inversion embeddings - one for each text encoder model.\n\nLet's download the SDXL textual inversion embeddings and have a closer look at it's structure:\n\n```py\nfrom huggingface_hub import hf_hub_download\nfrom safetensors.torch import load_file\n\nfile = hf_hub_download(\"dn118/unaestheticXL\", filename=\"unaestheticXLv31.safetensors\")\nstate_dict = load_file(file)\nstate_dict\n```\n\n```\n{'clip_g': tensor([[ 0.0077, -0.0112,  0.0065,  ...,  0.0195,  0.0159,  0.0275],\n         ...,\n         [-0.0170,  0.0213,  0.0143,  ..., -0.0302, -0.0240, -0.0362]],\n 'clip_l': tensor([[ 0.0023,  0.0192,  0.0213,  ..., -0.0385,  0.0048, -0.0011],\n         ...,\n         [ 0.0475, -0.0508, -0.0145,  ...,  0.0070, -0.0089, -0.0163]],\n```\n\nThere are two tensors, `\"clip_g\"` and `\"clip_l\"`.\n`\"clip_g\"` corresponds to the bigger text encoder in SDXL and refers to\n`pipe.text_encoder_2` and `\"clip_l\"` refers to `pipe.text_encoder`.\n\nNow you can load each tensor separately by passing them along with the correct text encoder and tokenizer\nto [`~loaders.TextualInversionLoaderMixin.load_textual_inversion`]:\n\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipe = AutoPipelineForText2Image.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", variant=\"fp16\", torch_dtype=torch.float16)\npipe.to(\"cuda\")\n\npipe.load_textual_inversion(state_dict[\"clip_g\"], token=\"unaestheticXLv31\", text_encoder=pipe.text_encoder_2, tokenizer=pipe.tokenizer_2)\npipe.load_textual_inversion(state_dict[\"clip_l\"], token=\"unaestheticXLv31\", text_encoder=pipe.text_encoder, tokenizer=pipe.tokenizer)\n\n# the embedding should be used as a negative embeddi...", "# Textual Inversion fine-tuning example\n\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples.\nThe `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\n\n## Training with Intel Extension for PyTorch\n\nIntel Extension for PyTorch provides the optimizations for faster training and inference on CPUs. You can leverage the training example \"textual_inversion.py\". Follow the [instructions](https://github.com/huggingface/diffusers/tree/main/examples/textual_inversion) to get the model and [dataset](https://huggingface.co/sd-concepts-library/dicoo2) before running the script.\n\nThe example supports both single node and multi-node distributed training:\n\n### Single node training\n\n```bash\nexport MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\nexport DATA_DIR=\"path-to-dir-containing-dicoo-images\"\n\npython textual_inversion.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\n  --train_data_dir=$DATA_DIR \\\n  --learnable_property=\"object\" \\\n  --placeholder_token=\"<dicoo>\" --initializer_token=\"toy\" \\\n  --seed=7 \\\n  --resolution=512 \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=1 \\\n  --max_train_steps=3000 \\\n  --learning_rate=2.5e-03 --scale_lr \\\n  --output_dir=\"textual_inversion_dicoo\"\n```\n\nNote: Bfloat16 is available on Intel Xeon Scalable Processors Cooper Lake or Sapphire Rapids. You may not get performance speedup without Bfloat16 support.\n\n### Multi-node distributed training\n\nBefore running the scripts, make sure to install the library's training dependencies successfully:\n\n```bash\npython -m pip install oneccl_bind_pt==1.13 -f https://developer.intel.com/ipex-whl-stable-cpu\n```\n\n```bash\nexport MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\nexport DATA_DIR=\"path-to-dir-containing-dicoo-images\"\n\noneccl_bindings_for_pytorch_path=$(python -c \"from oneccl_bindings_for_pytorch import cwd; print(cwd)\")\nsource $oneccl_bindings_for_pytorch_path/env/setvars.sh\n\npython -m intel_extension_for_pytorch.cpu.launch --distributed \\\n  --hostfile hostfile --nnodes 2 --nproc_per_node 2 textual_inversion.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\n  --train_data_dir=$DATA_DIR \\\n  --learnable_property=\"object\" \\\n  --placeholder_token=\"<dicoo>\" --initializer_token=\"toy\" \\\n  --seed=7 \\ \n  --resolution=512 \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=1 \\\n  --max_train_steps=750 \\\n  --learning_rate=2.5e-03 --scale_lr \\\n  --output_dir=\"textual_inversion_dicoo\"\n```\nThe above is a simple distributed training usage on 2 nodes with 2 processes on each node. Add the right hostname or ip address in the \"hostfile\" and make sure these 2 nodes are reachable from each other. For more details, please refer to the [user guide](https://github.com/intel/torch-ccl).\n\n\n### Reference\n\nWe publish a [Medium blog](https://medium.com/intel-analytics-software/personalized-stable-diffusion-with-few-shot-fine-tuning-on-a-single-cpu-f01a3316b13) on how to create your own Stable Diffusion model on CPUs using textual inversion. Try it out now, if you have interests."], "test_settings": "chunk_7000_embeddings_openai-text-embedding-3-small_reader-model_gpt-4", "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response accurately describes the purpose of Textual Inversion, which is to personalize text-to-image models by learning new concepts from a few example images. This aligns well with the concept of Textual Inversion as a method for enhancing control over generated images and tailoring models to specific concepts. The response is mostly correct and factual, but it includes references to documents (\ubb38\uc11c 0, \ubb38\uc11c 1, \ubb38\uc11c 2) that are not necessary or relevant in this context, as they do not contribute to the explanation. Despite this minor issue, the core explanation is accurate and aligns with the purpose of Textual Inversion."}, {"question": "What is the recommended multiple of batch size for fp16 data type on an A100 GPU?\n", "true_answer": "64", "source_doc": "huggingface/transformers/blob/main/docs/source/en/perf_train_gpu_one.md", "generated_answer": "The context does not explicitly mention the recommended multiple of batch size for the fp16 data type on an A100 GPU. Therefore, I cannot provide an answer based on the given information.", "retrieved_docs": ["While the diagram shows that there is a bubble of \"dead\" time that can't be parallelized because the last `forward` stage has to wait for `backward` to complete the pipeline, the purpose of finding the best value for `chunks` is to enable a high concurrent GPU utilization across all participating GPUs which translates to minimizing the size of the bubble.\n\nThis scheduling mechanism is known as `all forward all backward`. Some other alternatives are [one forward one backward](https://www.microsoft.com/en-us/research/publication/pipedream-generalized-pipeline-parallelism-for-dnn-training/) and [interleaved one forward one backward](https://arxiv.org/abs/2104.04473).\n\nWhile both Megatron-LM and DeepSpeed have their own implementation of the PP protocol, Megatron-DeepSpeed uses the DeepSpeed implementation as it's integrated with other aspects of DeepSpeed.\n\nOne other important issue here is the size of the word embedding matrix. While normally a word embedding matrix consumes less memory than the transformer block, in our case with a huge 250k vocabulary, the embedding layer needed 7.2GB in bf16 weights and the transformer block is just 4.9GB. Therefore, we had to instruct Megatron-Deepspeed to consider the embedding layer as a transformer block. So we had a pipeline of 72 layers, 2 of which were dedicated to the embedding (first and last). This allowed to balance out the GPU memory consumption. If we didn't do it, we would have had the first and the last stages consume most of the GPU memory, and 95% of GPUs would be using much less memory and thus the training would be far from being efficient.\n\n\n\n## DP+PP\n\nThe following diagram from the DeepSpeed [pipeline tutorial](https://www.deepspeed.ai/tutorials/pipeline/) demonstrates how one combines DP with PP.\n\n![dp-pp-2d](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/parallelism-zero-dp-pp.png)\n\nHere it's important to see how DP rank 0 doesn't see GPU2 and DP rank 1 doesn't see GPU3. To DP there are just GPUs 0 and 1 where it feeds data as if there were just 2 GPUs. GPU0 \"secretly\" offloads some of its load to GPU2 using PP. And GPU1 does the same by enlisting GPU3 to its aid.\n\nSince each dimension requires at least 2 GPUs, here you'd need at least 4 GPUs.\n\n## DP+PP+TP\n\nTo get an even more efficient training PP is combined with TP and DP which is called 3D parallelism. This can be seen in the following diagram.\n\n![dp-pp-tp-3d](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/parallelism-deepspeed-3d.png)\n\nThis diagram is from a blog post [3D parallelism: Scaling to trillion-parameter models](https://www.microsoft.com/en-us/research/blog/deepspeed-extreme-scale-model-training-for-everyone/), which is a good read as well.\n\nSince each dimension requires at least 2 GPUs, here you'd need at least 8 GPUs for full 3D parallelism.\n\n## ZeRO DP+PP+TP\n\nOne of the main features of DeepSpeed is ZeRO, which is a super-scalable extension of DP. It has already been discussed in [ZeRO Data Parallelism](#zero-data-parallelism). Normally it's a standalone feature that doesn't require PP or TP. But it can be combined with PP and TP.\n\nWhen ZeRO-DP is combined with PP (and optionally TP) it typically enables only ZeRO stage 1, which shards only optimizer states. ZeRO stage 2 additionally shards gradients, and stage 3 also shards the model weights.\n\nWhile it's theoretically possible to use ZeRO stage 2 with Pipeline Parallelism, it will have bad performance impacts. There would need to be an additional reduce-scatter collective for every micro-batch to aggregate the gradients before sharding, which adds a potentially significant communication overhead. By nature of Pipeline Parallelism, small micro-batches are used and instead the focus is on trying to balance arithmetic intensity (micro-batch size) with minimizing the Pipeline bubble (number of micro-batches). Therefore those communication costs are going to hurt.\n\nIn addition, there are already fewer layers than normal due to PP and so the memory savings won't be huge. PP already reduces gradient size by ``1/PP``, and so gradient sharding savings on top of that are less significant than pure DP.\n\nZeRO stage 3 can also be used to train models at this scale, however, it requires more communication than the DeepSpeed 3D parallel implementation. After careful evaluation in our environment which happened a year ago we found Megatron-DeepSpeed 3D parallelism performed best. Since then ZeRO stage 3 performance has dramatically improved and if we were to evaluate it today perhaps we would have chosen stage 3 instead.\n\n\n## BF16Optimizer\n\nTraining huge LLM models in FP16 is a no-no.\n\nWe have proved it to ourselves by spending several months [training a 104B model](https://github.com/bigscience-workshop/bigscience/tree/master/train/tr8-104B-wide) which as you can tell from the [tensorboard](https://huggingface.co/bigscience/tr8-104B-logs/tensorboard) was but a complete failure. We lear...", "### V100 (batch size: 1)\n\n| **Pipeline** | **torch 2.0 - <br>no compile** | **torch nightly - <br>no compile** | **torch 2.0 - <br>compile** | **torch nightly - <br>compile** |\n|:---:|:---:|:---:|:---:|:---:|\n| SD - txt2img | 18.99 | 19.14 | 20.95 | 22.17 |\n| SD - img2img | 18.56 | 19.18 | 20.95 | 22.11 |\n| SD - inpaint | 19.14 | 19.06 | 21.08 | 22.20 |\n| SD - controlnet | 13.48 | 13.93 | 15.18 | 15.88 |\n| IF |  20.01 / <br>9.08 / <br>23.34 | 19.79 / <br>8.98 / <br>24.10 | \u274c | 55.75 / <br>11.57 / <br>57.67 |\n\n### V100 (batch size: 4)\n\n| **Pipeline** | **torch 2.0 - <br>no compile** | **torch nightly - <br>no compile** | **torch 2.0 - <br>compile** | **torch nightly - <br>compile** |\n|:---:|:---:|:---:|:---:|:---:|\n| SD - txt2img | 5.96 | 5.89 | 6.83 | 6.86 |\n| SD - img2img | 5.90 | 5.91 | 6.81 | 6.82 |\n| SD - inpaint | 5.99 | 6.03 | 6.93 | 6.95 |\n| SD - controlnet | 4.26 | 4.29 | 4.92 | 4.93 |\n| IF | 15.41 | 14.76 | \u274c | 22.95 |\n\n### V100 (batch size: 16)\n\n| **Pipeline** | **torch 2.0 - <br>no compile** | **torch nightly - <br>no compile** | **torch 2.0 - <br>compile** | **torch nightly - <br>compile** |\n|:---:|:---:|:---:|:---:|:---:|\n| SD - txt2img | 1.66 | 1.66 | 1.92 | 1.90 |\n| SD - img2img | 1.65 | 1.65 | 1.91 | 1.89 |\n| SD - inpaint | 1.69 | 1.69 | 1.95 | 1.93 |\n| SD - controlnet | 1.19 | 1.19 | OOM after warmup | 1.36 |\n| IF | 5.43 | 5.29 | \u274c | 7.06 |\n\n### T4 (batch size: 1)\n\n| **Pipeline** | **torch 2.0 - <br>no compile** | **torch nightly - <br>no compile** | **torch 2.0 - <br>compile** | **torch nightly - <br>compile** |\n|:---:|:---:|:---:|:---:|:---:|\n| SD - txt2img | 6.9 | 6.95 | 7.3 | 7.56 |\n| SD - img2img | 6.84 | 6.99 | 7.04 | 7.55 |\n| SD - inpaint | 6.91 | 6.7 | 7.01 | 7.37 |\n| SD - controlnet | 4.89 | 4.86 | 5.35 | 5.48 |\n| IF | 17.42 / <br>2.47 / <br>18.52 | 16.96 / <br>2.45 / <br>18.69 | \u274c | 24.63 / <br>2.47 / <br>23.39 |\n| SDXL - txt2img | 1.15 | 1.16 | - | - |\n\n### T4 (batch size: 4)\n\n| **Pipeline** | **torch 2.0 - <br>no compile** | **torch nightly - <br>no compile** | **torch 2.0 - <br>compile** | **torch nightly - <br>compile** |\n|:---:|:---:|:---:|:---:|:---:|\n| SD - txt2img | 1.79 | 1.79 | 2.03 | 1.99 |\n| SD - img2img | 1.77 | 1.77 | 2.05 | 2.04 |\n| SD - inpaint | 1.81 | 1.82 | 2.09 | 2.09 |\n| SD - controlnet | 1.34 | 1.27 | 1.47 | 1.46 |\n| IF | 5.79 |  5.61 | \u274c | 7.39 |\n| SDXL - txt2img | 0.288 | 0.289 | - | - |\n\n### T4 (batch size: 16)\n\n| **Pipeline** | **torch 2.0 - <br>no compile** | **torch nightly - <br>no compile** | **torch 2.0 - <br>compile** | **torch nightly - <br>compile** |\n|:---:|:---:|:---:|:---:|:---:|\n| SD - txt2img | 2.34s | 2.30s | OOM after 2nd iteration | 1.99s |\n| SD - img2img | 2.35s | 2.31s | OOM after warmup | 2.00s |\n| SD - inpaint | 2.30s | 2.26s | OOM after 2nd iteration | 1.95s |\n| SD - controlnet | OOM after 2nd iteration | OOM after 2nd iteration | OOM after warmup | OOM after warmup |\n| IF * | 1.44 | 1.44 | \u274c | 1.94 |\n| SDXL - txt2img | OOM | OOM | - | - |\n\n### RTX 3090 (batch size: 1)\n\n| **Pipeline** | **torch 2.0 - <br>no compile** | **torch nightly - <br>no compile** | **torch 2.0 - <br>compile** | **torch nightly - <br>compile** |\n|:---:|:---:|:---:|:---:|:---:|\n| SD - txt2img | 22.56 | 22.84 | 23.84 | 25.69 |\n| SD - img2img | 22.25 | 22.61 | 24.1 | 25.83 |\n| SD - inpaint | 22.22 | 22.54 | 24.26 | 26.02 |\n| SD - controlnet | 16.03 | 16.33 | 17.38 | 18.56 |\n| IF | 27.08 / <br>9.07 / <br>31.23 | 26.75 / <br>8.92 / <br>31.47 | \u274c | 68.08 / <br>11.16 / <br>65.29 |\n\n### RTX 3090 (batch size: 4)\n\n| **Pipeline** | **torch 2.0 - <br>no compile** | **torch nightly - <br>no compile** | **torch 2.0 - <br>compile** | **torch nightly - <br>compile** |\n|:---:|:---:|:---:|:---:|:---:|\n| SD - txt2img | 6.46 | 6.35 | 7.29 | 7.3 |\n| SD - img2img | 6.33 | 6.27 | 7.31 | 7.26 |\n| SD - inpaint | 6.47 | 6.4 | 7.44 | 7.39 |\n| SD - controlnet | 4.59 | 4.54 | 5.27 | 5.26 |\n| IF | 16.81 | 16.62 | \u274c | 21.57 |\n\n### RTX 3090 (batch size: 16)\n\n| **Pipeline** | **torch 2.0 - <br>no compile** | **torch nightly - <br>no compile** | **torch 2.0 - <br>compile** | **torch nightly - <br>compile** |\n|:---:|:---:|:---:|:---:|:---:|\n| SD - txt2img | 1.7 | 1.69 | 1.93 | 1.91 |\n| SD - img2img | 1.68 | 1.67 | 1.93 | 1.9 |\n| SD - inpaint | 1.72 | 1.71 | 1.97 | 1.94 |\n| SD - controlnet | 1.23 | 1.22 | 1.4 | 1.38 |\n| IF | 5.01 | 5.00 | \u274c | 6.33 |\n\n### RTX 4090 (batch size: 1)\n\n| **Pipeline** | **torch 2.0 - <br>no compile** | **torch nightly - <br>no compile** | **torch 2.0 - <br>compile** | **torch nightly - <br>compile** |\n|:---:|:---:|:---:|:---:|:---:|\n| SD - txt2img | 40.5 | 41.89 | 44.65 | 49.81 |\n| SD - img2img | 40.39 | 41.95 | 44.46 | 49.8 |\n| SD - inpaint | 40.51 | 41.88 | 44.58 | 49.72 |\n| SD - controlnet | 29.27 | 30.29 | 32.26 | 36.03 |\n| IF | 69.71 / <br>18.78 / <br>85.49 | 69.13 / <br>18.80 / <br>85.56 | \u274c | 124.60 / <br>26.37 / <br>138.79 |\n| SDXL - txt2img | 6.8 | 8.18 | - | - |\n\n### RTX 4090 (batch size: 4)\n\n| **Pipeline** | **torch 2.0 - <br>no compile** | **torch nightly - <...", "You can enable gradient accumulation by adding the `gradient_accumulation_steps` argument to  [`TrainingArguments`]: \n\n```py\ntraining_args = TrainingArguments(per_device_train_batch_size=1, gradient_accumulation_steps=4, **default_args)\n```\n\nIn the above example, your effective batch size becomes 4. \n\nAlternatively, use \ud83e\udd17 Accelerate to gain full control over the training loop. Find the \ud83e\udd17 Accelerate example \n[further down in this guide](#using-accelerate).\n\nWhile it is advised to max out GPU usage as much as possible, a high number of gradient accumulation steps can \nresult in a more pronounced training slowdown. Consider the following example. Let's say, the `per_device_train_batch_size=4` \nwithout gradient accumulation hits the GPU's limit. If you would like to train with batches of size 64, do not set the \n`per_device_train_batch_size` to 1 and `gradient_accumulation_steps` to 64. Instead, keep `per_device_train_batch_size=4` \nand set `gradient_accumulation_steps=16`. This results in the same effective batch size while making better use of \nthe available GPU resources.\n\nFor additional information, please refer to batch size and gradient accumulation benchmarks for [RTX-3090](https://github.com/huggingface/transformers/issues/14608#issuecomment-1004392537)\nand [A100](https://github.com/huggingface/transformers/issues/15026#issuecomment-1005033957).\n\n## Gradient Checkpointing\n\nSome large models may still face memory issues even when the batch size is set to 1 and gradient accumulation is used. \nThis is because there are other components that also require memory storage.\n\nSaving all activations from the forward pass in order to compute the gradients during the backward pass can result in \nsignificant memory overhead. The alternative approach of discarding the activations and recalculating them when needed \nduring the backward pass, would introduce a considerable computational overhead and slow down the training process.\n\n**Gradient checkpointing** offers a compromise between these two approaches and saves strategically selected activations \nthroughout the computational graph so only a fraction of the activations need to be re-computed for the gradients. For \nan in-depth explanation of gradient checkpointing, refer to [this great article](https://medium.com/tensorflow/fitting-larger-networks-into-memory-583e3c758ff9).\n\nTo enable gradient checkpointing in the [`Trainer`], pass the corresponding a flag to [`TrainingArguments`]:\n\n```py\ntraining_args = TrainingArguments(\n    per_device_train_batch_size=1, gradient_accumulation_steps=4, gradient_checkpointing=True, **default_args\n)\n```\n\nAlternatively, use \ud83e\udd17 Accelerate - find the \ud83e\udd17 Accelerate example [further in this guide](#using-accelerate). \n\n<Tip>\n\nWhile gradient checkpointing may improve memory efficiency, it slows training by approximately 20%.\n\n</Tip>\n\n## Mixed precision training\n\n**Mixed precision training** is a technique that aims to optimize the computational efficiency of training models by \nutilizing lower-precision numerical formats for certain variables. Traditionally, most models use 32-bit floating point \nprecision (fp32 or float32) to represent and process variables. However, not all variables require this high precision \nlevel to achieve accurate results. By reducing the precision of certain variables to lower numerical formats like 16-bit \nfloating point (fp16 or float16), we can speed up the computations. Because in this approach some computations are performed \nin half-precision, while some are still in full precision, the approach is called mixed precision training.\n\nMost commonly mixed precision training is achieved by using fp16 (float16) data types, however, some GPU architectures \n(such as the Ampere architecture) offer bf16 and tf32 (CUDA internal data type) data types. Check \nout the [NVIDIA Blog](https://developer.nvidia.com/blog/accelerating-ai-training-with-tf32-tensor-cores/) to learn more about \nthe differences between these data types.\n\n### fp16\n\nThe main advantage of mixed precision training comes from saving the activations in half precision (fp16). \nAlthough the gradients are also computed in half precision they are converted back to full precision for the optimization \nstep so no memory is saved here. \nWhile mixed precision training results in faster computations, it can also lead to more GPU memory being utilized, especially for small batch sizes.\nThis is because the model is now present on the GPU in both 16-bit and 32-bit precision (1.5x the original model on the GPU).\n\nTo enable mixed precision training, set the `fp16` flag to `True`:\n\n```py\ntraining_args = TrainingArguments(per_device_train_batch_size=4, fp16=True, **default_args)\n```\n\nIf you prefer to use \ud83e\udd17 Accelerate, find the \ud83e\udd17 Accelerate example [further in this guide](#using-accelerate). \n\n### BF16\n\nIf you have access to an Ampere or newer hardware you can use bf16 for mixed precision training and evaluation. While \nbf16 has a worse precision than fp16, it has a much..."], "test_settings": "chunk_7000_embeddings_openai-text-embedding-3-small_reader-model_gpt-4", "eval_score_GPT4": "1", "eval_feedback_GPT4": "The response does not provide the correct answer to the question, which is \"64\" as per the given criteria. Instead, it states that the context does not provide the information, which is not relevant to the task of answering the question directly. The response fails to meet the accuracy and factuality required by the rubric, as it does not address the specific query about the recommended multiple of batch size for fp16 data type on an A100 GPU. Therefore, the response is considered incorrect and does not align with the expected answer."}, {"question": "How do you run a Gradio Blocks app in reload mode using a Python IDE?\n", "true_answer": "Run `gradio run.py` in the terminal.", "source_doc": "gradio-app/gradio/blob/main/guides/09_other-tutorials/developing-faster-with-reload-mode.md", "generated_answer": "To run a Gradio Blocks app in reload mode using a Python IDE, you should replace the `python` command with `gradio` when running your code. For example, if your file is named `run.py`, you would use the command `gradio run.py` in the terminal. This will automatically reload the app whenever changes are detected in the code file. Make sure the Gradio Blocks/Interface demo is named `demo` or provide the name as the second parameter if it is named differently [\ubb38\uc11c 0].", "retrieved_docs": ["Developing Faster with Auto-Reloading\n\n**Prerequisite**: This Guide requires you to know about Blocks. Make sure to [read the Guide to Blocks first](https://gradio.app/guides/quickstart/#blocks-more-flexibility-and-control).\n\nThis guide covers auto reloading, reloading in a Python IDE, and using gradio with Jupyter Notebooks.\n\n## Why Auto-Reloading?\n\nWhen you are building a Gradio demo, particularly out of Blocks, you may find it cumbersome to keep re-running your code to test your changes.\n\nTo make it faster and more convenient to write your code, we've made it easier to \"reload\" your Gradio apps instantly when you are developing in a **Python IDE** (like VS Code, Sublime Text, PyCharm, or so on) or generally running your Python code from the terminal. We've also developed an analogous \"magic command\" that allows you to re-run cells faster if you use **Jupyter Notebooks** (or any similar environment like Colab).\n\nThis short Guide will cover both of these methods, so no matter how you write Python, you'll leave knowing how to build Gradio apps faster.\n\n## Python IDE Reload \ud83d\udd25\n\nIf you are building Gradio Blocks using a Python IDE, your file of code (let's name it `run.py`) might look something like this:\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\"# Greetings from Gradio!\")\n    inp = gr.Textbox(placeholder=\"What is your name?\")\n    out = gr.Textbox()\n\n    inp.change(fn=lambda x: f\"Welcome, {x}!\",\n               inputs=inp,\n               outputs=out)\n\nif __name__ == \"__main__\":\n    demo.launch()\n```\n\nThe problem is that anytime that you want to make a change to your layout, events, or components, you have to close and rerun your app by writing `python run.py`.\n\nInstead of doing this, you can run your code in **reload mode** by changing 1 word: `python` to `gradio`:\n\nIn the terminal, run `gradio run.py`. That's it!\n\nNow, you'll see that after you'll see something like this:\n\n```bash\nWatching: '/Users/freddy/sources/gradio/gradio', '/Users/freddy/sources/gradio/demo/'\n\nRunning on local URL:  http://127.0.0.1:7860\n```\n\nThe important part here is the line that says `Watching...` What's happening here is that Gradio will be observing the directory where `run.py` file lives, and if the file changes, it will automatically rerun the file for you. So you can focus on writing your code, and your Gradio demo will refresh automatically \ud83e\udd73\n\n\u26a0\ufe0f Warning: the `gradio` command does not detect the parameters passed to the `launch()` methods because the `launch()` method is never called in reload mode. For example, setting `auth`, or `show_error` in `launch()` will not be reflected in the app.\n\nThere is one important thing to keep in mind when using the reload mode: Gradio specifically looks for a Gradio Blocks/Interface demo called `demo` in your code. If you have named your demo something else, you will need to pass in the name of your demo as the 2nd parameter in your code. So if your `run.py` file looked like this:\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as my_demo:\n    gr.Markdown(\"# Greetings from Gradio!\")\n    inp = gr.Textbox(placeholder=\"What is your name?\")\n    out = gr.Textbox()\n\n    inp.change(fn=lambda x: f\"Welcome, {x}!\",\n               inputs=inp,\n               outputs=out)\n\nif __name__ == \"__main__\":\n    my_demo.launch()\n```\n\nThen you would launch it in reload mode like this: `gradio run.py my_demo`.\n\nBy default, the Gradio use UTF-8 encoding for scripts. **For reload mode**, If you are using encoding formats other than UTF-8 (such as cp1252), make sure you've done like this:\n\n1. Configure encoding declaration of python script, for example: `# -*- coding: cp1252 -*-`\n2. Confirm that your code editor has identified that encoding format. \n3. Run like this: `gradio run.py --encoding cp1252`\n\n\ud83d\udd25 If your application accepts command line arguments, you can pass them in as well. Here's an example:\n\n```python\nimport gradio as gr\nimport argparse\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--name\", type=str, default=\"User\")\nargs, unknown = parser.parse_known_args()\n\nwith gr.Blocks() as demo:\n    gr.Markdown(f\"# Greetings {args.name}!\")\n    inp = gr.Textbox()\n    out = gr.Textbox()\n\n    inp.change(fn=lambda x: x, inputs=inp, outputs=out)\n\nif __name__ == \"__main__\":\n    demo.launch()\n```\n\nWhich you could run like this: `gradio run.py --name Gretel`\n\nAs a small aside, this auto-reloading happens if you change your `run.py` source code or the Gradio source code. Meaning that this can be useful if you decide to [contribute to Gradio itself](https://github.com/gradio-app/gradio/blob/main/CONTRIBUTING.md) \u2705\n\n## Jupyter Notebook Magic \ud83d\udd2e\n\nWhat about if you use Jupyter Notebooks (or Colab Notebooks, etc.) to develop code? We got something for you too!\n\nWe've developed a **magic command** that will create and run a Blocks demo for you. To use this, load the gradio extension at the top of your notebook:\n\n`%load_ext gradio`\n\nThen, in the cell that you are developing your Gradio demo,...", "\u901a\u8fc7\u81ea\u52a8\u91cd\u8f7d\u5b9e\u73b0\u66f4\u5feb\u7684\u5f00\u53d1\n\n**\u5148\u51b3\u6761\u4ef6**\uff1a\u672c\u6307\u5357\u8981\u6c42\u60a8\u4e86\u89e3\u5757\u7684\u77e5\u8bc6\u3002\u8bf7\u786e\u4fdd[\u5148\u9605\u8bfb\u5757\u6307\u5357](https://gradio.app/quickstart/#blocks-more-flexibility-and-control)\u3002\n\n\u672c\u6307\u5357\u4ecb\u7ecd\u4e86\u81ea\u52a8\u91cd\u65b0\u52a0\u8f7d\u3001\u5728 Python IDE \u4e2d\u91cd\u65b0\u52a0\u8f7d\u4ee5\u53ca\u5728 Jupyter Notebooks \u4e2d\u4f7f\u7528 gradio \u7684\u65b9\u6cd5\u3002\n\n## \u4e3a\u4ec0\u4e48\u8981\u4f7f\u7528\u81ea\u52a8\u91cd\u8f7d\uff1f\n\n\u5f53\u60a8\u6784\u5efa Gradio \u6f14\u793a\u65f6\uff0c\u7279\u522b\u662f\u4f7f\u7528 Blocks \u6784\u5efa\u65f6\uff0c\u60a8\u53ef\u80fd\u4f1a\u53d1\u73b0\u53cd\u590d\u8fd0\u884c\u4ee3\u7801\u4ee5\u6d4b\u8bd5\u66f4\u6539\u5f88\u9ebb\u70e6\u3002\n\n\u4e3a\u4e86\u66f4\u5feb\u901f\u3001\u66f4\u4fbf\u6377\u5730\u7f16\u5199\u4ee3\u7801\uff0c\u6211\u4eec\u5df2\u7ecf\u7b80\u5316\u4e86\u5728 **Python IDE**\uff08\u5982 VS Code\u3001Sublime Text\u3001PyCharm \u7b49\uff09\u4e2d\u5f00\u53d1\u6216\u4ece\u7ec8\u7aef\u8fd0\u884c Python \u4ee3\u7801\u65f6\u201c\u91cd\u65b0\u52a0\u8f7d\u201dGradio \u5e94\u7528\u7684\u65b9\u5f0f\u3002\u6211\u4eec\u8fd8\u5f00\u53d1\u4e86\u4e00\u4e2a\u7c7b\u4f3c\u7684\u201c\u9b54\u6cd5\u547d\u4ee4\u201d\uff0c\u4f7f\u60a8\u53ef\u4ee5\u66f4\u5feb\u901f\u5730\u91cd\u65b0\u8fd0\u884c\u5355\u5143\u683c\uff0c\u5982\u679c\u60a8\u4f7f\u7528 Jupyter Notebooks\uff08\u6216\u7c7b\u4f3c\u7684\u73af\u5883\uff0c\u5982 Colab\uff09\u7684\u8bdd\u3002\n\n\u8fd9\u4e2a\u7b80\u77ed\u7684\u6307\u5357\u5c06\u6db5\u76d6\u8fd9\u4e24\u79cd\u65b9\u6cd5\uff0c\u6240\u4ee5\u65e0\u8bba\u60a8\u5982\u4f55\u7f16\u5199 Python \u4ee3\u7801\uff0c\u60a8\u90fd\u5c06\u77e5\u9053\u5982\u4f55\u66f4\u5feb\u5730\u6784\u5efa Gradio \u5e94\u7528\u7a0b\u5e8f\u3002\n\n## Python IDE \u91cd\u8f7d \ud83d\udd25\n\n\u5982\u679c\u60a8\u4f7f\u7528 Python IDE \u6784\u5efa Gradio Blocks\uff0c\u90a3\u4e48\u4ee3\u7801\u6587\u4ef6\uff08\u5047\u8bbe\u547d\u540d\u4e3a `run.py`\uff09\u53ef\u80fd\u5982\u4e0b\u6240\u793a\uff1a\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\"# \u6765\u81eaGradio\u7684\u95ee\u5019\uff01\")\n    inp = gr.Textbox(placeholder=\"\u60a8\u53eb\u4ec0\u4e48\u540d\u5b57\uff1f\")\n    out = gr.Textbox()\n\n    inp.change(fn=lambda x: f\"\u6b22\u8fce\uff0c{x}\uff01\",\n               inputs=inp,\n               outputs=out)\n\nif __name__ == \"__main__\":\n    demo.launch()\n```\n\n\u95ee\u9898\u5728\u4e8e\uff0c\u6bcf\u5f53\u60a8\u60f3\u8981\u66f4\u6539\u5e03\u5c40\u3001\u4e8b\u4ef6\u6216\u7ec4\u4ef6\u65f6\uff0c\u90fd\u5fc5\u987b\u901a\u8fc7\u7f16\u5199 `python run.py` \u6765\u5173\u95ed\u548c\u91cd\u65b0\u8fd0\u884c\u5e94\u7528\u7a0b\u5e8f\u3002\n\n\u800c\u4e0d\u662f\u8fd9\u6837\u505a\uff0c\u60a8\u53ef\u4ee5\u901a\u8fc7\u66f4\u6539 1 \u4e2a\u5355\u8bcd\u6765\u4ee5**\u91cd\u65b0\u52a0\u8f7d\u6a21\u5f0f**\u8fd0\u884c\u4ee3\u7801\uff1a\u5c06 `python` \u66f4\u6539\u4e3a `gradio`\uff1a\n\n\u5728\u7ec8\u7aef\u4e2d\u8fd0\u884c `gradio run.py`\u3002\u5c31\u662f\u8fd9\u6837\uff01\n\n\u73b0\u5728\uff0c\u60a8\u5c06\u770b\u5230\u7c7b\u4f3c\u4e8e\u8fd9\u6837\u7684\u5185\u5bb9\uff1a\n\n```bash\nLaunching in *reload mode* on: http://127.0.0.1:7860 (Press CTRL+C to quit)\n\nWatching...\n\nWARNING:  The --reload flag should not be used in production on Windows.\n```\n\n\u8fd9\u91cc\u6700\u91cd\u8981\u7684\u4e00\u884c\u662f `\u6b63\u5728\u89c2\u5bdf ...`\u3002\u8fd9\u91cc\u53d1\u751f\u7684\u60c5\u51b5\u662f Gradio \u5c06\u89c2\u5bdf `run.py` \u6587\u4ef6\u6240\u5728\u7684\u76ee\u5f55\uff0c\u5982\u679c\u6587\u4ef6\u53d1\u751f\u66f4\u6539\uff0c\u5b83\u5c06\u81ea\u52a8\u4e3a\u60a8\u91cd\u65b0\u8fd0\u884c\u6587\u4ef6\u3002\u56e0\u6b64\uff0c\u60a8\u53ea\u9700\u4e13\u6ce8\u4e8e\u7f16\u5199\u4ee3\u7801\uff0cGradio \u6f14\u793a\u5c06\u81ea\u52a8\u5237\u65b0 \ud83e\udd73\n\n\u26a0\ufe0f \u8b66\u544a\uff1a`gradio` \u547d\u4ee4\u4e0d\u4f1a\u68c0\u6d4b\u4f20\u9012\u7ed9 `launch()` \u65b9\u6cd5\u7684\u53c2\u6570\uff0c\u56e0\u4e3a\u5728\u91cd\u65b0\u52a0\u8f7d\u6a21\u5f0f\u4e0b\u4ece\u672a\u8c03\u7528 `launch()` \u65b9\u6cd5\u3002\u4f8b\u5982\uff0c\u8bbe\u7f6e `launch()` \u4e2d\u7684 `auth` \u6216 `show_error` \u4e0d\u4f1a\u5728\u5e94\u7528\u7a0b\u5e8f\u4e2d\u53cd\u6620\u51fa\u6765\u3002\n\n\u5f53\u60a8\u4f7f\u7528\u91cd\u65b0\u52a0\u8f7d\u6a21\u5f0f\u65f6\uff0c\u8bf7\u8bb0\u4f4f\u4e00\u4ef6\u91cd\u8981\u7684\u4e8b\u60c5\uff1aGradio \u4e13\u95e8\u67e5\u627e\u540d\u4e3a `demo` \u7684 Gradio Blocks/Interface \u6f14\u793a\u3002\u5982\u679c\u60a8\u5c06\u6f14\u793a\u547d\u540d\u4e3a\u5176\u4ed6\u540d\u79f0\uff0c\u60a8\u9700\u8981\u5728\u4ee3\u7801\u4e2d\u7684\u7b2c\u4e8c\u4e2a\u53c2\u6570\u4e2d\u4f20\u5165\u6f14\u793a\u7684 FastAPI \u5e94\u7528\u7a0b\u5e8f\u7684\u540d\u79f0\u3002\u5bf9\u4e8e Gradio \u6f14\u793a\uff0c\u53ef\u4ee5\u4f7f\u7528 `.app` \u5c5e\u6027\u8bbf\u95ee FastAPI \u5e94\u7528\u7a0b\u5e8f\u3002\u56e0\u6b64\uff0c\u5982\u679c\u60a8\u7684 `run.py` \u6587\u4ef6\u5982\u4e0b\u6240\u793a\uff1a\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as my_demo:\n    gr.Markdown(\"# \u6765\u81eaGradio\u7684\u95ee\u5019\uff01\")\n    inp = gr.Textbox(placeholder=\"\u60a8\u53eb\u4ec0\u4e48\u540d\u5b57\uff1f\")\n    out = gr.Textbox()\n\n    inp.change(fn=lambda x: f\"\u6b22\u8fce\uff0c{x}\uff01\",\n               inputs=inp,\n               outputs=out)\n\nif __name__ == \"__main__\":\n    my_demo.launch()\n```\n\n\u90a3\u4e48\u60a8\u53ef\u4ee5\u8fd9\u6837\u542f\u52a8\u5b83\uff1a`gradio run.py my_demo.app`\u3002\n\nGradio\u9ed8\u8ba4\u4f7f\u7528UTF-8\u7f16\u7801\u683c\u5f0f\u3002\u5bf9\u4e8e**\u91cd\u65b0\u52a0\u8f7d\u6a21\u5f0f**\uff0c\u5982\u679c\u4f60\u7684\u811a\u672c\u4f7f\u7528\u7684\u662f\u9664UTF-8\u4ee5\u5916\u7684\u7f16\u7801\uff08\u5982GBK\uff09\uff1a\n\n1. \u5728Python\u811a\u672c\u7684\u7f16\u7801\u58f0\u660e\u5904\u6307\u5b9a\u4f60\u60f3\u8981\u7684\u7f16\u7801\u683c\u5f0f\uff0c\u5982\uff1a`# -*- coding: gbk -*-`\n2. \u786e\u4fdd\u4f60\u7684\u4ee3\u7801\u7f16\u8f91\u5668\u8bc6\u522b\u5230\u8be5\u683c\u5f0f\u3002 \n3. \u6267\u884c\uff1a`gradio run.py --encoding gbk`\n\n\ud83d\udd25 \u5982\u679c\u60a8\u7684\u5e94\u7528\u7a0b\u5e8f\u63a5\u53d7\u547d\u4ee4\u884c\u53c2\u6570\uff0c\u60a8\u4e5f\u53ef\u4ee5\u4f20\u9012\u5b83\u4eec\u3002\u4e0b\u9762\u662f\u4e00\u4e2a\u4f8b\u5b50\uff1a\n\n```python\nimport gradio as gr\nimport argparse\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--name\", type=str, default=\"User\")\nargs, unknown = parser.parse_known_args()\n\nwith gr.Blocks() as demo:\n    gr.Markdown(f\"# \u6b22\u8fce {args.name}\uff01\")\n    inp = gr.Textbox()\n    out = gr.Textbox()\n\n    inp.change(fn=lambda x: x, inputs=inp, outputs=out)\n\nif __name__ == \"__main__\":\n    demo.launch()\n```\n\n\u60a8\u53ef\u4ee5\u50cf\u8fd9\u6837\u8fd0\u884c\u5b83\uff1a`gradio run.py --name Gretel`\n\n\u4f5c\u4e3a\u4e00\u4e2a\u5c0f\u63d0\u793a\uff0c\u53ea\u8981\u66f4\u6539\u4e86 `run.py` \u6e90\u4ee3\u7801\u6216 Gradio \u6e90\u4ee3\u7801\uff0c\u81ea\u52a8\u91cd\u65b0\u52a0\u8f7d\u5c31\u4f1a\u53d1\u751f\u3002\u8fd9\u610f\u5473\u7740\u5982\u679c\u60a8\u51b3\u5b9a[\u4e3a Gradio \u505a\u8d21\u732e](https://github.com/gradio-app/gradio/blob/main/CONTRIBUTING.md)\uff0c\u8fd9\u5c06\u975e\u5e38\u6709\u7528 \u2705\n\n## Jupyter Notebook \u9b54\u6cd5\u547d\u4ee4\ud83d\udd2e\n\n\u5982\u679c\u60a8\u4f7f\u7528 Jupyter Notebooks\uff08\u6216 Colab Notebooks \u7b49\uff09\u8fdb\u884c\u5f00\u53d1\uff0c\u6211\u4eec\u4e5f\u4e3a\u60a8\u63d0\u4f9b\u4e86\u4e00\u4e2a\u89e3\u51b3\u65b9\u6848\uff01\n\n\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a **magic command \u9b54\u6cd5\u547d\u4ee4**\uff0c\u53ef\u4ee5\u4e3a\u60a8\u521b\u5efa\u548c\u8fd0\u884c\u4e00\u4e2a Blocks \u6f14\u793a\u3002\u8981\u4f7f\u7528\u6b64\u529f\u80fd\uff0c\u5728\u7b14\u8bb0\u672c\u9876\u90e8\u52a0\u8f7d gradio \u6269\u5c55\uff1a\n\n`%load_ext gradio`\n\n\u7136\u540e\uff0c\u5728\u60a8\u6b63\u5728\u5f00\u53d1 Gradio \u6f14\u793a\u7684\u5355\u5143\u683c\u4e2d\uff0c\u53ea\u9700\u5728\u9876\u90e8\u5199\u5165\u9b54\u6cd5\u547d\u4ee4**`%%blocks`**\uff0c\u7136\u540e\u50cf\u5e73\u5e38\u4e00\u6837\u7f16\u5199\u5e03\u5c40\u548c\u7ec4\u4ef6\uff1a\n\n```py\n%%blocks\n\nimport gradio as gr\n\ngr.Markdown(\"# \u6765\u81eaGradio\u7684\u95ee\u5019\uff01\")\ninp = gr.Textbox(placeholder=\"\u60a8\u53eb\u4ec0\u4e48\u540d\u5b57\uff1f\")\nout = gr.Textbox()\n\ninp.change(fn=lambda x: f\"\u6b22\u8fce\uff0c{x}\uff01\",\n           inputs=inp,\n           outputs=out)\n```\n\n\u8bf7\u6ce8\u610f\uff1a\n\n- \u60a8\u4e0d\u9700\u8981\u653e\u7f6e\u6837\u677f\u4ee3\u7801 `with gr.Blocks() as demo:` \u548c `demo.launch()` \u2014 Gradio \u4f1a\u81ea\u52a8\u4e3a\u60a8\u5b8c\u6210\uff01\n\n- \u6bcf\u6b21\u91cd\u65b0\u8fd0\u884c\u5355\u5143\u683c\u65f6\uff0cGradio \u90fd\u5c06\u5728\u76f8\u540c\u7684\u7aef\u53e3\u4e0a\u91cd\u65b0\u542f\u52a8\u60a8\u7684\u5e94\u7528\u7a0b\u5e8f\uff0c\u5e76\u4f7f\u7528\u76f8\u540c\u7684\u5e95\u5c42\u7f51\u7edc\u670d\u52a1\u5668\u3002\u8fd9\u610f\u5473\u7740\u60a8\u5c06\u6bd4\u6b63\u5e38\u91cd\u65b0\u8fd0\u884c\u5355\u5143\u683c\u66f4\u5feb\u5730\u770b\u5230\u53d8\u5316\u3002\n\n\u4e0b\u9762\u662f\u5728 Jupyter Notebook \u4e2d\u7684\u793a\u4f8b\uff1a\n\n![](https://i.ibb.co/nrszFws/Blocks.gif)\n\n\ud83e\ude84\u8fd9\u5728 colab \u7b14\u8bb0\u672c\u4e2d\u4e5f\u9002\u7528\uff01[\u8fd9\u662f\u4e00\u4e2a colab \u7b14\u8bb0\u672c](https://colab.research.google.com/drive/1jUlX1w7JqckRHVE-nbDyMPyZ7fYD8488?authuser=1#scrollTo=zxHYjbCTTz_5)\uff0c\u60a8\u53ef\u4ee5\u5728\u5176\u4e2d\u770b\u5230 Blocks \u9b54\u6cd5\u6548\u679c\u3002\u5c1d\u8bd5\u8fdb\u884c\u4e00\u4e9b\u66f4\u6539\u5e76\u91cd\u65b0\u8fd0\u884c\u5e26\u6709 Gradio \u4ee3\u7801\u7684\u5355\u5143\u683c\uff01\n\nNotebook Magic \u73b0\u5728\u662f\u4f5c\u8005\u6784\u5efa Gradio \u6f14\u793a\u7684\u9996\u9009\u65b9\u5f0f\u3002\u65e0\u8bba\u60a8\u5982\u4f55\u7f16\u5199 Python \u4ee3\u7801\uff0c\u6211\u4eec\u90fd\u5e0c\u671b\u8fd9\u4e24\u79cd\u65b9\u6cd5\u90fd\u80fd\u4e3a\u60a8\u63d0\u4f9b\u66f4\u597d\u7684 Gradio \u5f00\u53d1\u4f53\u9a8c\u3002\n\n---\n\n## \u4e0b\u4e00\u6b65\n\n\u65e2\u7136\u60a8\u5df2\u7ecf\u4e86\u89e3\u4e86\u5982\u4f55\u4f7f\u7528 Gradio \u5feb\u901f\u5f00\u53d1\uff0c\u8bf7\u5f00\u59cb\u6784\u5efa\u81ea\u5df1\u7684\u5e94\u7528\u7a0b\u5e8f\u5427\uff01\n\n\u5982\u679c\u4f60\u6b63\u5728\u5bfb\u627e\u7075\u611f\uff0c\u8bf7\u5c1d\u8bd5\u6d4f\u89c8\u5176\u4ed6\u4eba\u7528 Gradio \u6784\u5efa\u7684\u6f14\u793a\uff0c[\u6d4f\u89c8 Hugging Face Spaces](http://hf.space/) \ud83e\udd17", "Gradio Demo: blocks_page_load\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\n\ndef print_message(n):\n    return \"Welcome! This page has loaded for \" + n\n\n\nwith gr.Blocks() as demo:\n    t = gr.Textbox(\"Frank\", label=\"Name\")\n    t2 = gr.Textbox(label=\"Output\")\n    demo.load(print_message, t, t2)\n\n\nif __name__ == \"__main__\":\n    demo.launch()\n\n```"], "test_settings": "chunk_7000_embeddings_openai-text-embedding-3-small_reader-model_gpt-4", "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response provides a mostly correct explanation of how to run a Gradio Blocks app in reload mode using a Python IDE. It correctly mentions using the `gradio run.py` command in the terminal, which aligns with the criterion answer. However, the response includes additional information about naming conventions and parameters that are not necessary for the task at hand. This extra detail does not detract from the accuracy but is not required according to the criterion answer. Overall, the response is mostly accurate and factual, but the inclusion of unnecessary details prevents it from being fully aligned with the criterion answer."}, {"question": "How can you install the Hugging Face Unity API in your Unity project?\n", "true_answer": "To install the Hugging Face Unity API in your Unity project, go to `Window` -> `Package Manager`, click `+` and select `Add Package from git URL`, then enter `https://github.com/huggingface/unity-api.git`.", "source_doc": "huggingface/blog/blob/main/unity-api.md", "generated_answer": "To install the Hugging Face Unity API in your Unity project, follow these steps:\n\n1. Open your Unity project.\n2. Go to `Window` -> `Package Manager`.\n3. Click `+` and select `Add Package from git URL`.\n4. Enter `https://github.com/huggingface/unity-api.git`.\n5. Once installed, the Unity API wizard should pop up. If not, go to `Window` -> `Hugging Face API Wizard`.\n6. Enter your API key, which can be created in your Hugging Face account settings.\n7. Test the API key by clicking `Test API key` in the API Wizard.\n8. Optionally, change the model endpoints if desired.\n9. Configure advanced settings if needed.\n10. Click `Install Examples` to see usage examples, and then close the API Wizard. \n\nThese steps will set up the Hugging Face Unity API in your project [Document 0].", "retrieved_docs": ["--\ntitle: \"How to Install and Use the Hugging Face Unity API\"\nthumbnail: /blog/assets/124_ml-for-games/unity-api-thumbnail.png\nauthors:\n- user: dylanebert\n---\n\n# How to Install and Use the Hugging Face Unity API\n\n<!-- {authors} --> \n\nThe [Hugging Face Unity API](https://github.com/huggingface/unity-api) is an easy-to-use integration of the [Hugging Face Inference API](https://huggingface.co/inference-api), allowing developers to access and use Hugging Face AI models in their Unity projects. In this blog post, we'll walk through the steps to install and use the Hugging Face Unity API.\n\n## Installation\n\n1. Open your Unity project\n2. Go to `Window` -> `Package Manager`\n3. Click `+` and select `Add Package from git URL`\n4. Enter `https://github.com/huggingface/unity-api.git`\n5. Once installed, the Unity API wizard should pop up. If not, go to `Window` -> `Hugging Face API Wizard`\n\n<figure class=\"image text-center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/packagemanager.gif\">\n</figure> \n\n6. Enter your API key. Your API key can be created in your [Hugging Face account settings](https://huggingface.co/settings/tokens).\n7. Test the API key by clicking `Test API key` in the API Wizard.\n8. Optionally, change the model endpoints to change which model to use. The model endpoint for any model that supports the inference API can be found by going to the model on the Hugging Face website, clicking `Deploy` -> `Inference API`, and copying the url from the `API_URL` field.\n9. Configure advanced settings if desired. For up-to-date information, visit the project repository at `https://github.com/huggingface/unity-api`\n10. To see examples of how to use the API, click `Install Examples`. You can now close the API Wizard.\n\n<figure class=\"image text-center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/apiwizard.png\">\n</figure> \n\nNow that the API is set up, you can make calls from your scripts to the API. Let's look at an example of performing a Sentence Similarity task:\n\n```\nusing HuggingFace.API;\n\n/* other code */\n\n// Make a call to the API\nvoid Query() {\n    string inputText = \"I'm on my way to the forest.\";\n    string[] candidates = {\n        \"The player is going to the city\",\n        \"The player is going to the wilderness\",\n        \"The player is wandering aimlessly\"\n    };\n    HuggingFaceAPI.SentenceSimilarity(inputText, OnSuccess, OnError, candidates);\n}\n\n// If successful, handle the result\nvoid OnSuccess(float[] result) {\n    foreach(float value in result) {\n        Debug.Log(value);\n    }\n}\n\n// Otherwise, handle the error\nvoid OnError(string error) {\n    Debug.LogError(error);\n}\n\n/* other code */\n```\n\n## Supported Tasks and Custom Models\n\nThe Hugging Face Unity API also currently supports the following tasks:\n\n- [Conversation](https://huggingface.co/tasks/conversational)\n- [Text Generation](https://huggingface.co/tasks/text-generation)\n- [Text to Image](https://huggingface.co/tasks/text-to-image)\n- [Text Classification](https://huggingface.co/tasks/text-classification)\n- [Question Answering](https://huggingface.co/tasks/question-answering)\n- [Translation](https://huggingface.co/tasks/translation)\n- [Summarization](https://huggingface.co/tasks/summarization)\n- [Speech Recognition](https://huggingface.co/tasks/automatic-speech-recognition)\n\nUse the corresponding methods provided by the `HuggingFaceAPI` class to perform these tasks.\n\nTo use your own custom model hosted on Hugging Face, change the model endpoint in the API Wizard.\n\n## Usage Tips\n\n1. Keep in mind that the API makes calls asynchronously, and returns a response or error via callbacks.\n2. Address slow response times or performance issues by changing model endpoints to lower resource models.\n\n## Conclusion\n\nThe Hugging Face Unity API offers a simple way to integrate AI models into your Unity projects. We hope you found this tutorial helpful. If you have any questions or would like to get more involved in using Hugging Face for Games, join the [Hugging Face Discord](https://hf.co/join/discord)!", "--\ntitle: \"How to host a Unity game in a Space\"\nthumbnail: /blog/assets/124_ml-for-games/unity-in-spaces-thumbnail.png\nauthors:\n- user: dylanebert\n---\n\n# How to host a Unity game in a Space\n\n<!-- {authors} --> \n\n\nDid you know you can host a Unity game in a Hugging Face Space? No? Well, you can!\n\nHugging Face Spaces are an easy way to build, host, and share demos. While they are typically used for Machine Learning demos, \nthey can also host playable Unity games. Here are some examples:\n- [Huggy](https://huggingface.co/spaces/ThomasSimonini/Huggy)\n- [Farming Game](https://huggingface.co/spaces/dylanebert/FarmingGame) \n- [Unity API Demo](https://huggingface.co/spaces/dylanebert/UnityDemo)\n\nHere's how you can host your own Unity game in a Space.\n\n## Step 1: Create a Space using the Static HTML template\n\nFirst, navigate to [Hugging Face Spaces](https://huggingface.co/new-space) to create a space.\n\n<figure class=\"image text-center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/games-in-spaces/1.png\">\n</figure> \n\nSelect the \"Static HTML\" template, give your Space a name, and create it.\n\n<figure class=\"image text-center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/games-in-spaces/2.png\">\n</figure> \n\n## Step 2: Use Git to Clone the Space\n\nClone your newly created Space to your local machine using Git. You can do this by running the following command in your terminal or command prompt:\n\n```\ngit clone https://huggingface.co/spaces/{your-username}/{your-space-name}\n```\n\n## Step 3: Open your Unity Project\n\nOpen the Unity project you want to host in your Space.\n\n<figure class=\"image text-center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/games-in-spaces/3.png\">\n</figure> \n\n## Step 4: Switch the Build Target to WebGL\n\nNavigate to `File > Build Settings` and switch the Build Target to WebGL.\n\n<figure class=\"image text-center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/games-in-spaces/4.png\">\n</figure> \n\n## Step 5: Open Player Settings\n\nIn the Build Settings window, click the \"Player Settings\" button to open the Player Settings panel.\n\n<figure class=\"image text-center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/games-in-spaces/5.png\">\n</figure> \n\n## Step 6: Optionally, Download the Hugging Face Unity WebGL Template\n\nYou can enhance your game's appearance in a Space by downloading the Hugging Face Unity WebGL template, available [here](https://github.com/huggingface/Unity-WebGL-template-for-Hugging-Face-Spaces). Just download the repository and drop it in your project files.\n\nThen, in the Player Settings panel, switch the WebGL template to Hugging Face. To do so, in Player Settings, click \"Resolution and Presentation\", then select the Hugging Face WebGL template.\n\n<figure class=\"image text-center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/games-in-spaces/6.png\">\n</figure> \n\n## Step 7: Change the Compression Format to Disabled\n\nIn the Player Settings panel, navigate to the \"Publishing Settings\" section and change the Compression Format to \"Disabled\".\n\n<figure class=\"image text-center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/games-in-spaces/7.png\">\n</figure> \n\n## Step 8: Build your Project\n\nReturn to the Build Settings window and click the \"Build\" button. Choose a location to save your build files, and Unity will build the project for WebGL.\n\n<figure class=\"image text-center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/games-in-spaces/8.png\">\n</figure> \n\n## Step 9: Copy the Contents of the Build Folder\n\nAfter the build process is finished, navigate to the folder containing your build files. Copy the files in the build folder to the repository you cloned in [Step 2](#step-2-use-git-to-clone-the-space).\n<figure class=\"image text-center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/games-in-spaces/9.png\">\n</figure> \n\n## Step 10: Enable Git-LFS for Large File Storage\n\nNavigate to your repository. Use the following commands to track large build files.\n\n```\ngit lfs install\ngit lfs track Build/* \n```\n\n## Step 11: Push your Changes\n\nFinally, use the following Git commands to push your changes:\n\n```\ngit add .\ngit commit -m \"Add Unity WebGL build files\"\ngit push\n```\n\n## Done!\n\nCongratulations! Refresh your Space. You should now be able to play your game in a Hugging Face Space.\n\nWe hope you found this tutorial helpful. If you have any questions or would like to get more involved in using Hugging Face for Games, join the [...", "his package provides core backend functionality for the Hugging Face Simulate project: (https://github.com/huggingface/simulate).\n\nTo use this package, add a GameObject to the scene, then add a `Simulator` component to it.\n\nCore Classes:\n- `Simulator`: Master controller singleton that tracks objects, loads and unloads plugins, etc.\n- `Node`: Component attached to every tracked object.\n- `RenderCamera`: Component attached to every tracked camera. Call `Render()` to asynchronously request a color buffer.\n- `Client`: Manages communication with the backend. Don't modify directly; instead, use plugins.\n\nPlugins:\n- `IPlugin`: For adding custom behaviour to the backend.\n- `ICommand`: For defining custom commands from the Python frontend.\n- `IGLTFExtension`: For defining custom GLTF extensions.\n\nTo add functionality to the backend:\n- Create a separate folder. For example, `RLAgents`.\n- Add an assembly definition to this folder (Right Click > Create > Assembly Definition).\n- In the assembly definition, add a reference to `Simulate`.\n- In the folder, create a script that implements `IPlugin`. For a simple example, look at `IPlugin.cs`, or for a more complex example, see `AgentsPlugin.cs`.\n\nIf this folder is in the Unity project, it will automatically be included in the build.\n\nTo add functionality post-build, a plugin can be compiled externally, then the DLL can be placed in the build's `Resources/Plugins/` folder."], "test_settings": "chunk_7000_embeddings_openai-text-embedding-3-small_reader-model_gpt-4", "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response provides a detailed and accurate step-by-step guide on how to install the Hugging Face Unity API in a Unity project. It includes all necessary steps, such as opening the Package Manager, adding the package from a git URL, and configuring the API key. Additionally, it offers optional steps for testing the API key, changing model endpoints, and installing example usage, which are helpful for users who may need further guidance. The response is comprehensive and aligns well with the criteria for a 5-point answer, as it is entirely correct, precise, and factual."}, {"question": "What is the pretraining objective of the Wav2Vec2 context network?\n", "true_answer": "The pretraining objective of the Wav2Vec2 context network is a contrastive task where the model has to predict the true quantized speech representation of the masked prediction from a set of false ones.", "source_doc": "huggingface/transformers/blob/main/docs/source/en/tasks_explained.md", "generated_answer": "The pretraining objective of the Wav2Vec2 context network is to mask the speech input in the latent space and solve a contrastive task defined over a quantization of the latent representations, which are jointly learned [\ubb38\uc11c 0].", "retrieved_docs": ["!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Wav2Vec2\n\n## Overview\n\nThe Wav2Vec2 model was proposed in [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations](https://arxiv.org/abs/2006.11477) by Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli.\n\nThe abstract from the paper is the following:\n\n*We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on\ntranscribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks\nthe speech input in the latent space and solves a contrastive task defined over a quantization of the latent\nrepresentations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the\nclean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state\nof the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and\npre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech\nrecognition with limited amounts of labeled data.*\n\nThis model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten).\n\n## Usage tips\n\n- Wav2Vec2 is a speech model that accepts a float array corresponding to the raw waveform of the speech signal.\n- Wav2Vec2 model was trained using connectionist temporal classification (CTC) so the model output has to be decoded\n  using [`Wav2Vec2CTCTokenizer`].\n\n## Resources\n\nA list of official Hugging Face and community (indicated by \ud83c\udf0e) resources to help you get started with Wav2Vec2. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n<PipelineTag pipeline=\"audio-classification\"/>\n\n- A notebook on how to [leverage a pretrained Wav2Vec2 model for emotion classification](https://colab.research.google.com/github/m3hrdadfi/soxan/blob/main/notebooks/Emotion_recognition_in_Greek_speech_using_Wav2Vec2.ipynb). \ud83c\udf0e\n- [`Wav2Vec2ForCTC`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/audio-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/audio_classification.ipynb).\n- [Audio classification task guide](../tasks/audio_classification)\n\n<PipelineTag pipeline=\"automatic-speech-recognition\"/>\n\n- A blog post on [boosting Wav2Vec2 with n-grams in \ud83e\udd17 Transformers](https://huggingface.co/blog/wav2vec2-with-ngram).\n- A blog post on how to [finetune Wav2Vec2 for English ASR with \ud83e\udd17 Transformers](https://huggingface.co/blog/fine-tune-wav2vec2-english).\n- A blog post on [finetuning XLS-R for Multi-Lingual ASR with \ud83e\udd17 Transformers](https://huggingface.co/blog/fine-tune-xlsr-wav2vec2).\n- A notebook on how to [create YouTube captions from any video by transcribing audio with Wav2Vec2](https://colab.research.google.com/github/Muennighoff/ytclipcc/blob/main/wav2vec_youtube_captions.ipynb). \ud83c\udf0e\n- [`Wav2Vec2ForCTC`] is supported by a notebook on [how to finetune a speech recognition model in English](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/speech_recognition.ipynb), and [how to finetune a speech recognition model in any language](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multi_lingual_speech_recognition.ipynb).\n- [Automatic speech recognition task guide](../tasks/asr)\n\n\ud83d\ude80 Deploy\n\n- A blog post on how to deploy Wav2Vec2 for [Automatic Speech Recogntion with Hugging Face's Transformers & Amazon SageMaker](https://www.philschmid.de/automatic-speech-recognition-sagemaker).\n\n## Wav2Vec2Config\n\n[[autodoc]] Wav2Vec2Config\n\n## Wav2Vec2CTCTokenizer\n\n[[autodoc]] Wav2Vec2CTCTokenizer\n    - __call__\n    - save_vocabulary\n    - decode\n    - batch_decode\n    - set_target_lang\n\n## Wav2Vec2FeatureExtractor\n\n[[autodoc]] Wav2Vec2FeatureExtractor\n    - __call__\n\n## Wav2Vec2Processor\n\n[[autodoc]] Wav2Vec2Processor\n    - __call__\n    - pad\n    - from_pretrained\n    - save_pretrained\n    - batch_decode\n    -...", "Wav2Vec2 Contrastive Loss PreTraining examples\n\nThe following example showcases how to pretrain a wav2vec2 model using the JAX/Flax backend.\nPretraining Wav2Vec2 is rather complex, so it is highly recommended to read the \n[official paper](https://arxiv.org/abs/2006.11477).\n\nJAX/Flax allows you to trace pure functions and compile them into efficient, fused accelerator code on both GPU and TPU.\nModels written in JAX/Flax are **immutable** and updated in a purely functional\nway which enables simple and efficient model parallelism.\n\n`run_wav2vec2_pretrain_flax.py` is a lightweight example of how to download and preprocess a dataset from the \ud83e\udd17 Datasets library or use your own files (jsonlines or csv), then pretrain the wav2vec2 architectures above on it.\n\nFor custom datasets in `jsonlines` format please see: [the Datasets documentation](https://huggingface.co/docs/datasets/loading_datasets#json-files) and you also will find examples of these below.\n\nLet's start by creating a model repository to save the trained model and logs.\nHere we call the model `\"wav2vec2-base-robust\"`, but you can change the model name as you like.\n\nYou can do this either directly on [huggingface.co](https://huggingface.co/new) (assuming that\nyou are logged in) or via the command line:\n\n```\nhuggingface-cli repo create wav2vec2-base-robust\n```\n\nNext we clone the model repository to add the tokenizer and model files.\n\n```\ngit clone https://huggingface.co/<your-username>/wav2vec2-base-robust\n```\n\nTo ensure that all tensorboard traces will be uploaded correctly, we need to \ntrack them. You can run the following command inside your model repo to do so.\n\n```\ncd wav2vec2-base-robust\ngit lfs track \"*tfevents*\"\n```\n\nGreat, we have set up our model repository. During training, we will automatically\npush the training logs and model weights to the repo.\n\nNext, let's add a symbolic link to the `run_wav2vec2_pretrain_flax`.\n\n```bash\nexport MODEL_DIR=\"./wav2vec2-base-robust\"\nln -s ~/transformers/examples/research_projects/jax-projects/wav2vec2/run_wav2vec2_pretrain_flax.py ./\n```\n\n### Create the model configuration\n\nLet's first create the model configuration and store it in the model repository. \nNote that many training parameters can be set in the model configuration including\nthe configuration about the masking distribution (`mask_time_length`, `mask_time_prob`), \ndropout (`attention_dropout`, ...), the trade-off between the contrastive loss and \nthe diversity loss, etc...\nMostly likely you will need to change these parameters depending on your use case.\nAgain, we highly recommend to read the [official paper](https://arxiv.org/abs/2006.11477) \nto better understand which parameters can be set for pretraining.\n\nFor this example, we will be using a `\"base\"`-sized model of Wav2Vec2 with robust \nlayer norm and keep most of the default settings.\n\n```python\nmodel_dir=\"./wav2vec2-base-robust\"\n\nfrom transformers import Wav2Vec2Config\nconfig = Wav2Vec2Config.from_pretrained(\n    \"facebook/wav2vec2-base\", \n    mask_time_length=10,\n    mask_time_prob=0.05,\n    diversity_loss_weight=0.1,\n    num_negatives=100,\n    do_stable_layer_norm=True,\n    feat_extract_norm=\"layer\",\n)\nconfig.save_pretrained(model_dir)\n```\n\n### Create a feature extractor configuration\n\nBefore we can start the training, we need to define \na feature extractor that takes care of normalization, etc...\n\nHere we can also re-use the feature extractor of [wav2vec2-base-960h](https://huggingface.co/facebook/wav2vec2-base) while making sure that padding is allowed.\n\n\n```python\nmodel_dir=\"./wav2vec2-base-robust\"\n\nfrom transformers import Wav2Vec2FeatureExtractor\nconfig = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\", return_attention_mask=True)\nconfig.save_pretrained(model_dir)\n```\n\n### Train the model\nFinally, we can run the example script to train the model:\n\n```bash\n./run_wav2vec2_pretrain_flax.py \\\n    --output_dir=${MODEL_DIR} \\\n    --num_train_epochs=\"5\" \\\n    --per_device_train_batch_size=\"32\" \\\n    --per_device_eval_batch_size=\"32\" \\\n    --learning_rate=\"5e-4\" \\\n    --weight_decay=\"0.01\" \\\n    --warmup_steps=\"2000\" \\\n    --model_name_or_path=${MODEL_DIR} \\\n    --dataset_name=\"librispeech_asr\" \\\n    --dataset_config_name=\"clean\" \\\n    --train_split_name=\"train.100\" \\\n    --preprocessing_num_workers=\"4\" \\\n    --max_duration_in_seconds=\"10.0\" \\\n    --adam_beta1=\"0.9\" \\\n    --adam_beta2=\"0.98\" \\\n    --pad_to_multiple_of=\"16384\" \\\n    --push_to_hub\n```\n\nNote that this script is not fully tested yet, so we cannot ensure that \nthe above script leads to satisfying results.", "!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Wav2Vec2Phoneme\n\n## Overview\n\nThe Wav2Vec2Phoneme model was proposed in [Simple and Effective Zero-shot Cross-lingual Phoneme Recognition (Xu et al.,\n2021](https://arxiv.org/abs/2109.11680) by Qiantong Xu, Alexei Baevski, Michael Auli.\n\nThe abstract from the paper is the following:\n\n*Recent progress in self-training, self-supervised pretraining and unsupervised learning enabled well performing speech\nrecognition systems without any labeled data. However, in many cases there is labeled data available for related\nlanguages which is not utilized by these methods. This paper extends previous work on zero-shot cross-lingual transfer\nlearning by fine-tuning a multilingually pretrained wav2vec 2.0 model to transcribe unseen languages. This is done by\nmapping phonemes of the training languages to the target language using articulatory features. Experiments show that\nthis simple method significantly outperforms prior work which introduced task-specific architectures and used only part\nof a monolingually pretrained model.*\n\nRelevant checkpoints can be found under https://huggingface.co/models?other=phoneme-recognition.\n\nThis model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten)\n\nThe original code can be found [here](https://github.com/pytorch/fairseq/tree/master/fairseq/models/wav2vec).\n\n## Usage tips\n\n- Wav2Vec2Phoneme uses the exact same architecture as Wav2Vec2\n- Wav2Vec2Phoneme is a speech model that accepts a float array corresponding to the raw waveform of the speech signal.\n- Wav2Vec2Phoneme model was trained using connectionist temporal classification (CTC) so the model output has to be\n  decoded using [`Wav2Vec2PhonemeCTCTokenizer`].\n- Wav2Vec2Phoneme can be fine-tuned on multiple language at once and decode unseen languages in a single forward pass\n  to a sequence of phonemes\n- By default, the model outputs a sequence of phonemes. In order to transform the phonemes to a sequence of words one\n  should make use of a dictionary and language model.\n\n\n<Tip>\n\nWav2Vec2Phoneme's architecture is based on the Wav2Vec2 model, for API reference, check out [`Wav2Vec2`](wav2vec2)'s documentation page \nexcept for the tokenizer.\n\n</Tip>\n\n## Wav2Vec2PhonemeCTCTokenizer\n\n[[autodoc]] Wav2Vec2PhonemeCTCTokenizer\n\t- __call__\n\t- batch_decode\n\t- decode\n\t- phonemize"], "test_settings": "chunk_7000_embeddings_openai-text-embedding-3-small_reader-model_gpt-4", "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response provides a generally accurate description of the pretraining objective of the Wav2Vec2 context network. It correctly identifies the use of a contrastive task and mentions the masking of speech input in the latent space. However, it lacks specificity in explaining that the model predicts the true quantized speech representation from a set of false ones, which is a key aspect of the pretraining objective. Additionally, the reference to \"[\ubb38\uc11c 0]\" is unnecessary and does not contribute to the clarity or accuracy of the response. Overall, the response is mostly correct but could be improved with more precise details."}, {"question": "What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n", "true_answer": "distilbert base uncased finetuned sst2 english", "source_doc": "huggingface/course/blob/main/subtitles/en/raw/chapter2/02_inside-pipeline-pt.md", "generated_answer": "The default checkpoint used by the sentiment analysis pipeline in the Transformers library is \"distilbert base uncased finetuned sst2 english\" [\ubb38\uc11c 0].", "retrieved_docs": ["hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated. Lastly, the return_tensors option tells the tokenizer to return a TensorFlow tensor. Looking at the result, we see we have a dictionary with two keys. Input IDs contains the IDs of both sentences, with 0s where the padding is applied. The second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. This is all what is inside the tokenization step. Now let's have a look at the second step, the model. As for the tokenizer, there is an TFAutoModel API, with a from_pretrained method. It will download and cache the configuration of the model as well as the pretrained weights. However, the TFAutoModel API will only instantiate the body of the model, that is, the part of the model that is left once the pretraining head is removed. It will output a high-dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. Here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size of our model 768. To get an output linked to our classification problem, we need to use the TFAutoModelForSequenceClassification class. It works exactly as the AutoModel class, except that it will build a model with a classification head. There is one auto class for each common NLP task in the Transformers library. Here, after giving our model the two sentences, we get a tensor of size two by two: one result for each sentence and for each possible label. Those outputs are not probabilities yet (we can see they don't sum to 1). This is because each model of the Transformers library returns logits. To make sense of those logits, we need to dig into the third and last step of the pipeline: post-processing. To convert logits into probabilities, we need to apply a SoftMax layer to them. As we can see, this transforms them into positive numbers that sum up to 1. The last step is to know which of those corresponds to the positive or the negative label. This is given by the id2label field of the model config. The first probabilities (index 0) correspond to the negative label, and the seconds (index 1) correspond to the positive label. This is how our classifier built with the pipeline function picked those labels and computed those scores. Now that you know how each steps works, you can easily tweak them to your needs.", "hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated. Lastly, the return_tensors option tells the tokenizer to return a PyTorch tensor. Looking at the result, we see we have a dictionary with two keys. Input IDs contains the IDs of both sentences, with 0s where the padding is applied. The second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. This is all what is inside the tokenization step. Now let's have a look at the second step, the model. As for the tokenizer, there is an AutoModel API, with a from_pretrained method. It will download and cache the configuration of the model as well as the pretrained weights. However, the AutoModel API will only instantiate the body of the model, that is, the part of the model that is left once the pretraining head is removed. It will output a high-dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. Here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size of our model 768. To get an output linked to our classification problem, we need to use the AutoModelForSequenceClassification class. It works exactly as the AutoModel class, except that it will build a model with a classification head. There is one auto class for each common NLP task in the Transformers library. Here, after giving our model the two sentences, we get a tensor of size two by two: one result for each sentence and for each possible label. Those outputs are not probabilities yet (we can see they don't sum to 1). This is because each model of the Transformers library returns logits. To make sense of those logits, we need to dig into the third and last step of the pipeline: post-processing. To convert logits into probabilities, we need to apply a SoftMax layer to them. As we can see, this transforms them into positive numbers that sum up to 1. The last step is to know which of those corresponds to the positive or the negative label. This is given by the id2label field of the model config. The first probabilities (index 0) correspond to the negative label, and the seconds (index 1) correspond to the positive label. This is how our classifier built with the pipeline function picked those labels and computed those scores. Now that you know how each steps works, you can easily tweak them to your needs.", "he pipeline function. The pipeline function is the most high-level API of the Transformers library. It regroups together all the steps to go from raw texts to usable predictions. The model used is at the core of a pipeline, but the pipeline also include all the necessary pre-processing (since the model does not expect texts, but numbers) as well as some post-processing to make the output of the model human-readable. Let's look at a first example with the sentiment analysis pipeline. This pipeline performs text classification on a given input, and determines if it's positive or negative. Here, it attributed the positive label on the given text, with a confidence of 95%. You can pass multiple texts to the same pipeline, which will be processed and passed through the model together, as a batch. The output is a list of individual results, in the same order as the input texts. Here we find the same label and score for the first text, and the second text is judged positive with a confidence of 99.99%. The zero-shot classification pipeline is a more general text-classification pipeline: it allows you to provide the labels you want. Here we want to classify our input text along the labels \"education\", \"politics\" and \"business\". The pipeline successfully recognizes it's more about education than the other labels, with a confidence of 84%. Moving on to other tasks, the text generation pipeline will auto-complete a given prompt. The output is generated with a bit of randomness, so it changes each time you call the generator object on a given prompt. Up until now, we have used the pipeline API with the default model associated to each task, but you can use it with any model that has been pretrained or fine-tuned on this task. Going on the model hub (huggingface.co/models), you can filter the available models by task. The default model used in our previous example was gpt2, but there are many more models available, and not just in English! Let's go back to the text generation pipeline and load it with another model, distilgpt2. This is a lighter version of gpt2 created by the Hugging Face team. When applying the pipeline to a given prompt, we can specify several arguments, such as the maximum length of the generated texts, or the number of sentences we want to return (since there is some randomness in the generation). Generating text by guessing the next word in a sentence was the pretraining objective of GPT-2, the fill mask pipeline is the pretraining objective of BERT, which is to guess the value of masked word. In this case, we ask the two most likely values for the missing words (according to the model) and get mathematical or computational as possible answers. Another task Transformers model can perform is to classify each word in the sentence instead of the sentence as a whole. One example of this is Named Entity Recognition, which is the task of identifying entities, such as persons, organizations or locations in a sentence. Here, the model correctly finds the person (Sylvain), the organization (Hugging Face) as well as the location (Brooklyn) inside the input text. The grouped_entities=True argument used is to make the pipeline group together the different words linked to the same entity (such as Hugging and Face here). Another task available with the pipeline API is extractive question answering. Providing a context and a question, the model will identify the span of text in the context containing the answer to the question. Getting short summaries of very long articles is also something the Transformers library can help with, with the summarization pipeline. Finally, the last task supported by the pipeline API is translation. Here we use a French/English model found on the model hub to get the English version of our input text. Here is a brief summary of all the tasks we looked into in this video. Try then out through the inference widgets in the model hub!"], "test_settings": "chunk_7000_embeddings_openai-text-embedding-3-small_reader-model_gpt-4", "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response correctly identifies the default checkpoint used by the sentiment analysis pipeline in the Transformers library as \"distilbert base uncased finetuned sst2 english.\" However, the inclusion of \"[\ubb38\uc11c 0]\" is unnecessary and does not contribute to the accuracy or factual nature of the response. Despite this minor issue, the core information provided is accurate and matches the criteria for a correct answer. Therefore, the response is mostly correct and factual, but the extraneous text slightly detracts from its precision."}, {"question": "What is the purpose of the notebook \"How to use DeepSpeed to train models with billions of parameters on Habana Gaudi\"?\n", "true_answer": "To show how to use DeepSpeed to pre-train/fine-tune the 1.6B-parameter GPT2-XL for causal language modeling on Habana Gaudi.", "source_doc": "huggingface/optimum/blob/main/docs/source/notebooks.md", "generated_answer": "The purpose of the notebook \"How to use DeepSpeed to train models with billions of parameters on Habana Gaudi\" is to demonstrate how to use DeepSpeed to pre-train or fine-tune the 1.6B-parameter GPT2-XL for causal language modeling on Habana Gaudi [\ubb38\uc11c 0].", "retrieved_docs": ["!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# \ud83e\udd17 Optimum notebooks\n\nYou can find here a list of the notebooks associated with each accelerator in \ud83e\udd17 Optimum.\n\n## Optimum Habana\n\n| Notebook                                                                                                                                                                               | Description                                                                                                                                                                       |  Colab                                                                                                                                                                                                          |        Studio Lab                                                                                                                                                                                                   |\n|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|\n| [How to use DeepSpeed to train models with billions of parameters on Habana Gaudi](https://github.com/huggingface/optimum-habana/blob/main/notebooks/AI_HW_Summit_2022.ipynb) | Show how to use DeepSpeed to pre-train/fine-tune the 1.6B-parameter GPT2-XL for causal language modeling on Habana Gaudi. |  [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/optimum-habana/blob/main/notebooks/AI_HW_Summit_2022.ipynb) | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/optimum-habana/blob/main/notebooks/AI_HW_Summit_2022.ipynb) |\n\n## Optimum Intel\n\n### OpenVINO\n\n| Notebook                                                                                                                                                                               | Description                                                                                                                                                                       |                                     Colab                                                                                                                                                                                                          |        Studio Lab                                                                                                                                                                                                   |\n|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|\n| [How to run inference with OpenVINO](https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipynb) | Explains how to export your model to OpenVINO and run inference with OpenVINO Runtime on various tasks| [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/optimum-in...", "Finally, you can launch the script as follows:\n```bash\npython ../gaudi_spawn.py --use_deepspeed --world_size 8 run_generation.py --model_name_or_path bigscience/bloomz --use_hpu_graphs --use_kv_cache --max_new_tokens 100\n```\n\nFor multi-node inference, you can follow [this guide](https://huggingface.co/docs/optimum/habana/usage_guides/multi_node_training) from the documentation of Optimum Habana.\n\nYou can also load any dataset from the Hugging Face Hub to get prompts that will be used for generation using the argument `--dataset_name my_dataset_name`.\n\nThis benchmark was performed with Transformers v4.28.1, SynapseAI v1.9.0 and Optimum Habana v1.5.0.\n\nFor GPUs, [here](https://github.com/huggingface/transformers-bloom-inference/blob/main/bloom-inference-scripts/bloom-ds-inference.py) is the script that led to the results that were previously presented in [this blog post](https://huggingface.co/blog/bloom-inference-pytorch-scripts) (and [here](https://github.com/huggingface/transformers-bloom-inference/tree/main/bloom-inference-scripts#deepspeed-inference) are the instructions to use it). To use CUDA graphs, static shapes are necessary and this is not supported in \ud83e\udd17 Transformers. You can use [this repo](https://github.com/HabanaAI/Model-References/tree/1.8.0/PyTorch/nlp/bloom) written by the Habana team to enable them.\n\n\n## Conclusion\n\nWe see in this article that **Habana Gaudi2 performs BLOOMZ inference faster than Nvidia A100 80GB**. And there is no need to write a complicated script as \ud83e\udd17 [Optimum Habana](https://huggingface.co/docs/optimum/habana/index) provides easy-to-use tools to run inference with multi-billion-parameter models on HPUs. Future releases of Habana's SynapseAI SDK are expected to speed up performance, so we will update this benchmark regularly as LLM inference optimizations on SynapseAI continue to advance. We are also looking forward to the performance benefits that will come with FP8 inference on Gaudi2.\n\nWe also presented the results achieved with first-generation Gaudi. For smaller models, it can perform on par with or even better than A100 for almost a third of its price. It is a good alternative option to using GPUs for running inference with such a big model like BLOOMZ.\n\nIf you are interested in accelerating your Machine Learning training and inference workflows using the latest AI hardware accelerators and software libraries, check out our [Expert Acceleration Program](https://huggingface.co/support). To learn more about Habana solutions, [read about our partnership and contact them here](https://huggingface.co/hardware/habana). To learn more about Hugging Face efforts to make AI hardware accelerators easy to use, check out our [Hardware Partner Program](https://huggingface.co/hardware).\n\n\n### Related Topics\n\n- [Faster Training and Inference: Habana Gaudi-2 vs Nvidia A100 80GB](https://huggingface.co/blog/habana-gaudi-2-benchmark)\n- [Leverage DeepSpeed to Train Faster and Cheaper Large Scale Transformer Models with Hugging Face and Habana Labs Gaudi](https://developer.habana.ai/events/leverage-deepspeed-to-train-faster-and-cheaper-large-scale-transformer-models-with-hugging-face-and-habana-labs-gaudi/)\n\n---\n\nThanks for reading! If you have any questions, feel free to contact me, either through [Github](https://github.com/huggingface/optimum-habana) or on the [forum](https://discuss.huggingface.co/c/optimum/59). You can also connect with me on [LinkedIn](https://www.linkedin.com/in/regispierrard/).\n\n[^1]: \u201cZero-shot\u201d refers to the ability of a model to complete a task on new or unseen input data, i.e. without having been provided any training examples of this kind of data. We provide the model with a prompt and a sequence of text that describes what we want our model to do, in natural language. Zero-shot classification excludes any examples of the desired task being completed. This differs from single or few-shot classification, as these tasks include a single or a few examples of the selected task.", "The following table displays the throughputs we got for first-gen Gaudi, Gaudi2 and Nvidia A100 80GB GPUs:\n\n<center>\n\n|   | First-gen Gaudi (BS=32) | Gaudi2 (BS=32) | Gaudi2 (BS=64) | A100 (BS=32) | A100 (BS=64) |\n|:-:|:-----------------------:|:--------------:|:--------------:|:-------:|:---------------------:|\n| Throughput (samples/s) | 520.2 | 1580.2 | 1835.8 | 981.6 | 1082.6 |\n| Speedup | x1.0 | x3.04 | x3.53 | x1.89 | x2.08 |\n\n</center>\n\n*BS* is the batch size per device. The Gaudi runs were performed in mixed precision (bf16/fp32) and the A100 runs in fp16. All runs were *distributed* runs on *8 devices*.\n\n\n### Generating Images from Text with Stable Diffusion\n\nOne of the main new features of \ud83e\udd17 Optimum Habana release 1.3 is [the support for Stable Diffusion](https://huggingface.co/docs/optimum/habana/usage_guides/stable_diffusion). It is now very easy to generate images from text on Gaudi. Unlike with \ud83e\udd17 Diffusers on GPUs, images are generated by batches. Due to model compilation times, the first two batches will be slower than the following iterations. In this benchmark, these first two iterations were discarded to compute the throughputs for both first-gen Gaudi and Gaudi2.\n\n[This script](https://github.com/huggingface/optimum-habana/tree/main/examples/stable-diffusion) was run for a batch size of 8 samples. It uses the [`Habana/stable-diffusion`](https://huggingface.co/Habana/stable-diffusion) Gaudi configuration.\n\nThe results we got, which are consistent with the numbers published by Habana [here](https://developer.habana.ai/resources/habana-models-performance/), are displayed in the table below.\n**Gaudi2 showcases latencies that are x3.51 faster than first-gen Gaudi (3.25s versus 0.925s) and x2.84 faster than Nvidia A100 (2.63s versus 0.925s).** It can also support bigger batch sizes.\n\n<center>\n\n|                 | First-gen Gaudi (BS=8) | Gaudi2 (BS=8) | A100 (BS=1) |\n|:---------------:|:----------------------:|:-------------:|:-----------:|\n| Latency (s/img) | 3.25                   | 0.925         | 2.63        |\n| Speedup         | x1.0                   | x3.51         | x1.24       |\n\n</center>\n\n*Update: the figures above were updated as SynapseAI 1.10 and Optimum Habana 1.6 bring an additional speedup on first-gen Gaudi and Gaudi2.*\n\n*BS* is the batch size.\nThe Gaudi runs were performed in *bfloat16* precision and the A100 runs in *fp16* precision (more information [here](https://huggingface.co/docs/diffusers/optimization/fp16)). All runs were *single-device* runs.\n\n\n### Fine-tuning T5-3B\n\nWith 96 GB of memory per device, Gaudi2 enables running much bigger models. For instance, we managed to fine-tune T5-3B (containing 3 billion parameters) with gradient checkpointing being the only applied memory optimization. This is not possible on first-gen Gaudi.\n[Here](https://huggingface.co/regisss/t5-3b-summarization-gaudi-2/tensorboard?scroll=1#scalars) are the logs of this run where the model was fine-tuned on the CNN DailyMail dataset for text summarization using [this script](https://github.com/huggingface/optimum-habana/tree/main/examples/summarization).\n\nThe results we achieved are presented in the table below. **Gaudi2 is x2.44 faster than A100 80GB.** We observe that we cannot fit a batch size larger than 1 on Gaudi2 here. This is due to the memory space taken by the graph where operations are accumulated during the first iteration of the run. Habana is working on optimizing the memory footprint in future releases of SynapseAI. We are looking forward to expanding this benchmark using newer versions of Habana's SDK and also using [DeepSpeed](https://www.deepspeed.ai/) to see if the same trend holds.\n\n<center>\n\n|   | First-gen Gaudi | Gaudi2 (BS=1) | A100 (BS=16) |\n|:-:|:-------:|:--------------:|:------------:|\n| Throughput (samples/s) | N/A | 19.7 | 8.07 |\n| Speedup | / | x2.44 | x1.0 |\n\n</center>\n\n*BS* is the batch size per device. Gaudi2 and A100 runs were performed in fp32 with gradient checkpointing enabled. All runs were *distributed* runs on *8 devices*.\n\n\n## Conclusion\n\nIn this article, we discuss our first experience with Gaudi2. The transition from first generation Gaudi to Gaudi2 is completely seamless since SynapseAI, Habana's SDK, is fully compatible with both. This means that new optimizations proposed by future releases will benefit both of them.\n\nYou have seen that Habana Gaudi2 significantly improves performance over first generation Gaudi and delivers about twice the throughput speed as Nvidia A100 80GB for both training and inference.\n\nYou also know now how to setup a Gaudi2 instance through the Intel Developer Zone. Check out the [examples](https://github.com/huggingface/optimum-habana/tree/main/examples) you can easily run on it with \ud83e\udd17 Optimum Habana.\n\nIf you are interested in accelerating your Machine Learning training and inference workflows using the latest AI hardware accelerators and software libraries, check out our [Expert Acceleration Program](https://huggingface...."], "test_settings": "chunk_7000_embeddings_openai-text-embedding-3-small_reader-model_gpt-4", "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response accurately captures the main purpose of the notebook, which is to demonstrate the use of DeepSpeed for training the GPT2-XL model on Habana Gaudi. It correctly identifies the model size (1.6B parameters) and the task (causal language modeling). However, the inclusion of \"[\ubb38\uc11c 0]\" is unnecessary and detracts slightly from the clarity and conciseness of the response. Despite this minor issue, the response is mostly aligned with the criteria for a 5-point answer, as it is largely correct and factual."}, {"question": "What command line module does PyTorch provide to run a script on multiple GPUs?\n", "true_answer": "torchrun", "source_doc": "huggingface/blog/blob/main/pytorch-ddp-accelerate-transformers.md", "generated_answer": "PyTorch provides the `torchrun` command line module to run a script on multiple GPUs. [\ubb38\uc11c 1]", "retrieved_docs": ["```bash\n_CudaDeviceProperties(name='GeForce RTX 3090', major=8, minor=6, total_memory=24268MB, multi_processor_count=82)\n```\n\nthen you know that this card's arch is `8.6`.\n\nYou can also leave `TORCH_CUDA_ARCH_LIST` out completely and then the build program will automatically query the\narchitecture of the GPUs the build is made on. This may or may not match the GPUs on the target machines, that's why\nit's best to specify the desired archs explicitly.\n\nIf after trying everything suggested you still encounter build issues, please, proceed with the GitHub Issue of\n[Deepspeed](https://github.com/microsoft/DeepSpeed/issues),\n\n\n\n<a id='deepspeed-multi-gpu'></a>\n\n### Deployment with multiple GPUs\n\nTo deploy the DeepSpeed integration adjust the [`Trainer`] command line arguments to include a new argument `--deepspeed ds_config.json`, where `ds_config.json` is the DeepSpeed configuration file as\n   documented [here](https://www.deepspeed.ai/docs/config-json/). The file naming is up to you.\n   It's recommended to use DeepSpeed's `add_config_arguments` utility to add the necessary command line arguments to your code.\n   For more information please see [DeepSpeed's Argument Parsing](https://deepspeed.readthedocs.io/en/latest/initialize.html#argument-parsing) doc.\n\nYou can use a launcher of your choice here. You can continue using the pytorch launcher:\n\n```bash\ntorch.distributed.run --nproc_per_node=2 your_program.py <normal cl args> --deepspeed ds_config.json\n```\nor use the launcher provided by `deepspeed`:\n\n```bash\ndeepspeed --num_gpus=2 your_program.py <normal cl args> --deepspeed ds_config.json\n```\n\nAs you can see the arguments aren't the same, but for most needs either of them works. The\nfull details on how to configure various nodes and GPUs can be found [here](https://www.deepspeed.ai/getting-started/#resource-configuration-multi-node).\n\nWhen you use the `deepspeed` launcher and you want to use all available gpus you can just omit the `--num_gpus` flag.\n\nHere is an example of running `run_translation.py` under DeepSpeed deploying all available GPUs:\n\n```bash\ndeepspeed examples/pytorch/translation/run_translation.py \\\n--deepspeed tests/deepspeed/ds_config_zero3.json \\\n--model_name_or_path t5-small --per_device_train_batch_size 1 \\\n--output_dir output_dir --overwrite_output_dir --fp16 \\\n--do_train --max_train_samples 500 --num_train_epochs 1 \\\n--dataset_name wmt16 --dataset_config \"ro-en\" \\\n--source_lang en --target_lang ro\n```\n\nNote that in the DeepSpeed documentation you are likely to see `--deepspeed --deepspeed_config ds_config.json` - i.e.\ntwo DeepSpeed-related arguments, but for the sake of simplicity, and since there are already so many arguments to deal\nwith, we combined the two into a single argument.\n\nFor some practical usage examples, please, see this [post](https://github.com/huggingface/transformers/issues/8771#issuecomment-759248400).\n\n\n\n<a id='deepspeed-one-gpu'></a>\n\n### Deployment with one GPU\n\nTo deploy DeepSpeed with one GPU adjust the [`Trainer`] command line arguments as follows:\n\n```bash\ndeepspeed --num_gpus=1 examples/pytorch/translation/run_translation.py \\\n--deepspeed tests/deepspeed/ds_config_zero2.json \\\n--model_name_or_path t5-small --per_device_train_batch_size 1 \\\n--output_dir output_dir --overwrite_output_dir --fp16 \\\n--do_train --max_train_samples 500 --num_train_epochs 1 \\\n--dataset_name wmt16 --dataset_config \"ro-en\" \\\n--source_lang en --target_lang ro\n```\n\nThis is almost the same as with multiple-GPUs, but here we tell DeepSpeed explicitly to use just one GPU via\n`--num_gpus=1`. By default, DeepSpeed deploys all GPUs it can see on the given node. If you have only 1 GPU to start\nwith, then you don't need this argument. The following [documentation](https://www.deepspeed.ai/getting-started/#resource-configuration-multi-node) discusses the launcher options.\n\nWhy would you want to use DeepSpeed with just one GPU?\n\n1. It has a ZeRO-offload feature which can delegate some computations and memory to the host's CPU and RAM, and thus\n   leave more GPU resources for model's needs - e.g. larger batch size, or enabling a fitting of a very big model which\n   normally won't fit.\n2. It provides a smart GPU memory management system, that minimizes memory fragmentation, which again allows you to fit\n   bigger models and data batches.\n\nWhile we are going to discuss the configuration in details next, the key to getting a huge improvement on a single GPU\nwith DeepSpeed is to have at least the following configuration in the configuration file:\n\n```json\n{\n  \"zero_optimization\": {\n     \"stage\": 2,\n     \"offload_optimizer\": {\n         \"device\": \"cpu\",\n         \"pin_memory\": true\n     },\n     \"allgather_partitions\": true,\n     \"allgather_bucket_size\": 2e8,\n     \"reduce_scatter\": true,\n     \"reduce_bucket_size\": 2e8,\n     \"overlap_comm\": true,\n     \"contiguous_gradients\": true\n  }\n}\n```\n\nwhich enables optimizer offload and some other important features. You may experiment with the buffer sizes, you will\nfin...", "## Running quick tests\n\nMost examples are equipped with a mechanism to truncate the number of dataset samples to the desired length. This is useful for debugging purposes, for example to quickly check that all stages of the programs can complete, before running the same setup on the full dataset which may take hours to complete.\n\nFor example here is how to truncate all three splits to just 50 samples each:\n```\nexamples/pytorch/token-classification/run_ner.py \\\n--max_train_samples 50 \\\n--max_eval_samples 50 \\\n--max_predict_samples 50 \\\n[...]\n```\n\nMost example scripts should have the first two command line arguments and some have the third one. You can quickly check if a given example supports any of these by passing a `-h` option, e.g.:\n```\nexamples/pytorch/token-classification/run_ner.py -h\n```\n\n## Resuming training\n\nYou can resume training from a previous checkpoint like this:\n\n1. Pass `--output_dir previous_output_dir` without `--overwrite_output_dir` to resume training from the latest checkpoint in `output_dir` (what you would use if the training was interrupted, for instance).\n2. Pass `--resume_from_checkpoint path_to_a_specific_checkpoint` to resume training from that checkpoint folder.\n\nShould you want to turn an example into a notebook where you'd no longer have access to the command\nline, \ud83e\udd17 Trainer supports resuming from a checkpoint via `trainer.train(resume_from_checkpoint)`.\n\n1. If `resume_from_checkpoint` is `True` it will look for the last checkpoint in the value of `output_dir` passed via `TrainingArguments`.\n2. If `resume_from_checkpoint` is a path to a specific checkpoint it will use that saved checkpoint folder to resume the training from.\n\n\n### Upload the trained/fine-tuned model to the Hub\n\nAll the example scripts support automatic upload of your final model to the [Model Hub](https://huggingface.co/models) by adding a `--push_to_hub` argument. It will then create a repository with your username slash the name of the folder you are using as `output_dir`. For instance, `\"sgugger/test-mrpc\"` if your username is `sgugger` and you are working in the folder `~/tmp/test-mrpc`.\n\nTo specify a given repository name, use the `--hub_model_id` argument. You will need to specify the whole repository name (including your username), for instance `--hub_model_id sgugger/finetuned-bert-mrpc`. To upload to an organization you are a member of, just use the name of that organization instead of your username: `--hub_model_id huggingface/finetuned-bert-mrpc`.\n\nA few notes on this integration:\n\n- you will need to be logged in to the Hugging Face website locally for it to work, the easiest way to achieve this is to run `huggingface-cli login` and then type your username and password when prompted. You can also pass along your authentication token with the `--hub_token` argument.\n- the `output_dir` you pick will either need to be a new folder or a local clone of the distant repository you are using.\n\n## Distributed training and mixed precision\n\nAll the PyTorch scripts mentioned above work out of the box with distributed training and mixed precision, thanks to\nthe [Trainer API](https://huggingface.co/transformers/main_classes/trainer.html). To launch one of them on _n_ GPUs,\nuse the following command:\n\n```bash\ntorchrun \\\n    --nproc_per_node number_of_gpu_you_have path_to_script.py \\\n\t--all_arguments_of_the_script\n```\n\nAs an example, here is how you would fine-tune the BERT large model (with whole word masking) on the text\nclassification MNLI task using the `run_glue` script, with 8 GPUs:\n\n```bash\ntorchrun \\\n    --nproc_per_node 8 pytorch/text-classification/run_glue.py \\\n    --model_name_or_path bert-large-uncased-whole-word-masking \\\n    --task_name mnli \\\n    --do_train \\\n    --do_eval \\\n    --max_seq_length 128 \\\n    --per_device_train_batch_size 8 \\\n    --learning_rate 2e-5 \\\n    --num_train_epochs 3.0 \\\n    --output_dir /tmp/mnli_output/\n```\n\nIf you have a GPU with mixed precision capabilities (architecture Pascal or more recent), you can use mixed precision\ntraining with PyTorch 1.6.0 or latest, or by installing the [Apex](https://github.com/NVIDIA/apex) library for previous\nversions. Just add the flag `--fp16` to your command launching one of the scripts mentioned above!\n\nUsing mixed precision training usually results in 2x-speedup for training with the same final results (as shown in\n[this table](https://github.com/huggingface/transformers/tree/main/examples/text-classification#mixed-precision-training)\nfor text classification).\n\n## Running on TPUs\n\nWhen using Tensorflow, TPUs are supported out of the box as a `tf.distribute.Strategy`.\n\nWhen using PyTorch, we support TPUs thanks to `pytorch/xla`. For more context and information on how to setup your TPU environment refer to Google's documentation and to the\nvery detailed [pytorch/xla README](https://github.com/pytorch/xla/blob/master/README.md).\n\nIn this repo, we provide a very simple launcher script named\n[xla_spawn.py](https://github.com/huggingface/tra...", "!---\nCopyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n# Multiple-choice training (e.g. SWAG)\n\nThis folder contains the `run_swag.py` script, showing an examples of *multiple-choice answering* with the \n\ud83e\udd17 Transformers library. For straightforward use-cases you may be able to use these scripts without modification, \nalthough we have also included comments in the code to indicate areas that you may need to adapt to your own projects.\n\n### Multi-GPU and TPU usage\n\nBy default, the script uses a `MirroredStrategy` and will use multiple GPUs effectively if they are available. TPUs\ncan also be used by passing the name of the TPU resource with the `--tpu` argument.\n\n### Memory usage and data loading\n\nOne thing to note is that all data is loaded into memory in this script. Most multiple-choice datasets are small\nenough that this is not an issue, but if you have a very large dataset you will need to modify the script to handle\ndata streaming. This is particularly challenging for TPUs, given the stricter requirements and the sheer volume of data\nrequired to keep them fed. A full explanation of all the possible pitfalls is a bit beyond this example script and \nREADME, but for more information you can see the 'Input Datasets' section of \n[this document](https://www.tensorflow.org/guide/tpu).\n\n### Example command\n```bash\npython run_swag.py \\\n --model_name_or_path distilbert-base-cased \\\n --output_dir output \\\n --do_eval \\\n --do_train\n```"], "test_settings": "chunk_7000_embeddings_openai-text-embedding-3-small_reader-model_gpt-4", "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response accurately identifies \"torchrun\" as the correct command line module provided by PyTorch for running scripts on multiple GPUs. The inclusion of \"[\ubb38\uc11c 1]\" is extraneous but does not affect the correctness of the answer."}, {"question": "What is the most popular vision transformer model on the Hugging Face Model Hub for image classification?\n", "true_answer": "google/vit-base-patch16-224", "source_doc": "gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/image-classification-with-vision-transformers.md", "generated_answer": "I'm sorry, the context does not provide information about the most popular vision transformer model on the Hugging Face Model Hub for image classification.", "retrieved_docs": ["--\ntitle: \"Deep Dive: Vision Transformers On Hugging Face Optimum Graphcore\"\nthumbnail: /blog/assets/97_vision_transformers/thumbnail.png\nauthors:\n- user: juliensimon\n---\n\n# Deep Dive: Vision Transformers On Hugging Face Optimum Graphcore\n\n\nThis blog post will show how easy it is to fine-tune pre-trained Transformer models for your dataset using the Hugging Face Optimum library on Graphcore Intelligence Processing Units (IPUs). As an example, we will show a step-by-step guide and provide a notebook that takes a large, widely-used chest X-ray dataset and trains a vision transformer (ViT) model.\n\n<h2>Introducing vision transformer (ViT) models</h2>", "!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n\n<p align=\"center\">\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-dark.svg\">\n    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg\">\n    <img alt=\"Hugging Face Transformers Library\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg\" width=\"352\" height=\"59\" style=\"max-width: 100%;\">\n  </picture>\n  <br/>\n  <br/>\n</p>\n\n<p align=\"center\">\n    <a href=\"https://circleci.com/gh/huggingface/transformers\">\n        <img alt=\"Build\" src=\"https://img.shields.io/circleci/build/github/huggingface/transformers/main\">\n    </a>\n    <a href=\"https://github.com/huggingface/transformers/blob/main/LICENSE\">\n        <img alt=\"GitHub\" src=\"https://img.shields.io/github/license/huggingface/transformers.svg?color=blue\">\n    </a>\n    <a href=\"https://huggingface.co/docs/transformers/index\">\n        <img alt=\"Documentation\" src=\"https://img.shields.io/website/http/huggingface.co/docs/transformers/index.svg?down_color=red&down_message=offline&up_message=online\">\n    </a>\n    <a href=\"https://github.com/huggingface/transformers/releases\">\n        <img alt=\"GitHub release\" src=\"https://img.shields.io/github/release/huggingface/transformers.svg\">\n    </a>\n    <a href=\"https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md\">\n        <img alt=\"Contributor Covenant\" src=\"https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg\">\n    </a>\n    <a href=\"https://zenodo.org/badge/latestdoi/155220641\"><img src=\"https://zenodo.org/badge/155220641.svg\" alt=\"DOI\"></a>\n</p>\n\n<h4 align=\"center\">\n    <p>\n        <b>English</b> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/README_zh-hans.md\">\u7b80\u4f53\u4e2d\u6587</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/README_zh-hant.md\">\u7e41\u9ad4\u4e2d\u6587</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/README_ko.md\">\ud55c\uad6d\uc5b4</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/README_es.md\">Espa\u00f1ol</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/README_ja.md\">\u65e5\u672c\u8a9e</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/README_hd.md\">\u0939\u093f\u0928\u094d\u0926\u0940</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/README_ru.md\">\u0420\u0443\u0441\u0441\u043a\u0438\u0439</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/README_pt-br.md\">\u0420ortugu\u00eas</a> |\n        <a href=\"https://github.com/huggingface/transformers//blob/main/README_te.md\">\u0c24\u0c46\u0c32\u0c41\u0c17\u0c41</a> |\n    </p>\n</h4>\n\n<h3 align=\"center\">\n    <p>State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow</p>\n</h3>\n\n<h3 align=\"center\">\n    <a href=\"https://hf.co/course\"><img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/course_banner.png\"></a>\n</h3>\n\n\ud83e\udd17 Transformers provides thousands of pretrained models to perform tasks on different modalities such as text, vision, and audio.\n\nThese models can be applied on:\n\n* \ud83d\udcdd Text, for tasks like text classification, information extraction, question answering, summarization, translation, and text generation, in over 100 languages.\n* \ud83d\uddbc\ufe0f Images, for tasks like image classification, object detection, and segmentation.\n* \ud83d\udde3\ufe0f Audio, for tasks like speech recognition and audio classification.\n\nTransformer models can also perform tasks on **several modalities combined**, such as table question answering, optical character recognition, information extraction from scanned documents, video classification, and visual question answering.\n\n\ud83e\udd17 Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and then share them with the community on our [model hub](https://huggingface.co/models). At the same time, each python module defining an architecture is fully standalone and can be modified to enable quick research experiments.\n\n\ud83e\udd17 Transformers is backed by the three most popular deep learning libraries \u2014 [Jax](https://jax.readthedocs.io/en/latest/), [PyTorch](https://pytorch.org/) and [TensorFlow](https://www.tensorflow.org/) \u2014 with a seamless integration between them. It's straightforward to train your models with one before l...", "The Model Hub\n\n## What is the Model Hub?\n\nThe Model Hub is where the members of the Hugging Face community can host all of their model checkpoints for simple storage, discovery, and sharing. Download pre-trained models with the [`huggingface_hub` client library](https://huggingface.co/docs/huggingface_hub/index), with \ud83e\udd17  [`Transformers`](https://huggingface.co/docs/transformers/index) for fine-tuning and other usages or with any of the over [15 integrated libraries](./models-libraries). You can even leverage the [Inference API](./models-inference) to use models in production settings.\n\nYou can refer to the following video for a guide on navigating the Model Hub:\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube-nocookie.com/embed/XvSGPZFEjDY\" title=\"Model Hub Video\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n\nTo learn how to upload models to the Hub, you can refer to the [Repositories Getting Started Guide](./repositories-getting-started)."], "test_settings": "chunk_7000_embeddings_openai-text-embedding-3-small_reader-model_gpt-4", "eval_score_GPT4": "1", "eval_feedback_GPT4": "The response does not provide the specific information requested in the task, which is the name of the most popular vision transformer model on the Hugging Face Model Hub for image classification. Instead, it states a lack of context, which does not address the question. The response is not aligned with the criteria for accuracy and factuality as it fails to provide any relevant information. Therefore, it does not meet the requirements for a correct or accurate answer based on the rubric."}, {"question": "What is the command to upload an ESPnet model to a Hugging Face repository?\n", "true_answer": "./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo", "source_doc": "huggingface/hub-docs/blob/main/docs/hub/espnet.md", "generated_answer": "To upload an ESPnet model to a Hugging Face repository, you can use the following command:\n\n```bash\n./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo\n```\n\nThis command is part of the `run.sh` script used in ESPnet for uploading models. [Document 0]", "retrieved_docs": ["Using ESPnet at Hugging Face\n\n`espnet` is an end-to-end toolkit for speech processing, including automatic speech recognition, text to speech, speech enhancement, dirarization and other tasks.\n\n## Exploring ESPnet in the Hub\n\nYou can find hundreds of `espnet` models by filtering at the left of the [models page](https://huggingface.co/models?library=espnet&sort=downloads). \n\nAll models on the Hub come up with useful features:\n1. An automatically generated model card with a description, a training configuration, licenses and more.\n2. Metadata tags that help for discoverability and contain information such as license, language and datasets.\n3. An interactive widget you can use to play out with the model directly in the browser.\n4. An Inference API that allows to make inference requests.\n\n<div class=\"flex justify-center\">\n<img class=\"block dark:hidden\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_widget.png\"/>\n<img class=\"hidden dark:block\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_widget-dark.png\"/>\n</div>\n\n## Using existing models\n\nFor a full guide on loading pre-trained models, we recommend checking out the [official guide](https://github.com/espnet/espnet_model_zoo)). \n\nIf you're interested in doing inference, different classes for different tasks have a `from_pretrained` method that allows loading models from the Hub. For example:\n* `Speech2Text` for Automatic Speech Recognition.\n* `Text2Speech` for Text to Speech.\n* `SeparateSpeech` for Audio Source Separation.\n\nHere is an inference example:\n\n```py\nimport soundfile\nfrom espnet2.bin.tts_inference import Text2Speech\n\ntext2speech = Text2Speech.from_pretrained(\"model_name\")\nspeech = text2speech(\"foobar\")[\"wav\"]\nsoundfile.write(\"out.wav\", speech.numpy(), text2speech.fs, \"PCM_16\")\n```\n\nIf you want to see how to load a specific model, you can click `Use in ESPnet` and you will be given a working snippet that you can load it! \n\n<div class=\"flex justify-center\">\n<img class=\"block dark:hidden\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_snippet.png\"/>\n<img class=\"hidden dark:block\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_snippet-dark.png\"/>\n</div>\n\n## Sharing your models\n\n`ESPnet` outputs a `zip` file that can be uploaded to Hugging Face easily. For a full guide on sharing  models, we recommend checking out the [official guide](https://github.com/espnet/espnet_model_zoo#register-your-model)).\n\nThe `run.sh` script allows to upload a given model to a Hugging Face repository.\n\n```bash\n./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo\n```\n\n## Additional resources\n\n* ESPnet [docs](https://espnet.github.io/espnet/index.html).\n* ESPnet model zoo [repository](https://github.com/espnet/espnet_model_zoo).\n* Integration [docs](https://github.com/asteroid-team/asteroid/blob/master/docs/source/readmes/pretrained_models.md).", "Uploading models\n\nTo upload models to the Hub, you'll need to create an account at [Hugging Face](https://huggingface.co/join). Models on the Hub are [Git-based repositories](./repositories), which give you versioning, branches, discoverability and sharing features, integration with over a dozen libraries, and more! You have control over what you want to upload to your repository, which could include checkpoints, configs, and any other files.\n\nYou can link repositories with an individual, such as [osanseviero/fashion_brands_patterns](https://huggingface.co/osanseviero/fashion_brands_patterns), or with an organization, such as [facebook/bart-large-xsum](https://huggingface.co/facebook/bart-large-xsum). Organizations can collect models related to a company, community, or library! If you choose an organization, the model will be featured on the organization\u2019s page, and every member of the organization will have the ability to contribute to the repository. You can create a new organization [here](https://huggingface.co/organizations/new).\n\nThere are several ways to upload models to the Hub, described below. We suggest adding a [Model Card](./model-cards) to your repo to document your model.\n\n## Using the web interface\n\nTo create a brand new model repository, visit [huggingface.co/new](http://huggingface.co/new). Then follow these steps:\n\n1. In the \"Files and versions\" tab, select \"Add File\" and specify \"Upload File\":\n\n<div class=\"flex justify-center\">\n<img class=\"block dark:hidden\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/add-file.png\"/>\n<img class=\"hidden dark:block\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/add-file-dark.png\"/>\n</div>\n\n2. From there, select a file from your computer to upload and leave a helpful commit message to know what you are uploading:\n\n<div class=\"flex justify-center\">\n<img class=\"block dark:hidden\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/commit-file.png\"/>\n<img class=\"hidden dark:block\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/commit-file-dark.png\"/>\n</div>\n\n3. Afterwards, click **Commit changes** to upload your model to the Hub!\n\n4. Inspect files and history\n\nYou can check your repository with all the recently added files!\n\n<div class=\"flex justify-center\">\n<img class=\"block dark:hidden\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/repo_with_files.png\"/>\n<img class=\"hidden dark:block\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/repo_with_files-dark.png\"/>\n</div>\n\nThe UI allows you to explore the model files and commits and to see the diff introduced by each commit:\n\n<div class=\"flex justify-center\">\n<img class=\"block dark:hidden\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/explore_history.gif\"/>\n<img class=\"hidden dark:block\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/explore_history-dark.gif\"/>\n</div>\n\n5. Add metadata\n\nYou can add metadata to your model card. You can specify:\n* the type of task this model is for, enabling widgets and the Inference API.\n* the used library (`transformers`, `spaCy`, etc.)\n* the language\n* the dataset\n* metrics\n* license\n* a lot more!\n\nRead more about model tags [here](./model-cards#model-card-metadata).\n\n6. Add TensorBoard traces\n\nAny repository that contains TensorBoard traces (filenames that contain `tfevents`) is categorized with the [`TensorBoard` tag](https://huggingface.co/models?filter=tensorboard). As a convention, we suggest that you save traces under the `runs/` subfolder. The \"Training metrics\" tab then makes it easy to review charts of the logged variables, like the loss or the accuracy.\n\n<div class=\"flex justify-center\">\n<img class=\"block dark:hidden\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/tensorboard.png\"/>\n<img class=\"hidden dark:block\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/tensorboard-dark.png\"/>\n</div>\n\nModels trained with \ud83e\udd17 Transformers will generate [TensorBoard traces](https://huggingface.co/docs/transformers/main_classes/callback#transformers.integrations.TensorBoardCallback) by default if [`tensorboard`](https://pypi.org/project/tensorboard/) is installed.\n\n## Using Git\n\nSince model repos are just Git repositories, you can use Git to push your model files to the Hub. Follow the guide on [Getting Started with Repositories](repositories-getting-started) to learn about using the `git` CLI to commit and push your models.\n\n## Using the `huggingface_hub` client library\n\nThe rich feature set in the `huggingface_hub` library allows you to manage repositories, including creating repos and uploading models to the Model Hub. Visit [the client library's documentation](https:...", "Integrate your library with the Hub\n\nThe Hugging Face Hub aims to facilitate sharing machine learning models, checkpoints, and artifacts. This endeavor includes integrating the Hub into many of the amazing third-party libraries in the community. Some of the ones already integrated include [spaCy](https://spacy.io/usage/projects#huggingface_hub), [AllenNLP](https://allennlp.org/), and [timm](https://rwightman.github.io/pytorch-image-models/), among many others. Integration means users can download and upload files to the Hub directly from your library. We hope you will integrate your library and join us in democratizing artificial intelligence for everyone.\n\nIntegrating the Hub with your library provides many benefits, including:\n\n- Free model hosting for you and your users.\n- Built-in file versioning - even for huge files - made possible by [Git-LFS](https://git-lfs.github.com/).\n- All public models are powered by the [Inference API](https://huggingface.co/docs/api-inference/index).\n- In-browser widgets allow users to interact with your hosted models directly.\n\nThis tutorial will help you integrate the Hub into your library so your users can benefit from all the features offered by the Hub.\n\nBefore you begin, we recommend you create a [Hugging Face account](https://huggingface.co/join) from which you can manage your repositories and files. \n\nIf you need help with the integration, feel free to open an [issue](https://github.com/huggingface/huggingface_hub/issues/new/choose), and we would be more than happy to help you.\n\n## Installation\n\n1. Install the `huggingface_hub` library with pip in your environment:\n\n   ```bash\n   python -m pip install huggingface_hub\n   ```\n\n2. Once you have successfully installed the `huggingface_hub` library, log in to your Hugging Face account:\n\n   ```bash\n   huggingface-cli login\n   ```\n\n   ```bash\n        _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n        _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n        _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n        _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n        _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n\n        \n   Username: \n   Password:\n   ```\n\n3. Alternatively, if you prefer working from a Jupyter or Colaboratory notebook, login with `notebook_login`:\n\n   ```python\n   >>> from huggingface_hub import notebook_login\n   >>> notebook_login()\n   ```\n\n   `notebook_login` will launch a widget in your notebook from which you can enter your Hugging Face credentials.\n\n## Download files from the Hub\n\nIntegration allows users to download your hosted files directly from the Hub using your library. \n\nUse the `hf_hub_download` function to retrieve a URL and download files from your repository. Downloaded files are stored in your cache: `~/.cache/huggingface/hub`. You don't have to re-download the file the next time you use it, and for larger files, this can save a lot of time. Furthermore, if the repository is updated with a new version of the file, `huggingface_hub` will automatically download the latest version and store it in the cache for you. Users don't have to worry about updating their files.\n\nFor example, download the `config.json` file from the [lysandre/arxiv-nlp](https://huggingface.co/lysandre/arxiv-nlp) repository:\n\n```python\n>>> from huggingface_hub import hf_hub_download\n>>> hf_hub_download(repo_id=\"lysandre/arxiv-nlp\", filename=\"config.json\")\n```\n\nDownload a specific version of the file by specifying the `revision` parameter. The `revision` parameter can be a branch name, tag, or commit hash. \n\nThe commit hash must be a full-length hash instead of the shorter 7-character commit hash:\n\n```python\n>>> from huggingface_hub import hf_hub_download\n>>> hf_hub_download(repo_id=\"lysandre/arxiv-nlp\", filename=\"config.json\", revision=\"877b84a8f93f2d619faa2a6e514a32beef88ab0a\")\n```\n\nUse the `cache_dir` parameter to change where a file is stored:\n\n```python\n>>> from huggingface_hub import hf_hub_download\n>>> hf_hub_download(repo_id=\"lysandre/arxiv-nlp\", filename=\"config.json\", cache_dir=\"/home/lysandre/test\")\n```\n\n### Code sample\n\nWe recommend adding a code snippet to explain how to use a model in your downstream library. \n\n<div class=\"flex justify-center\">\n<img class=\"block dark:hidden\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/code_snippet.png\"/>\n<img class=\"hidden dark:block\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/code_snippet-dark.png\"/>\n</div>\n\nFirst, register your library by editing [model-libaries.ts](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/model-libraries.ts).\n\nThen, add a code ..."], "test_settings": "chunk_7000_embeddings_openai-text-embedding-3-small_reader-model_gpt-4", "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response provides the correct command to upload an ESPnet model to a Hugging Face repository, matching the criteria of the ideal answer. The inclusion of the command within a code block is appropriate and enhances clarity. The additional context about the command being part of the `run.sh` script used in ESPnet is accurate and provides useful information, although it is not necessary for the answer to be considered correct. Overall, the response is entirely correct, accurate, and factual."}, {"question": "What file should be added to a model repository to install custom Python dependencies for Inference Endpoints?\n", "true_answer": "requirements.txt", "source_doc": "huggingface/hf-endpoints-documentation/blob/main/docs/source/guides/custom_dependencies.mdx", "generated_answer": "To install custom Python dependencies for Inference Endpoints, a `requirements.txt` file should be added to a model repository. [\ubb38\uc11c 0]", "retrieved_docs": ["Add custom Dependencies\n\nInference Endpoints\u2019 base image includes all required libraries to run inference on \ud83e\udd17 Transformers models, but it also supports custom dependencies. This is useful if you want to:\n\n* [customize your inference pipeline](/docs/inference-endpoints/guides/custom_handler) and need additional Python dependencies\n* run a model which requires special dependencies like the newest or a fixed version of a library (for example, `tapas` (`torch-scatter`)).\n\nTo add custom dependencies, add a `requirements.txt` [file](https://huggingface.co/philschmid/distilbert-onnx-banking77/blob/main/requirements.txt) with the Python dependencies you want to install in your model repository on the Hugging Face Hub. When your Endpoint and Image artifacts are created, Inference Endpoints checks if the model repository contains a `requirements.txt ` file and installs the dependencies listed within.\n\n```bash\noptimum[onnxruntime]==1.2.3\nmkl-include\nmkl\n```\n\nCheck out the `requirements.txt` files in the following model repositories for examples:\n\n* [Optimum and onnxruntime](https://huggingface.co/philschmid/distilbert-onnx-banking77/blob/main/requirements.txt)\n* [diffusers](https://huggingface.co/philschmid/stable-diffusion-v1-4-endpoints/blob/main/requirements.txt)\n\nFor more information, take a look at how you can create and install dependencies when you [use your own custom container](/docs/inference-endpoints/guides/custom_container) for inference.", "Use a custom Container Image\n\n\nInference Endpoints not only allows you to [customize your inference handler](/docs/inference-endpoints/guides/custom_handler), but it also allows you to provide a custom container image. Those can be public images like `tensorflow/serving:2.7.3` or private Images hosted on [Docker Hub](https://hub.docker.com/), [AWS ECR](https://aws.amazon.com/ecr/?nc1=h_ls), [Azure ACR](https://azure.microsoft.com/de-de/services/container-registry/), or [Google GCR](https://cloud.google.com/container-registry?hl=de). \n\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/custom_container.png\" alt=\"custom container config\" />\n\n\nThe [creation flow](/docs/inference-endpoints/guides/create_endpoint) of your Image artifacts from a custom image is the same as the base image. This means Inference Endpoints will create a unique image artifact derived from your provided image, including all Model Artifacts. \n\nThe Model Artifacts (weights) are stored under `/repository`. For example, if you use` tensorflow/serving` as your custom image, then you have to set `model_base_path=\"/repository\":\n\n\n```\ntensorflow_model_server \\\n  --rest_api_port=5000 \\\n  --model_name=my_model \\\n  --model_base_path=\"/repository\"\n```", "Inference Endpoints Version\n\nHugging Face Inference Endpoints comes with a default serving container which is used for all [supported Transformers and Sentence-Transformers tasks](/docs/inference-endpoints/supported_tasks) and for [custom inference handler](/docs/inference-endpoints/guides/custom_handler) and implement batching.\nBelow you will find information about the installed packages and versions used.\n\nYou can always upgrade installed packages and a custom packages by adding a `requirements.txt` file to your model repository. Read more in [Add custom Dependencies](/docs/inference-endpoints/guides/custom_dependencies).\n\n## Installed packages & version\n\nThe installed packages are split into `general`, `CPU` & `GPU` packages. The `general` packages are installed in all containers, the `CPU` and `GPU` packages are only installed in the corresponding containers.\nThe Hugging Face Inference Runtime has separate versions for `PyTorch` and `TensorFlow` for `CPU` and `GPU`, which are used based on the selected `framework` when creating an Inference Endpoint. The `TensorFlow` and `PyTorch` flavors are grouped together in the list below.\n\n### General\n\n- `Python`: `3.9.13`\n\n### CPU\n\n- `transformers[sklearn,sentencepiece,audio,vision]`: `4.27.2`\n- `diffusers`: `0.14.0`\n- `accelerate`: `0.17.1`\n- `sentence_transformers`: `latest`\n- `pandas`: `latest`\n- `pytorch`: `1.13.1`\n- `torchvision`: `0.14.1`\n- `tensorflow`: `2.9.1`\n\n### GPU\n\n- `transformers[sklearn,sentencepiece,audio,vision]`: `4.27.2`\n- `diffusers`: `0.14.0`\n- `accelerate`: `0.17.1`\n- `sentence_transformers`: `latest`\n- `pandas`: `latest`\n- `pytorch`: `1.13.1=py3.9_cuda11.8*`\n- `torchvision`: `0.14.1`\n- `tensorflow`: `2.9.1=*cuda112*py39*`"], "test_settings": "chunk_7000_embeddings_openai-text-embedding-3-small_reader-model_gpt-4", "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response correctly identifies that a `requirements.txt` file should be added to a model repository to install custom Python dependencies for Inference Endpoints. The answer is accurate and aligns with the provided criteria for a 5-point response. The additional reference \"[\ubb38\uc11c 0]\" does not detract from the correctness of the answer, but it is unnecessary for the context of this question. Overall, the response is precise and factual."}, {"question": "How many images are needed to teach new concepts to Stable Diffusion using Textual Inversion?\n", "true_answer": "3-5 images", "source_doc": "huggingface/diffusers/blob/main/examples/research_projects/intel_opts/textual_inversion_dfq/README.md", "generated_answer": "To teach new concepts to Stable Diffusion using Textual Inversion, you need just 3-5 sample images [2].", "retrieved_docs": ["!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Textual inversion\n\n[[open-in-colab]]\n\nThe [`StableDiffusionPipeline`] supports textual inversion, a technique that enables a model like Stable Diffusion to learn a new concept from just a few sample images. This gives you more control over the generated images and allows you to tailor the model towards specific concepts. You can get started quickly with a collection of community created concepts in the [Stable Diffusion Conceptualizer](https://huggingface.co/spaces/sd-concepts-library/stable-diffusion-conceptualizer).\n\nThis guide will show you how to run inference with textual inversion using a pre-learned concept from the Stable Diffusion Conceptualizer. If you're interested in teaching a model new concepts with textual inversion, take a look at the [Textual Inversion](../training/text_inversion) training guide.\n\nImport the necessary libraries:\n\n```py\nimport torch\nfrom diffusers import StableDiffusionPipeline\nfrom diffusers.utils import make_image_grid\n```\n\n## Stable Diffusion 1 and 2\n\nPick a Stable Diffusion checkpoint and a pre-learned concept from the [Stable Diffusion Conceptualizer](https://huggingface.co/spaces/sd-concepts-library/stable-diffusion-conceptualizer):\n\n```py\npretrained_model_name_or_path = \"runwayml/stable-diffusion-v1-5\"\nrepo_id_embeds = \"sd-concepts-library/cat-toy\"\n```\n\nNow you can load a pipeline, and pass the pre-learned concept to it:\n\n```py\npipeline = StableDiffusionPipeline.from_pretrained(\n    pretrained_model_name_or_path, torch_dtype=torch.float16, use_safetensors=True\n).to(\"cuda\")\n\npipeline.load_textual_inversion(repo_id_embeds)\n```\n\nCreate a prompt with the pre-learned concept by using the special placeholder token `<cat-toy>`, and choose the number of samples and rows of images you'd like to generate:\n\n```py\nprompt = \"a grafitti in a favela wall with a <cat-toy> on it\"\n\nnum_samples_per_row = 2\nnum_rows = 2\n```\n\nThen run the pipeline (feel free to adjust the parameters like `num_inference_steps` and `guidance_scale` to see how they affect image quality), save the generated images and visualize them with the helper function you created at the beginning:\n\n```py\nall_images = []\nfor _ in range(num_rows):\n    images = pipeline(prompt, num_images_per_prompt=num_samples_per_row, num_inference_steps=50, guidance_scale=7.5).images\n    all_images.extend(images)\n\ngrid = make_image_grid(all_images, num_rows, num_samples_per_row)\ngrid\n```\n\n<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/textual_inversion_inference.png\">\n</div>\n\n## Stable Diffusion XL\n\nStable Diffusion XL (SDXL) can also use textual inversion vectors for inference. In contrast to Stable Diffusion 1 and 2, SDXL has two text encoders so you'll need two textual inversion embeddings - one for each text encoder model.\n\nLet's download the SDXL textual inversion embeddings and have a closer look at it's structure:\n\n```py\nfrom huggingface_hub import hf_hub_download\nfrom safetensors.torch import load_file\n\nfile = hf_hub_download(\"dn118/unaestheticXL\", filename=\"unaestheticXLv31.safetensors\")\nstate_dict = load_file(file)\nstate_dict\n```\n\n```\n{'clip_g': tensor([[ 0.0077, -0.0112,  0.0065,  ...,  0.0195,  0.0159,  0.0275],\n         ...,\n         [-0.0170,  0.0213,  0.0143,  ..., -0.0302, -0.0240, -0.0362]],\n 'clip_l': tensor([[ 0.0023,  0.0192,  0.0213,  ..., -0.0385,  0.0048, -0.0011],\n         ...,\n         [ 0.0475, -0.0508, -0.0145,  ...,  0.0070, -0.0089, -0.0163]],\n```\n\nThere are two tensors, `\"clip_g\"` and `\"clip_l\"`.\n`\"clip_g\"` corresponds to the bigger text encoder in SDXL and refers to\n`pipe.text_encoder_2` and `\"clip_l\"` refers to `pipe.text_encoder`.\n\nNow you can load each tensor separately by passing them along with the correct text encoder and tokenizer\nto [`~loaders.TextualInversionLoaderMixin.load_textual_inversion`]:\n\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipe = AutoPipelineForText2Image.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", variant=\"fp16\", torch_dtype=torch.float16)\npipe.to(\"cuda\")\n\npipe.load_textual_inversion(state_dict[\"clip_g\"], token=\"unaestheticXLv31\", text_encoder=pipe.text_encoder_2, tokenizer=pipe.tokenizer_2)\npipe.load_textual_inversion(state_dict[\"clip_l\"], token=\"unaestheticXLv31\", text_encoder=pipe.text_encoder, tokenizer=pipe.tokenizer)\n\n# the embedding should be used as a negative embeddi...", "# Textual Inversion fine-tuning example\n\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples.\nThe `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\n\n## Training with Intel Extension for PyTorch\n\nIntel Extension for PyTorch provides the optimizations for faster training and inference on CPUs. You can leverage the training example \"textual_inversion.py\". Follow the [instructions](https://github.com/huggingface/diffusers/tree/main/examples/textual_inversion) to get the model and [dataset](https://huggingface.co/sd-concepts-library/dicoo2) before running the script.\n\nThe example supports both single node and multi-node distributed training:\n\n### Single node training\n\n```bash\nexport MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\nexport DATA_DIR=\"path-to-dir-containing-dicoo-images\"\n\npython textual_inversion.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\n  --train_data_dir=$DATA_DIR \\\n  --learnable_property=\"object\" \\\n  --placeholder_token=\"<dicoo>\" --initializer_token=\"toy\" \\\n  --seed=7 \\\n  --resolution=512 \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=1 \\\n  --max_train_steps=3000 \\\n  --learning_rate=2.5e-03 --scale_lr \\\n  --output_dir=\"textual_inversion_dicoo\"\n```\n\nNote: Bfloat16 is available on Intel Xeon Scalable Processors Cooper Lake or Sapphire Rapids. You may not get performance speedup without Bfloat16 support.\n\n### Multi-node distributed training\n\nBefore running the scripts, make sure to install the library's training dependencies successfully:\n\n```bash\npython -m pip install oneccl_bind_pt==1.13 -f https://developer.intel.com/ipex-whl-stable-cpu\n```\n\n```bash\nexport MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\nexport DATA_DIR=\"path-to-dir-containing-dicoo-images\"\n\noneccl_bindings_for_pytorch_path=$(python -c \"from oneccl_bindings_for_pytorch import cwd; print(cwd)\")\nsource $oneccl_bindings_for_pytorch_path/env/setvars.sh\n\npython -m intel_extension_for_pytorch.cpu.launch --distributed \\\n  --hostfile hostfile --nnodes 2 --nproc_per_node 2 textual_inversion.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\n  --train_data_dir=$DATA_DIR \\\n  --learnable_property=\"object\" \\\n  --placeholder_token=\"<dicoo>\" --initializer_token=\"toy\" \\\n  --seed=7 \\ \n  --resolution=512 \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=1 \\\n  --max_train_steps=750 \\\n  --learning_rate=2.5e-03 --scale_lr \\\n  --output_dir=\"textual_inversion_dicoo\"\n```\nThe above is a simple distributed training usage on 2 nodes with 2 processes on each node. Add the right hostname or ip address in the \"hostfile\" and make sure these 2 nodes are reachable from each other. For more details, please refer to the [user guide](https://github.com/intel/torch-ccl).\n\n\n### Reference\n\nWe publish a [Medium blog](https://medium.com/intel-analytics-software/personalized-stable-diffusion-with-few-shot-fine-tuning-on-a-single-cpu-f01a3316b13) on how to create your own Stable Diffusion model on CPUs using textual inversion. Try it out now, if you have interests.", "# Textual Inversion fine-tuning example\n\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples.\nThe `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\n\n## Running on Colab\n\nColab for training\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_textual_inversion_training.ipynb)\n\nColab for inference\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_conceptualizer_inference.ipynb)\n\n## Running locally with PyTorch\n### Installing the dependencies\n\nBefore running the scripts, make sure to install the library's training dependencies:\n\n**Important**\n\nTo make sure you can successfully run the latest versions of the example scripts, we highly recommend **installing from source** and keeping the install up to date as we update the example scripts frequently and install some example-specific requirements. To do this, execute the following steps in a new virtual environment:\n```bash\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install .\n```\n\nThen cd in the example folder  and run\n```bash\npip install -r requirements.txt\n```\n\nAnd initialize an [\ud83e\udd17Accelerate](https://github.com/huggingface/accelerate/) environment with:\n\n```bash\naccelerate config\n```\n\n\n### Cat toy example\n\nYou need to accept the model license before downloading or using the weights. In this example we'll use model version `v1-5`, so you'll need to visit [its card](https://huggingface.co/runwayml/stable-diffusion-v1-5), read the license and tick the checkbox if you agree.\n\nYou have to be a registered user in \ud83e\udd17 Hugging Face Hub, and you'll also need to use an access token for the code to work. For more information on access tokens, please refer to [this section of the documentation](https://huggingface.co/docs/hub/security-tokens).\n\nRun the following command to authenticate your token\n\n```bash\nhuggingface-cli login\n```\n\nIf you have already cloned the repo, then you won't need to go through these steps.\n\n<br>\n\nNow let's get our dataset. For this example we will use some cat images: https://huggingface.co/datasets/diffusers/cat_toy_example .\n\nLet's first download it locally:\n\n```py\nfrom huggingface_hub import snapshot_download\n\nlocal_dir = \"./cat\"\nsnapshot_download(\"diffusers/cat_toy_example\", local_dir=local_dir, repo_type=\"dataset\", ignore_patterns=\".gitattributes\")\n```\n\nThis will be our training data.\nNow we can launch the training using\n\n## Use ONNXRuntime to accelerate training\nIn order to leverage onnxruntime to accelerate training, please use textual_inversion.py\n\nThe command to train on custom data with onnxruntime:\n\n```bash\nexport MODEL_NAME=\"runwayml/stable-diffusion-v1-5\"\nexport DATA_DIR=\"path-to-dir-containing-images\"\n\naccelerate launch textual_inversion.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\n  --train_data_dir=$DATA_DIR \\\n  --learnable_property=\"object\" \\\n  --placeholder_token=\"<cat-toy>\" --initializer_token=\"toy\" \\\n  --resolution=512 \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=4 \\\n  --max_train_steps=3000 \\\n  --learning_rate=5.0e-04 --scale_lr \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --output_dir=\"textual_inversion_cat\"\n```\n\nPlease contact Prathik Rao (prathikr), Sunghoon Choi (hanbitmyths), Ashwini Khade (askhade), or Peng Wang (pengwa) on github with any questions."], "test_settings": "chunk_7000_embeddings_openai-text-embedding-3-small_reader-model_gpt-4", "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response accurately states the number of images needed, which is 3-5, aligning with the criteria answer. However, the inclusion of \"[2]\" is unnecessary and could be misleading. Overall, the response is mostly correct and factual."}, {"question": "What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n", "true_answer": "10GB", "source_doc": "huggingface/transformers/blob/main/docs/source/en/big_models.md", "generated_answer": "In Transformers version 4.18.0, model checkpoints that take more than 10GB of space are automatically sharded into smaller pieces [0].", "retrieved_docs": ["!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Instantiating a big model\n\nWhen you want to use a very big pretrained model, one challenge is to minimize the use of the RAM. The usual workflow\nfrom PyTorch is:\n\n1. Create your model with random weights.\n2. Load your pretrained weights.\n3. Put those pretrained weights in your random model.\n\nStep 1 and 2 both require a full version of the model in memory, which is not a problem in most cases, but if your model starts weighing several GigaBytes, those two copies can make you get out of RAM. Even worse, if you are using `torch.distributed` to launch a distributed training, each process will load the pretrained model and store these two copies in RAM.\n\n<Tip>\n\nNote that the randomly created model is initialized with \"empty\" tensors, which take the space in memory without filling it (thus the random values are whatever was in this chunk of memory at a given time). The random initialization following the appropriate distribution for the kind of model/parameters instantiated (like a normal distribution for instance) is only performed after step 3 on the non-initialized weights, to be as fast as possible! \n\n</Tip>\n\nIn this guide, we explore the solutions Transformers offer to deal with this issue. Note that this is an area of active development, so the APIs explained here may change slightly in the future.\n\n## Sharded checkpoints\n\nSince version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do `model.save_pretrained(save_dir)`, you will end up with several partial checkpoints (each of which being of size < 10GB) and an index that maps parameter names to the files they are stored in.\n\nYou can control the maximum size before sharding with the `max_shard_size` parameter, so for the sake of an example, we'll use a normal-size models with a small shard size: let's take a traditional BERT model.\n\n```py\nfrom transformers import AutoModel\n\nmodel = AutoModel.from_pretrained(\"bert-base-cased\")\n```\n\nIf you save it using [`~PreTrainedModel.save_pretrained`], you will get a new folder with two files: the config of the model and its weights:\n\n```py\n>>> import os\n>>> import tempfile\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir)\n...     print(sorted(os.listdir(tmp_dir)))\n['config.json', 'pytorch_model.bin']\n```\n\nNow let's use a maximum shard size of 200MB:\n\n```py\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     print(sorted(os.listdir(tmp_dir)))\n['config.json', 'pytorch_model-00001-of-00003.bin', 'pytorch_model-00002-of-00003.bin', 'pytorch_model-00003-of-00003.bin', 'pytorch_model.bin.index.json']\n```\n\nOn top of the configuration of the model, we see three different weights files, and an `index.json` file which is our index. A checkpoint like this can be fully reloaded using the [`~PreTrainedModel.from_pretrained`] method:\n\n```py\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     new_model = AutoModel.from_pretrained(tmp_dir)\n```\n\nThe main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in RAM to the model size plus the size of the biggest shard.\n\nBehind the scenes, the index file is used to determine which keys are in the checkpoint, and where the corresponding weights are stored. We can load that index like any json and get a dictionary:\n\n```py\n>>> import json\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     with open(os.path.join(tmp_dir, \"pytorch_model.bin.index.json\"), \"r\") as f:\n...         index = json.load(f)\n\n>>> print(index.keys())\ndict_keys(['metadata', 'weight_map'])\n```\n\nThe metadata just consists of the total size of the model for now. We plan to add other information in the future:\n\n```py\n>>> index[\"metadata\"]\n{'total_size': 433245184}\n```\n\nThe weights map is the main part of this index, which maps each parameter name (as usually found in a PyTorch model `state_dict`) to the f...", "```python\nfrom accelerate import infer_auto_device_map, init_empty_weights\nfrom transformers import AutoConfig, AutoModelForCausalLM\n\nconfig = AutoConfig.from_pretrained(\"facebook/opt-13b\")\nwith init_empty_weights():\n    model = AutoModelForCausalLM.from_config(config)\n\ndevice_map = infer_auto_device_map(model)\n```\n\nThis will return a dictionary mapping modules or weights to a device. On a machine with one Titan RTX for instance, we get the following:\n\n```python out\n{'model.decoder.embed_tokens': 0,\n 'model.decoder.embed_positions': 0,\n 'model.decoder.final_layer_norm': 0,\n 'model.decoder.layers.0': 0,\n 'model.decoder.layers.1': 0,\n ...\n 'model.decoder.layers.9': 0,\n 'model.decoder.layers.10.self_attn': 0,\n 'model.decoder.layers.10.activation_fn': 0,\n 'model.decoder.layers.10.self_attn_layer_norm': 0,\n 'model.decoder.layers.10.fc1': 'cpu',\n 'model.decoder.layers.10.fc2': 'cpu',\n 'model.decoder.layers.10.final_layer_norm': 'cpu',\n 'model.decoder.layers.11': 'cpu',\n ...\n 'model.decoder.layers.17': 'cpu',\n 'model.decoder.layers.18.self_attn': 'cpu',\n 'model.decoder.layers.18.activation_fn': 'cpu',\n 'model.decoder.layers.18.self_attn_layer_norm': 'cpu',\n 'model.decoder.layers.18.fc1': 'disk',\n 'model.decoder.layers.18.fc2': 'disk',\n 'model.decoder.layers.18.final_layer_norm': 'disk',\n 'model.decoder.layers.19': 'disk',\n ...\n 'model.decoder.layers.39': 'disk',\n 'lm_head': 'disk'}\n```\n\nAccelerate evaluated that the embeddings and the decoder up until the 9th block could all fit on the GPU (device 0), then part of the 10th block needs to be on the CPU, as well as the following weights until the 17th layer. Then the 18th layer is split between the CPU and the disk and the following layers must all be offloaded to disk\n\nActually using this device map later on won't work, because the layers composing this model have residual connections (where the input of the block is added to the output of the block) so all of a given layer should be on the same device. We can indicate this to Accelerate by passing a list of module names that shouldn't be split with the `no_split_module_classes` keyword argument:\n\n```python\ndevice_map = infer_auto_device_map(model, no_split_module_classes=[\"OPTDecoderLayer\"])\n```\n\nThis will then return\n\n```python out\n'model.decoder.embed_tokens': 0,\n 'model.decoder.embed_positions': 0,\n 'model.decoder.final_layer_norm': 0,\n 'model.decoder.layers.0': 0,\n 'model.decoder.layers.1': 0,\n ...\n 'model.decoder.layers.9': 0,\n 'model.decoder.layers.10': 'cpu',\n 'model.decoder.layers.11': 'cpu',\n ...\n 'model.decoder.layers.17': 'cpu',\n 'model.decoder.layers.18': 'disk',\n ...\n 'model.decoder.layers.39': 'disk',\n 'lm_head': 'disk'}\n\n\n```\n\nNow, each layer is always on the same device.\n\nIn Transformers, when using `device_map` in the `from_pretrained()` method or in a `pipeline`, those classes of blocks to leave on the same device are automatically provided, so you don't need to worry about them. Note that you have the following options for `device_map` (only relevant when you have more than one GPU):\n- `\"auto\"` or `\"balanced\"`: Accelerate will split the weights so that each GPU is used equally;\n- `\"balanced_low_0\"`: Accelerate will split the weights so that each GPU is used equally except the first one, where it will try to have as little weights as possible (useful when you want to work with the outputs of the model on one GPU, for instance when  using the `generate` function);\n- `\"sequential\"`: Accelerate will fill the GPUs in order (so the last ones might not be used at all).\n\nYou can also pass your own `device_map` as long as it follows the format we saw before (dictionary layer/module names to device).\n\nFinally, note that the results of the `device_map` you receive depend on the selected dtype (as different types of floats take a different amount of space). Providing `dtype=\"float16\"` will give us different results:\n\n```python\ndevice_map = infer_auto_device_map(model, no_split_module_classes=[\"OPTDecoderLayer\"], dtype=\"float16\")\n```\n\nIn this precision, we can fit the model up to layer 21 on the GPU:\n\n```python out\n\n\n{'model.decoder.embed_tokens': 0,\n 'model.decoder.embed_positions': 0,\n 'model.decoder.final_layer_norm': 0,\n 'model.decoder.layers.0': 0,\n 'model.decoder.layers.1': 0,\n ...\n 'model.decoder.layers.21': 0,\n 'model.decoder.layers.22': 'cpu',\n ...\n 'model.decoder.layers.37': 'cpu',\n 'model.decoder.layers.38': 'disk',\n 'model.decoder.layers.39': 'disk',\n 'lm_head': 'disk'}\n```\n\nNow that we know where each weight is supposed to go, we can progressively load the pretrained weights inside the model.\n\n## Sharding state dicts\n\nTraditionally, PyTorch models are saved in a whole file containing a map from parameter name to weight. This map is often called a `state_dict`. Here is an excerpt from the [PyTorch documentation](https://pytorch.org/tutorials/beginner/basics/saveloadrun_tutorial.html) on saving on loading:\n\n```python\n# Save the model weights\ntorch.save(my_model.state_dict(), 'model_weights.pth')\n\n...", "We call the checkpoints saved in several files like BLOOM *sharded checkpoints*, and we have standardized their format as such: \n- One file (called `pytorch_model.bin.index.json`) contains some metadata and a map parameter name to file name, indicating where to find each weight\n- All the other files are standard PyTorch state dicts, they just contain a part of the model instead of the whole one. You can have a look at the content of the index file [here](https://huggingface.co/bigscience/bloom/blob/main/pytorch_model.bin.index.json).\n\nTo load such a sharded checkpoint into a model, we just need to loop over the various shards. Accelerate provides a function called `load_checkpoint_in_model` that will do this for you if you have cloned one of the repos of the Hub, or you can directly use the `from_pretrained` method of Transformers, which will handle the downloading and caching for you:\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM\n\n# Will error\ncheckpoint = \"facebook/opt-13b\"\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\", torch_dtype=torch.float16)\n```\n\nIf the device map computed automatically requires some weights to be offloaded on disk because you don't have enough GPU and CPU RAM, you will get an error indicating you need to pass an folder where the weights that should be stored on disk will be offloaded:\n\n```python out\nValueError: The current `device_map` had weights offloaded to the disk. Please provide an \n`offload_folder` for them.\n```\n\nAdding this argument should resolve the error:\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM\n\n# Will go out of RAM on Colab\ncheckpoint = \"facebook/opt-13b\"\nmodel = AutoModelForCausalLM.from_pretrained(\n    checkpoint, device_map=\"auto\", offload_folder=\"offload\", torch_dtype=torch.float16\n)\n```\n\nNote that if you are trying to load a very large model that require some disk offload on top of CPU offload, you might run out of RAM when the last shards of the checkpoint are loaded, since there is the part of the model staying on CPU taking space. If that is the case, use the option `offload_state_dict=True` to temporarily offload the part of the model staying on CPU while the weights are all loaded, and reload it in RAM once all the weights have been processed\n\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM\n\ncheckpoint = \"facebook/opt-13b\"\nmodel = AutoModelForCausalLM.from_pretrained(\n    checkpoint, device_map=\"auto\", offload_folder=\"offload\", offload_state_dict = True, torch_dtype=torch.float16\n)\n```\n\nThis will fit in Colab, but will be so close to using all the RAM available that it will go out of RAM when you try to generate a prediction. To get a model we can use, we need to offload one more layer on the disk. We can do so by taking the `device_map` computed in the previous section, adapting it a bit, then passing it to the `from_pretrained` call:\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM\n\ncheckpoint = \"facebook/opt-13b\"\ndevice_map[\"model.decoder.layers.37\"] = \"disk\"\nmodel = AutoModelForCausalLM.from_pretrained(\n    checkpoint, device_map=device_map, offload_folder=\"offload\", offload_state_dict = True, torch_dtype=torch.float16\n)\n```\n\n## Running a model split on several devices\n\nOne last part we haven't touched is how Accelerate enables your model to run with its weight spread across several GPUs, CPU RAM, and the disk folder. This is done very simply using hooks. \n\n> [hooks](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_forward_hook) are a PyTorch API that adds functions executed just before each forward called \n\nWe couldn't use this directly since they only support models with regular arguments and no keyword arguments in their forward pass, but we took the same idea. Once the model is loaded, the `dispatch_model` function will add hooks to every module and submodule that are executed before and after each forward pass. They will:\n- make sure all the inputs of the module are on the same device as the weights;\n- if the weights have been offloaded to the CPU, move them to GPU 0 before the forward pass and back to the CPU just after;\n- if the weights have been offloaded to disk, load them in RAM then on the GPU 0 before the forward pass and free this memory just after.\n\nThe whole process is summarized in the following video:\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/MWCSGj9jEAo\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n\nThis way, your model can be loaded and run even if you don't have enough GPU RAM and CPU RAM. The only thing you need is disk space (and lots of patience!) While this solution is pretty naive if you have multiple GPUs (there is no clever pipeline parallelism involved, just using the GPUs sequentially) it still yields [pretty decent res..."], "test_settings": "chunk_7000_embeddings_openai-text-embedding-3-small_reader-model_gpt-4", "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response accurately states that in Transformers version 4.18.0, model checkpoints larger than 10GB are automatically sharded. The inclusion of \"[0]\" seems to indicate a reference or citation, which is not necessary in this context and slightly detracts from the clarity. However, the core information provided is correct and aligns with the 5-point criteria for being entirely correct, accurate, and factual."}, {"question": "What is the purpose of Weights and Biases (W&B) for data scientists and machine learning scientists?\n", "true_answer": "To track their machine learning experiments at every stage, from training to production.", "source_doc": "gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/Gradio-and-Wandb-Integration.md", "generated_answer": "\ubb38\ub9e5\uc5d0\uc11c Weights and Biases (W&B)\uc5d0 \ub300\ud55c \uc815\ubcf4\ub294 \uc81c\uacf5\ub418\uc9c0 \uc54a\uc558\uc2b5\ub2c8\ub2e4. \ub530\ub77c\uc11c \uc9c8\ubb38\uc5d0 \ub300\ud55c \ub2f5\ubcc0\uc744 \ub4dc\ub9b4 \uc218 \uc5c6\uc2b5\ub2c8\ub2e4.", "retrieved_docs": ["### Inference speed and memory footprint\n\nWhile recent years have seen increasingly large ML models achieve high performance on a large variety of tasks and benchmarks, deploying these multi-billion parameter models in practice can be a challenge in itself, and many organizations lack the resources for this. This is why considering the **inference speed** and **memory footprint** of models is important, especially when doing online model evaluation.\n\nInference speed refers to the time that it takes for a model to make a prediction -- this will vary depending on the hardware used and the way in which models are queried, e.g. in real time via an API or in batch jobs that run once a day.\n\nMemory footprint refers to the size of the model weights and how much hardware memory they occupy. If a model is too large to fit on a single GPU or CPU, then it has to be split over multiple ones, which can be more or less difficult depending on the model architecture and the deployment method.\n\nWhen doing online model evaluation, there is often a trade-off to be done between inference speed and accuracy or precision, whereas this is less the case for offline evaluation.\n\n## Limitations and bias\n\nAll models and all metrics have their limitations and biases, which depend on the way in which they were trained, the data that was used, and their intended uses. It is important to measure and communicate these limitations clearly to prevent misuse and unintended impacts, for instance via [model cards](https://huggingface.co/course/chapter4/4?fw=pt) which document the training and evaluation process.\n\nMeasuring biases can be done by evaluating models on datasets such as [Wino Bias](https://huggingface.co/datasets/wino_bias) or [MD Gender Bias](https://huggingface.co/datasets/md_gender_bias), and by doing [Interactive Error Analyis](https://huggingface.co/spaces/nazneen/error-analysis) to try to identify which subsets of the evaluation dataset a model performs poorly on.\n\nWe are currently working on additional measurements that can be used to quantify different dimensions of bias in both models and datasets -- stay tuned for more documentation on this topic!", "Conceptualizing bias as a risk factor then allows us to better understand the **shared responsibility** for machine biases between developers at all stages. Bias can never be fully removed, not least because the definitions of social biases and the power dynamics that tie them to discrimination vary vastly across social contexts. However:\n\n\n\n1. Each stage of the development process, from task specification, dataset curation, and model training, to model integration and system deployment, can take steps to minimize the aspects of machine bias** that most directly depend on its choices** and technical decisions, and\n2. Clear communication and **information flow between the various ML development stages** can make the difference between making choices that build on top of each other to attenuate the negative potential of bias (multipronged approach to bias mitigation, as in deployment scenario 1 above) _versus_ making choices that compound this negative potential to exacerbate the risk of harm (as in deployment scenario 3).\n\nIn the next section, we review these various stages along with some of the tools that can help us address machine bias at each of them.\n\n\n## Addressing Bias throughout the ML Development Cycle\n\nReady for some practical advice yet? Here we go \ud83e\udd17\n\nThere is no one single way to develop ML systems; which steps happen in what order depends on a number of factors including the development setting (university, large company, startup, grassroots organization, etc\u2026), the modality (text, tabular data, images, etc\u2026), and the preeminence or scarcity of publicly available ML resources. However, we can identify three common stages of particular interest in addressing bias. These are the task definition, the data curation, and the model training. Let\u2019s have a look at how bias handling may differ across these various stages.\n\n<p align=\"center\">\n <br>\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/ethics_soc_2/img_pipeline.png\" alt=\"The Bias ML Pipeline by Meg\" width=\"500\" />\n    <em>The Bias ML Pipeline by <a href=\"https://huggingface.co/meg\">Meg</a></em>\n</p>\n\n### I am <span style=\"text-decoration:underline;\">defining the task</span> of my ML system, how can I address bias?\n\nWhether and to what extent bias in the system concretely affects people ultimately depends on what the system is used for. As such, the first place developers can work to mitigate bias is when deciding how ML fits in their system, e.g., by deciding what optimization objective it will use.\n\nFor example, let\u2019s go back to one of the first highly-publicized cases of a Machine Learning system used in production for algorithmic content recommendation. From 2006 to 2009, Netflix ran the [Netflix Prize](https://www.cs.uic.edu/~liub/KDD-cup-2007/proceedings/The-Netflix-Prize-Bennett.pdf), a competition with a 1M$ cash prize challenging teams around the world to develop ML systems to accurately predict a user\u2019s rating for a new movie based on their past ratings. The [winning submission](https://www.asc.ohio-state.edu/statistics/dmsl/GrandPrize2009_BPC_BigChaos.pdf) improved the RMSE (Root-mean-square-error) of predictions on unseen user-movie pairs by over 10% over Netflix\u2019s own CineMatch algorithm, meaning it got much better at predicting how users would rate a new movie based on their history. This approach opened the door for much of modern algorithmic content recommendation by bringing the role of ML in modeling user preferences in recommender systems to public awareness.\n\nSo what does this have to do with bias? Doesn\u2019t showing people content that they\u2019re likely to enjoy sound like a good service from a content platform? Well, it turns out that showing people more examples of **what they\u2019ve liked in the past** ends up [reducing the diversity of the media they consume](https://dl.acm.org/doi/10.1145/3391403.3399532). Not only does it lead users to be [less satisfied in the long term](https://dl.acm.org/doi/abs/10.1145/3366423.3380281), but it also means that any biases or stereotypes captured by the initial models \u2014 such as when modeling [the preferences of Black American users](https://www.marieclaire.com/culture/a18817/netflix-algorithms-black-movies/) or [dynamics that systematically disadvantage](https://dl.acm.org/doi/10.1145/3269206.3272027) some artists \u2014 are likely to be reinforced if the model is [further trained on ongoing ML-mediated](https://arxiv.org/abs/2209.03942) user interactions. This reflects two of the types of bias-related concerns we\u2019ve mentioned above: the training objective acts as a **risk factor** for bias-related harms as it makes pre-existing biases much more likely to show up in predictions, and the task framing has the effect of **locking in** and exacerbating past biases.\n\nA promising bias mitigation strategy at this stage has been to reframe the task to explicitly [model both engagement and diversity](https://dl.acm.org/doi/10.1145/3437963.3441775) when applying M...", "<p align=\"center\">\n <br>\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/ethics_soc_2/img8.png\" alt=\"Systematic Error Analysis and Labeling (SEAL) by Nazneen\" />\n    <em><a href=\"https://huggingface.co/spaces/nazneen/seal\">Systematic Error Analysis and Labeling (SEAL)</a> by <a href=\"https://huggingface.co/nazneen\">Nazneen</a></em>\n</p>\n\nFinally, a few benchmarks exist that can measure bias-related phenomena in models. For language models, benchmarks such as [BOLD](https://github.com/amazon-science/bold), [HONEST](https://aclanthology.org/2021.naacl-main.191.pdf), or [WinoBias](https://aclanthology.org/N18-2003/) provide quantitative evaluations of targeted behaviors that are indicative of biases in the models. While the benchmarks have their [limitations](https://aclanthology.org/2021.acl-long.81/), they do provide a limited view into some pre-identified bias risks that can help describe how the models function or choose between different models. You can find these evaluations pre-computed on a range of common language models [in this exploration Space](https://huggingface.co/spaces/sasha/BiasDetection) to get a first sense of how they compare!\n\n\n<p align=\"center\">\n <br>\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/ethics_soc_2/img9.png\" alt=\"Language Model Bias Detection by Sasha\" />\n    <em><a href=\"https://huggingface.co/spaces/sasha/BiasDetection\">Language Model Bias Detection</a> by <a href=\"https://huggingface.co/sasha\">Sasha</a></em>\n</p>\n\nEven with access to a benchmark for the models you are considering, you might find that running evaluations of the larger language models you are considering can be prohibitively expensive or otherwise technically impossible with your own computing resources. The <a href=\"https://huggingface.co/spaces/autoevaluate/model-evaluator\">Evaluation on the Hub</a> tool we released this year can help with that: not only will it run the evaluations for you, but it will also help connect them to the model documentation so the results are available once and for all \u2014 so everyone can see, for example, that size <a href=\"https://huggingface.co/blog/zero-shot-eval-on-the-hub\">measurably increases bias risks in models like OPT</a>!\n\n<p align=\"center\">\n <br>\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/ethics_soc_2/img_winobias.png\" alt=\"Large model WinoBias scores computed with Evaluation on the Hub by Helen, Tristan, Abhishek, Lewis, and Douwe\" />\n    <em><a href=\"https://huggingface.co/spaces/sasha/BiasDetection\"><a href=\"https://huggingface.co/blog/zero-shot-eval-on-the-hub\">Large model WinoBias scores computed with Evaluation on the Hub</a> by <a href=\"https://huggingface.co/mathemakitten\">Helen</a>, <a href=\"https://huggingface.co/Tristan\">Tristan</a>, <a href=\"https://huggingface.co/abhishek\">Abhishek</a>, <a href=\"https://huggingface.co/lewtun\">Lewis</a>, and <a href=\"https://huggingface.co/douwekiela\">Douwe</a></em>\n</p>\n\n\n#### Model selection/development: recommendations\n\nFor models just as for datasets, different tools for documentation and evaluation will provide different views of bias risks in a model which all have a part to play in helping developers choose, develop, or understand ML systems.\n\n* Visualize\n    * Generative model: visualize how the model\u2019s outputs may reflect stereotypes\n    * Classification model: visualize model errors to identify failure modes that could lead to disparate performance\n* Evaluate\n    * When possible, evaluate models on relevant benchmarks\n* Document\n    * Share your learnings from visualization and qualitative evaluation\n    * Report your model\u2019s disaggregated performance and results on applicable fairness benchmarks\n\n\n## Conclusion and Overview of Bias Analysis and Documentation Tools from \ud83e\udd17\n\nAs we learn to leverage ML systems in more and more applications, reaping their benefits equitably will depend on our ability to actively mitigate the risks of bias-related harms associated with the technology. While there is no single answer to the question of how this should best be done in any possible setting, we can support each other in this effort by sharing lessons, tools, and methodologies to mitigate and document those risks. The present blog post outlines some of the ways Hugging Face team members have addressed this question of bias along with supporting tools, we hope that you will find them helpful and encourage you to develop and share your own!\n\nSummary of linked tools:\n* Tasks:\n    * Explore our directory of [ML Tasks](https://huggingface.co/tasks) to understand what technical framings and resources are available to choose from\n    * Use tools to explore the [full development lifecycle](https://huggingface.co/spaces/hf-task-exploration/ExploreACMnaacl) of specific tasks\n* Datasets:\n    * Make use of and contribute to [Dataset Cards](https://github.com/huggingface/datasets/blob/main/tem..."], "test_settings": "chunk_7000_embeddings_openai-text-embedding-3-small_reader-model_gpt-4", "eval_score_GPT4": "1", "eval_feedback_GPT4": "The response does not provide any information about the purpose of Weights and Biases (W&B) for data scientists and machine learning scientists. It simply states that there is no context provided, which is not a valid reason for not answering the question. The question is asking for a general purpose of W&B, which can be answered without additional context. Therefore, the response is entirely incorrect and does not meet any of the criteria for accuracy or factuality."}, {"question": "What is the name of the open-source library created by Hugging Face to simplify Transformer acceleration?\n", "true_answer": "Optimum", "source_doc": "huggingface/blog/blob/main/intel.md", "generated_answer": "The open-source library created by Hugging Face to simplify Transformer acceleration is called Optimum [1].", "retrieved_docs": ["The Hugging Face Course\n\nThis repo contains the content that's used to create the **[Hugging Face course](https://huggingface.co/course/chapter1/1)**. The course teaches you about applying Transformers to various tasks in natural language processing and beyond. Along the way, you'll learn how to use the [Hugging Face](https://huggingface.co/) ecosystem \u2014 [\ud83e\udd17 Transformers](https://github.com/huggingface/transformers), [\ud83e\udd17 Datasets](https://github.com/huggingface/datasets), [\ud83e\udd17 Tokenizers](https://github.com/huggingface/tokenizers), and [\ud83e\udd17 Accelerate](https://github.com/huggingface/accelerate) \u2014 as well as the [Hugging Face Hub](https://huggingface.co/models). It's completely free and open-source!\n\n## \ud83c\udf0e Languages and translations", "--\ntitle: \"Habana Labs and Hugging Face Partner to Accelerate Transformer Model Training\"\nthumbnail: /blog/assets/60_habana/habana.png\nauthors:\n- user: susanlansing\n  guest: true\n---\n\n# Habana Labs and Hugging Face Partner to Accelerate Transformer Model Training\n\n\n*Santa Clara and San Francisco, CA, April 12th, 2022*\n\nPowered by deep learning, transformer models deliver state-of-the-art performance on a wide range of machine learning tasks, such as natural language processing, computer vision, speech, and more. However, training them at scale often requires a large amount of computing power, making the whole process unnecessarily long, complex, and costly.\n\nToday, [Habana\u00ae Labs](https://habana.ai/), a pioneer in high-efficiency, purpose-built deep learning processors, and Hugging Face, the home of [Transformer](https://github.com/huggingface/transformers) models, are happy to announce that they\u2019re joining forces to make it easier and quicker to train high-quality transformer models. Thanks to the integration of Habana\u2019s [SynapseAI software suite](https://habana.ai/training-software/) with the Hugging Face [Optimum open-source library](https://github.com/huggingface/optimum), data scientists and machine learning engineers can now accelerate their Transformer training jobs on Habana processors with just a few lines of code and enjoy greater productivity as well as lower training cost.\n\n[Habana Gaudi](https://habana.ai/training/) training solutions, which power Amazon\u2019s EC2 DL1 instances and Supermicro\u2019s X12 Gaudi AI Training Server, deliver price/performance up to 40% lower than comparable training solutions and enable customers to train more while spending less. The integration of ten 100 Gigabit Ethernet ports onto every Gaudi processor enables system scaling from 1 to thousands of Gaudis with ease and cost-efficiency. Habana\u2019s SynapseAI\u00ae is optimized\u2014at inception\u2014to enable Gaudi performance and usability, supports TensorFlow and PyTorch frameworks, with a focus on computer vision and natural language processing applications.  \n\nWith 60,000+ stars on Github, 30,000+ models, and millions of monthly visits, Hugging Face is one of the fastest-growing projects in open source software history, and the go-to place for the machine learning community. \n\nWith its [Hardware Partner Program](https://huggingface.co/hardware), Hugging Face provides Gaudi\u2019s advanced deep learning hardware with the ultimate Transformer toolset. This partnership will enable rapid expansion of the Habana Gaudi training transformer model library, bringing Gaudi efficiency and ease of use to a wide array of customer use cases like natural language processing, computer vision, speech, and more. \n\n\u201c*We\u2019re excited to partner with Hugging Face and its many open-source developers to address the growing demand for transformer models that benefit from the efficiency, usability, and scalability of the Gaudi training platform*\u201d, said Sree Ganesan, head of software product management, Habana Labs. \n\n\u201cHabana Gaudi brings a new level of efficiency to deep learning model training, and we\u2019re super excited to make this performance easily accessible to Transformer users with minimal code changes through Optimum\u201d, said Jeff Boudier, product director at Hugging Face.\n\nTo learn how to get started training with Habana Gaudi, please visit [https://developer.habana.ai](https://developer.habana.ai). \n\nFor more info on the Hugging Face and Habana Gaudi collaboration, please visit [https://huggingface.co/Habana](https://huggingface.co/Habana).", "!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n\n<p align=\"center\">\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-dark.svg\">\n    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg\">\n    <img alt=\"Hugging Face Transformers Library\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg\" width=\"352\" height=\"59\" style=\"max-width: 100%;\">\n  </picture>\n  <br/>\n  <br/>\n</p>\n\n<p align=\"center\">\n    <a href=\"https://circleci.com/gh/huggingface/transformers\">\n        <img alt=\"Build\" src=\"https://img.shields.io/circleci/build/github/huggingface/transformers/main\">\n    </a>\n    <a href=\"https://github.com/huggingface/transformers/blob/main/LICENSE\">\n        <img alt=\"GitHub\" src=\"https://img.shields.io/github/license/huggingface/transformers.svg?color=blue\">\n    </a>\n    <a href=\"https://huggingface.co/docs/transformers/index\">\n        <img alt=\"Documentation\" src=\"https://img.shields.io/website/http/huggingface.co/docs/transformers/index.svg?down_color=red&down_message=offline&up_message=online\">\n    </a>\n    <a href=\"https://github.com/huggingface/transformers/releases\">\n        <img alt=\"GitHub release\" src=\"https://img.shields.io/github/release/huggingface/transformers.svg\">\n    </a>\n    <a href=\"https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md\">\n        <img alt=\"Contributor Covenant\" src=\"https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg\">\n    </a>\n    <a href=\"https://zenodo.org/badge/latestdoi/155220641\"><img src=\"https://zenodo.org/badge/155220641.svg\" alt=\"DOI\"></a>\n</p>\n\n<h4 align=\"center\">\n    <p>\n        <b>English</b> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/README_zh-hans.md\">\u7b80\u4f53\u4e2d\u6587</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/README_zh-hant.md\">\u7e41\u9ad4\u4e2d\u6587</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/README_ko.md\">\ud55c\uad6d\uc5b4</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/README_es.md\">Espa\u00f1ol</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/README_ja.md\">\u65e5\u672c\u8a9e</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/README_hd.md\">\u0939\u093f\u0928\u094d\u0926\u0940</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/README_ru.md\">\u0420\u0443\u0441\u0441\u043a\u0438\u0439</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/README_pt-br.md\">\u0420ortugu\u00eas</a> |\n        <a href=\"https://github.com/huggingface/transformers//blob/main/README_te.md\">\u0c24\u0c46\u0c32\u0c41\u0c17\u0c41</a> |\n    </p>\n</h4>\n\n<h3 align=\"center\">\n    <p>State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow</p>\n</h3>\n\n<h3 align=\"center\">\n    <a href=\"https://hf.co/course\"><img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/course_banner.png\"></a>\n</h3>\n\n\ud83e\udd17 Transformers provides thousands of pretrained models to perform tasks on different modalities such as text, vision, and audio.\n\nThese models can be applied on:\n\n* \ud83d\udcdd Text, for tasks like text classification, information extraction, question answering, summarization, translation, and text generation, in over 100 languages.\n* \ud83d\uddbc\ufe0f Images, for tasks like image classification, object detection, and segmentation.\n* \ud83d\udde3\ufe0f Audio, for tasks like speech recognition and audio classification.\n\nTransformer models can also perform tasks on **several modalities combined**, such as table question answering, optical character recognition, information extraction from scanned documents, video classification, and visual question answering.\n\n\ud83e\udd17 Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and then share them with the community on our [model hub](https://huggingface.co/models). At the same time, each python module defining an architecture is fully standalone and can be modified to enable quick research experiments.\n\n\ud83e\udd17 Transformers is backed by the three most popular deep learning libraries \u2014 [Jax](https://jax.readthedocs.io/en/latest/), [PyTorch](https://pytorch.org/) and [TensorFlow](https://www.tensorflow.org/) \u2014 with a seamless integration between them. It's straightforward to train your models with one before l..."], "test_settings": "chunk_7000_embeddings_openai-text-embedding-3-small_reader-model_gpt-4", "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response correctly identifies \"Optimum\" as the open-source library created by Hugging Face to simplify Transformer acceleration. The additional reference \"[1]\" is unnecessary and does not detract from the accuracy of the answer. The response is factually correct and aligns with the criteria for a perfect score."}, {"question": "What parameter is used to ensure that elements in a row have the same height in Gradio?\n", "true_answer": "equal_height", "source_doc": "gradio-app/gradio/blob/main/guides/cn/03_building-with-blocks/02_controlling-layout.md", "generated_answer": "The context provided does not contain information regarding a parameter used to ensure that elements in a row have the same height in Gradio.", "retrieved_docs": ["`@gradio/theme`\n\ncss for gradio", "`@gradio/form`\n\n```html\n<script>\n\timport { Form } from \"@gradio/form\";\n</script>\n```\n\nForm\n```javascript\n\texport let visible = true;\n\texport let scale: number | null = null;\n\texport let min_width = 0;\n```", "`@gradio/imageeditor`"], "test_settings": "chunk_7000_embeddings_openai-text-embedding-3-small_reader-model_gpt-4", "eval_score_GPT4": "1", "eval_feedback_GPT4": "The response does not provide the correct parameter used in Gradio to ensure that elements in a row have the same height. Instead, it states that the context does not contain the information, which is not a direct answer to the question. The correct parameter is \"equal_height,\" and the response fails to mention this. Therefore, the response is inaccurate and does not meet the criteria for a correct answer."}, {"question": "What is the command to install the latest version of Optimum with OpenVINO support?\n", "true_answer": "pip install --upgrade-strategy eager optimum[\"openvino\"]", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/optimization/open_vino.md", "generated_answer": "The command to install the latest version of Optimum with OpenVINO support is:\n\n```bash\npip install --upgrade-strategy eager optimum[openvino]\n``` \n\n[\ucd9c\ucc98 \ubb38\uc11c 1]", "retrieved_docs": ["!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Installation\n\n\ud83e\udd17 Optimum can be installed using `pip` as follows:\n\n```bash\npython -m pip install optimum\n```\n\nIf you'd like to use the accelerator-specific features of \ud83e\udd17 Optimum, you can install the required dependencies according to the table below:\n\n| Accelerator                                                                                                            | Installation                                      |\n|:-----------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------|\n| [ONNX runtime](https://onnxruntime.ai/docs/)                                                                           | `pip install --upgrade-strategy eager install optimum[onnxruntime]`|\n| [Intel Neural Compressor (INC)](https://www.intel.com/content/www/us/en/developer/tools/oneapi/neural-compressor.html) | `pip install --upgrade-strategy eager optimum[neural-compressor]`  |\n| [Intel OpenVINO](https://docs.openvino.ai/latest/index.html)                                                           | `pip install --upgrade-strategy eager optimum[openvino,nncf]`      |\n| [AMD Instinct GPUs and Ryzen AI NPU](https://www.amd.com/en/graphics/instinct-server-accelerators)                                      | `pip install --upgrade-strategy eager optimum[amd]`                |\n| [Habana Gaudi Processor (HPU)](https://habana.ai/training/)                                                            | `pip install --upgrade-strategy eager optimum[habana]`             |\n| [FuriosaAI](https://www.furiosa.ai/)                                                                                   | `pip install --upgrade-strategy eager optimum[furiosa]`            |\n\nThe `--upgrade-strategy eager` option is needed to ensure the different packages are upgraded to the latest possible version.\n\nIf you'd like to play with the examples or need the bleeding edge of the code and can't wait for a new release, you can install the base library from source as follows:\n\n```bash\npython -m pip install git+https://github.com/huggingface/optimum.git\n```\n\nFor the accelerator-specific features, you can install them by appending `optimum[accelerator_type]` to the `pip` command, e.g.\n\n```bash\npython -m pip install optimum[onnxruntime]@git+https://github.com/huggingface/optimum.git\n```", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# OpenVINO\n\n\ud83e\udd17 [Optimum](https://github.com/huggingface/optimum-intel) provides Stable Diffusion pipelines compatible with OpenVINO to perform inference on a variety of Intel processors (see the [full list](https://docs.openvino.ai/latest/openvino_docs_OV_UG_supported_plugins_Supported_Devices.html) of supported devices).\n\nYou'll need to install \ud83e\udd17 Optimum Intel with the `--upgrade-strategy eager` option to ensure [`optimum-intel`](https://github.com/huggingface/optimum-intel) is using the latest version:\n\n```bash\npip install --upgrade-strategy eager optimum[\"openvino\"]\n```\n\nThis guide will show you how to use the Stable Diffusion and Stable Diffusion XL (SDXL) pipelines with OpenVINO.\n\n## Stable Diffusion\n\nTo load and run inference, use the [`~optimum.intel.OVStableDiffusionPipeline`]. If you want to load a PyTorch model and convert it to the OpenVINO format on-the-fly, set `export=True`:\n\n```python\nfrom optimum.intel import OVStableDiffusionPipeline\n\nmodel_id = \"runwayml/stable-diffusion-v1-5\"\npipeline = OVStableDiffusionPipeline.from_pretrained(model_id, export=True)\nprompt = \"sailing ship in storm by Rembrandt\"\nimage = pipeline(prompt).images[0]\n\n# Don't forget to save the exported model\npipeline.save_pretrained(\"openvino-sd-v1-5\")\n```\n\nTo further speed-up inference, statically reshape the model. If you change any parameters such as the outputs height or width, you\u2019ll need to statically reshape your model again.\n\n```python\n# Define the shapes related to the inputs and desired outputs\nbatch_size, num_images, height, width = 1, 1, 512, 512\n\n# Statically reshape the model\npipeline.reshape(batch_size, height, width, num_images)\n# Compile the model before inference\npipeline.compile()\n\nimage = pipeline(\n    prompt,\n    height=height,\n    width=width,\n    num_images_per_prompt=num_images,\n).images[0]\n```\n<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/optimum/documentation-images/resolve/main/intel/openvino/stable_diffusion_v1_5_sail_boat_rembrandt.png\">\n</div>\n\nYou can find more examples in the \ud83e\udd17 Optimum [documentation](https://huggingface.co/docs/optimum/intel/inference#stable-diffusion), and Stable Diffusion is supported for text-to-image, image-to-image, and inpainting.\n\n## Stable Diffusion XL\n\nTo load and run inference with SDXL, use the [`~optimum.intel.OVStableDiffusionXLPipeline`]:\n\n```python\nfrom optimum.intel import OVStableDiffusionXLPipeline\n\nmodel_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\npipeline = OVStableDiffusionXLPipeline.from_pretrained(model_id)\nprompt = \"sailing ship in storm by Rembrandt\"\nimage = pipeline(prompt).images[0]\n```\n\nTo further speed-up inference, [statically reshape](#stable-diffusion) the model as shown in the Stable Diffusion section.\n\nYou can find more examples in the \ud83e\udd17 Optimum [documentation](https://huggingface.co/docs/optimum/intel/inference#stable-diffusion-xl), and running SDXL in OpenVINO is supported for text-to-image and image-to-image.", "![ONNX Runtime](https://github.com/huggingface/optimum/actions/workflows/test_onnxruntime.yml/badge.svg)](https://github.com/huggingface/optimum/actions/workflows/test_onnxruntime.yml)\n\n# Hugging Face Optimum\n\n\ud83e\udd17 Optimum is an extension of \ud83e\udd17 Transformers and Diffusers, providing a set of optimization tools enabling maximum efficiency to train and run models on targeted hardware, while keeping things easy to use.\n\n## Installation\n\n\ud83e\udd17 Optimum can be installed using `pip` as follows:\n\n```bash\npython -m pip install optimum\n```\n\nIf you'd like to use the accelerator-specific features of \ud83e\udd17 Optimum, you can install the required dependencies according to the table below:\n\n| Accelerator                                                                                                            | Installation                                      |\n|:-----------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------|\n| [ONNX Runtime](https://huggingface.co/docs/optimum/onnxruntime/overview)                                                                           | `pip install --upgrade-strategy eager optimum[onnxruntime]`       |\n| [Intel Neural Compressor](https://huggingface.co/docs/optimum/intel/index)       | `pip install --upgrade-strategy eager optimum[neural-compressor]`|\n| [OpenVINO](https://huggingface.co/docs/optimum/intel/index)                                                                 | `pip install --upgrade-strategy eager optimum[openvino,nncf]`    |\n| [AMD Instinct GPUs and Ryzen AI NPU](https://huggingface.co/docs/optimum/amd/index)                     | `pip install --upgrade-strategy eager optimum[amd]`              |\n| [Habana Gaudi Processor (HPU)](https://huggingface.co/docs/optimum/habana/index)                                                            | `pip install --upgrade-strategy eager optimum[habana]`           |\n| [FuriosaAI](https://huggingface.co/docs/optimum/furiosa/index)                                                                                   | `pip install --upgrade-strategy eager optimum[furiosa]`          |\n\nThe `--upgrade-strategy eager` option is needed to ensure the different packages are upgraded to the latest possible version.\n\nTo install from source:\n\n```bash\npython -m pip install git+https://github.com/huggingface/optimum.git\n```\n\nFor the accelerator-specific features, append `optimum[accelerator_type]` to the above command:\n\n```bash\npython -m pip install optimum[onnxruntime]@git+https://github.com/huggingface/optimum.git\n```\n\n## Accelerated Inference\n\n\ud83e\udd17 Optimum provides multiple tools to export and run optimized models on various ecosystems: \n\n- [ONNX](https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/export_a_model) / [ONNX Runtime](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/models)\n- TensorFlow Lite\n- [OpenVINO](https://huggingface.co/docs/optimum/intel/inference)\n- Habana first-gen Gaudi / Gaudi2, more details [here](https://huggingface.co/docs/optimum/main/en/habana/usage_guides/accelerate_inference)\n\nThe [export](https://huggingface.co/docs/optimum/exporters/overview) and optimizations can be done both programmatically and with a command line.\n\n### Features summary\n\n| Features                           | [ONNX Runtime](https://huggingface.co/docs/optimum/main/en/onnxruntime/overview)| [Neural Compressor](https://huggingface.co/docs/optimum/main/en/intel/optimization_inc)| [OpenVINO](https://huggingface.co/docs/optimum/main/en/intel/inference)| [TensorFlow Lite](https://huggingface.co/docs/optimum/main/en/exporters/tflite/overview)|\n|:----------------------------------:|:------------------:|:------------------:|:------------------:|:------------------:|\n| Graph optimization                 | :heavy_check_mark: | N/A                | :heavy_check_mark: | N/A                |\n| Post-training dynamic quantization | :heavy_check_mark: | :heavy_check_mark: | N/A                | :heavy_check_mark: |\n| Post-training static quantization  | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |\n| Quantization Aware Training (QAT)  | N/A                | :heavy_check_mark: | :heavy_check_mark: | N/A                |\n| FP16 (half precision)              | :heavy_check_mark: | N/A                | :heavy_check_mark: | :heavy_check_mark: |\n| Pruning                            | N/A                | :heavy_check_mark: | :heavy_check_mark: | N/A                |\n| Knowledge Distillation             | N/A                | :heavy_check_mark: | :heavy_check_mark: | N/A                |\n\n\n### OpenVINO\n\nBefore you begin, make sure you have all the necessary libraries installed :\n\n```bash\npip install --upgrade-strategy eager optimum[openvino,nncf]\n```\n\nIt is possible to export \ud83e\udd17 Transformers and Diffusers models to the OpenVINO format easily:\n\n```bash\noptimum-cli export openvino --model distilbert-base-uncased-fi..."], "test_settings": "chunk_7000_embeddings_openai-text-embedding-3-small_reader-model_gpt-4", "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response is mostly correct and provides the command to install Optimum with OpenVINO support. However, there is a minor discrepancy in the syntax of the command. The correct syntax should use double quotes around \"openvino\" as shown in the criteria answer: `optimum[\"openvino\"]`. The response uses square brackets without quotes, which might lead to a syntax error in some environments. Additionally, the mention of \"[\ucd9c\ucc98 \ubb38\uc11c 1]\" is unnecessary and does not contribute to the accuracy of the command itself. Overall, the response is mostly accurate but not entirely precise."}]