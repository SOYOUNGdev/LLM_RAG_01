{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fmRDhS_d0xnD"
   },
   "source": [
    "## 멀티턴을 고려한 쿼리 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lASJ4y5Pz6jj",
    "outputId": "f17df742-63f9-4a6d-ecbf-5390baab948f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\programdata\\anaconda3\\lib\\site-packages (0.3.19)\n",
      "Requirement already satisfied: langchain_openai in c:\\programdata\\anaconda3\\lib\\site-packages (0.3.7)\n",
      "Requirement already satisfied: langchain_community in c:\\programdata\\anaconda3\\lib\\site-packages (0.3.18)\n",
      "Requirement already satisfied: pypdf in c:\\programdata\\anaconda3\\lib\\site-packages (5.3.0)\n",
      "Requirement already satisfied: faiss-cpu in c:\\programdata\\anaconda3\\lib\\site-packages (1.10.0)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.35 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (0.3.40)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (0.3.6)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (0.3.11)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (2.10.6)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (2.0.25)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (3.9.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (9.0.0)\n",
      "Requirement already satisfied: numpy<2,>=1.26.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.58.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain_openai) (1.63.2)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain_openai) (0.9.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain_community) (2.8.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain_community) (0.4.0)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from faiss-cpu) (23.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.35->langchain) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.35->langchain) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (1.8.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (0.8.2)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\programdata\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (4.67.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (0.21.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2024.6.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\programdata\\anaconda3\\lib\\site-packages (from tiktoken<1,>=0.7->langchain_openai) (2023.10.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.35->langchain) (2.1)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>4->openai<2.0.0,>=1.58.1->langchain_openai) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain langchain_openai langchain_community pypdf faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n_BfkJ1h0C0Y",
    "outputId": "4aa5fd64-fe74-49a6-d063-54a262196d85"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3553: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain import PromptTemplate\n",
    "from langchain.docstore.document import Document\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from textwrap import dedent\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "xhw6JjmO0N9H"
   },
   "outputs": [],
   "source": [
    "# OpenAI API 키 설정\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "GjYY_5tV0THc"
   },
   "outputs": [],
   "source": [
    "# PDF 다운로드 및 로드\n",
    "urllib.request.urlretrieve(\"https://github.com/chatgpt-kr/openai-api-tutorial/raw/main/ch06/2023_%EB%B6%81%ED%95%9C%EC%9D%B8%EA%B6%8C%EB%B3%B4%EA%B3%A0%EC%84%9C.pdf\", filename=\"2023_북한인권보고서.pdf\")\n",
    "loader = PyPDFLoader('2023_북한인권보고서.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1iwyMjLiBgG1"
   },
   "source": [
    "이 코드는 원격 저장소에서 PDF 파일을 다운로드하고 LangChain의 PyPDFLoader를 사용하여 문서를 로드하는 과정을 구현합니다.\n",
    "\n",
    "구체적인 동작과 단계:\n",
    "\n",
    "1. PDF 파일 다운로드:\n",
    "  - urllib.request.urlretrieve 함수를 사용하여 원격 URL에서 PDF 파일을 다운로드합니다.\n",
    "  - URL: GitHub 저장소에 있는 '2023_북한인권보고서.pdf' 파일\n",
    "  - filename 매개변수를 통해 다운로드된 파일의 로컬 저장 경로를 '2023_북한인권보고서.pdf'로 지정합니다.\n",
    "  - 이 함수는 파일 다운로드를 완료한 후 로컬 파일 경로를 반환합니다.\n",
    "\n",
    "2. PDF 문서 로딩:\n",
    "  - LangChain의 PyPDFLoader 클래스를 사용하여 다운로드된 PDF 파일을 로드합니다.\n",
    "  - PyPDFLoader는 PDF 파일을 읽고 텍스트를 추출하는데 특화된 로더입니다.\n",
    "  - 이 시점에서는 파일이 메모리에 완전히 로드되지는 않으며, loader 객체만 생성됩니다.\n",
    "  - 실제 파일 내용 로딩과 텍스트 추출은 나중에 load() 또는 load_and_split() 메서드 호출 시 이루어집니다.\n",
    "\n",
    "이 코드는 RAG(Retrieval-Augmented Generation) 시스템의 첫 번째 단계로, 정보 검색의 대상이 될\n",
    "문서를 준비하는 과정입니다. 이후에는 로드된 PDF 문서를 청크(chunk)로 분할하고, 임베딩을 생성하여\n",
    "벡터 데이터베이스에 저장하는 단계가 이어집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "1SAWvkNg0UtR"
   },
   "outputs": [],
   "source": [
    "# 텍스트 분할\n",
    "# chunk_size=300이면 각 문서 당 최대 길이 300이 넘지 않도록 자르는 모듈.\n",
    "# chunk_overlap=100은 자를 때 앞의 문서랑 뒤의 문서랑 길이 100정도는 겹치게 해서 자르는 모듈.\n",
    "doc_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=100)\n",
    "docs = loader.load_and_split(doc_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'Adobe PDF Library 10.0.1', 'creator': 'Adobe InDesign CS6 (Windows)', 'creationdate': '2023-07-31T13:50:27+09:00', 'moddate': '2023-07-31T13:57:54+09:00', 'trapped': '/False', 'source': '2023_북한인권보고서.pdf', 'total_pages': 448, 'page': 0, 'page_label': '1'}, page_content='2023\\n북한인권보고서\\n2023 Report on North Korean Human Rights\\n통일부')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'본 보고서는 2017년 이후 북한의 인권실태를 진술한\\n북한이탈주민 508명의 증언을 중심으로 작성되었습니다.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[1].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023 북한인권보고서\\n04\\n올해로 유엔의 북한인권조사위원회 출범 10년, 북한인권결의 채\\n택 20년이 됩니다. 그동안 우리는 물론 국제사회가 북한인권을 증진\\n하기 위해 노력해 왔지만, 휴전선 이북의 북녘 땅은 여전히 최악의 \\n인권 사각지대로 남아 있습니다. 우리와 피를 나눈 북한 동포들이 \\n최소한의 인간적인 삶을 누릴 수 있도록 책임감을 갖고 보다 실효적\\n인 노력을 펼쳐가야만 합니다. \\n2016년 제정된 북한인권법에 기반하여 설립된 북한인권기록센\\n터는 2017년부터 북한이탈주민을 대상으로 북한의 전반적인 인권'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[3].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "288"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs[3].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'인 노력을 펼쳐가야만 합니다. \\n2016년 제정된 북한인권법에 기반하여 설립된 북한인권기록센\\n터는 2017년부터 북한이탈주민을 대상으로 북한의 전반적인 인권\\n실태를 심층적으로 조사하였습니다. 또한 파악된 북한의 인권침해 \\n사례들을 ‘세계인권선언’과 ‘국제인권조약’의 기준에 따라 분류하였\\n습니다. 이번에 발간되는 「북한인권보고서」는 북한의 인권 상황을 \\n시민적·정치적 권리, 경제적·사회적·문화적 권리 등 다양한 측면에\\n서 입체적으로 조명하였습니다. 아울러, 여성·아동·장애인 등 취약'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[4].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 설정\n",
    "embedding = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x0000026C368CA990>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x0000026C34923910>, model='text-embedding-3-large', dimensions=None, deployment='text-embedding-ada-002', openai_api_version=None, openai_api_base=None, openai_api_type=None, openai_proxy=None, embedding_ctx_length=8191, openai_api_key=SecretStr('**********'), openai_organization=None, allowed_special=None, disallowed_special=None, chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None, http_async_client=None, check_embedding_ctx_length=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAISS 벡터스토어 생성 및 저장\n",
    "faiss_store = FAISS.from_documents(docs, embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 벡터 DB를 파일로 저장\n",
    "persist_directory = \"/content/DB\"\n",
    "faiss_store.save_local(persist_directory)\n",
    "\n",
    "# 벡터 DB 저장한 파일을 로드\n",
    "vectordb = FAISS.load_local(persist_directory, embeddings=embedding, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k5YxZCUxBuFQ"
   },
   "source": [
    "이 코드는 PDF에서 추출한 텍스트를 처리하고 벡터 데이터베이스를 구축하는 RAG 시스템의 핵심 단계를 구현합니다.\n",
    "\n",
    "1. 텍스트 분할 (Chunking):\n",
    "  - RecursiveCharacterTextSplitter를 사용하여 텍스트를 작은 청크로 분할합니다.\n",
    "  - chunk_size=300: 각 청크의 최대 크기를 300자로 제한합니다.\n",
    "  - chunk_overlap=100: 인접한 청크 간에 100자의 중복을 허용하여 문맥의 연속성을 유지합니다.\n",
    "  - loader.load_and_split(): PDF를 로드하고 지정된 splitter를 사용하여 분할합니다.\n",
    "  - 이 과정에서 북한인권보고서의 내용이 여러 개의 작은 조각으로 나뉘게 됩니다.\n",
    "  - 적절한 청크 크기 설정은 RAG 시스템의 성능에 중요한 영향을 미칩니다:\n",
    "    * 너무 작으면 맥락이 손실될 수 있음\n",
    "    * 너무 크면 관련 정보를 정확히 검색하기 어려울 수 있음\n",
    "\n",
    "2. 임베딩 설정:\n",
    "  - OpenAI의 'text-embedding-3-large' 모델을 사용하여 텍스트 임베딩을 생성합니다.\n",
    "  - 이 모델은 텍스트를 고차원 벡터 공간에 표현하여 의미적으로 유사한 텍스트가\n",
    "    벡터 공간에서도 가깝게 위치하도록 합니다.\n",
    "  - 임베딩은 텍스트의 의미적 검색을 가능하게 하는 핵심 요소입니다.\n",
    "\n",
    "3. FAISS 벡터스토어 생성:\n",
    "  - FAISS(Facebook AI Similarity Search)는 효율적인 벡터 유사도 검색을 위한 라이브러리입니다.\n",
    "  - from_documents() 메서드를 통해 분할된 문서와 임베딩 모델을 사용하여 벡터스토어를 생성합니다.\n",
    "  - 이 과정에서 각 텍스트 청크는 임베딩 벡터로 변환되어 FAISS 인덱스에 저장됩니다.\n",
    "\n",
    "4. 벡터스토어 영구 저장:\n",
    "  - persist_directory를 '/content/DB'로 설정하여 벡터스토어를 로컬에 저장합니다.\n",
    "  - save_local() 메서드를 사용하여 FAISS 인덱스를 디스크에 저장합니다.\n",
    "  - 이렇게 하면 프로그램을 재시작해도 처음부터 임베딩을 다시 계산할 필요 없이\n",
    "    저장된 인덱스를 로드할 수 있습니다.\n",
    "\n",
    "5. 벡터 DB 로드:\n",
    "  - load_local() 메서드를 사용하여 저장된 FAISS 인덱스를 메모리에 로드합니다.\n",
    "  - allow_dangerous_deserialization=True: 직렬화된 Python 객체를 안전하지 않더라도\n",
    "    로드할 수 있게 허용합니다(주의: 신뢰할 수 있는 소스에서만 사용해야 함).\n",
    "  - 로드된 vectordb 객체는 이후 유사도 검색에 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "7rbKvwHw0YP_"
   },
   "outputs": [],
   "source": [
    "# 쿼리 재생성 클래스 정의\n",
    "class QueryRewriter:\n",
    "    def __init__(self, model_name=\"gpt-4o\", temperature=0):\n",
    "        self.llm = ChatOpenAI(temperature=temperature, model_name=model_name, max_tokens=1000)\n",
    "        self.template = \"\"\"\n",
    "        당신은 사용자의 이전 대화와 현재 질문을 기반으로 검색 쿼리를 재생성하는 전문가입니다.\n",
    "\n",
    "        # 지침\n",
    "        1. 이전 대화 맥락과 현재 질문을 고려하여 더 정확한 검색 쿼리를 생성하세요.\n",
    "        2. 현재 질문이 간략하거나 이전 대화의 맥락을 참조하는 경우, 이전 대화를 고려하여 완전한 쿼리를 구성하세요.\n",
    "        3. 이전 대화가 없거나 관련이 없는 경우 현재 질문만 사용하세요.\n",
    "        4. 반드시 명확하고 검색에 적합한 쿼리를 생성하세요.\n",
    "        5. 출력은 재생성된 쿼리 문자열만 포함해야 합니다. 추가 설명이나 주석은 포함하지 마세요.\n",
    "\n",
    "        # 입력\n",
    "        이전 대화 질문: {prev_query}\n",
    "        이전 대화 답변: {prev_response}\n",
    "        현재 질문: {current_query}\n",
    "\n",
    "        # 출력\n",
    "        재생성된 쿼리:\n",
    "        \"\"\"\n",
    "        self.prompt = PromptTemplate(\n",
    "            input_variables=[\"prev_query\", \"prev_response\", \"current_query\"],\n",
    "            template=self.template\n",
    "        )\n",
    "\n",
    "    def rewrite_query(self, prev_query: str, prev_response: str, current_query: str) -> str:\n",
    "        # 이전 대화가 없는 경우, LLM을 호출하지 말고 현재 검색어를 반환\n",
    "        if not prev_query or prev_query.strip() == \"\":\n",
    "            return current_query\n",
    "\n",
    "        # 쿼리 재생성. 프롬프트 템플릿과 LLM 객체를 연결\n",
    "        chain = self.prompt | self.llm\n",
    "\n",
    "        # 연결된 체인 객체를 호출\n",
    "        rewritten_query = chain.invoke({\n",
    "            \"prev_query\": prev_query,\n",
    "            \"prev_response\": prev_response,\n",
    "            \"current_query\": current_query\n",
    "        }).content.strip()\n",
    "\n",
    "        return rewritten_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iKO9LWqo_eD0"
   },
   "source": [
    "QueryRewriter 클래스는 대화형 검색 시스템에서 대화 맥락을 고려한 쿼리 재생성을 담당합니다. 이 클래스의 핵심 목적은 사용자의 모호하거나 짧은 후속 질문을 이전 대화 맥락을 활용해 더 구체적이고 명확한 검색 쿼리로 변환하는 것입니다.\n",
    "클래스는 초기화 시 ChatOpenAI 모델을 temperature 0으로 설정하여 결정론적인 응답을 보장합니다. 프롬프트 템플릿은 LLM에게 5가지 주요 지침을 제공합니다: 맥락 고려, 불완전한 질문 완성, 관련 없는 맥락 무시, 검색에 적합한 명확한 쿼리 생성, 그리고 추가 설명 없이 쿼리만 출력하는 것입니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NG6TJ5FP_lmV"
   },
   "source": [
    "rewrite_query 메서드는 세 가지 매개변수를 처리합니다:\n",
    "\n",
    "- prev_query: 이전 대화에서 사용자가 한 질문\n",
    "- prev_response: 이전 질문에 대한 시스템의 응답  \n",
    "- current_query: 현재 사용자의 질문\n",
    "\n",
    "이 메서드는 두 가지 주요 경우를 처리합니다:\n",
    "첫째, 이전 대화가 없는 경우(첫 번째 질문인 경우), 예를 들어 사용자가 \"서울의 관광지 추천해줘\"라고 처음 물었을 때는 현재 질문을 그대로 반환합니다. 이는 맥락이 없으므로 쿼리 재생성이 필요 없기 때문입니다.\n",
    "둘째, 이전 대화가 있는 경우, LLM을 사용하여 현재 질문을 이전 맥락과 결합합니다. 예를 들어:\n",
    "\n",
    "- 이전 질문: \"한국의 관광지 추천해줘\"\n",
    "- 이전 응답: \"한국의 유명한 관광지로는 서울의 경복궁, 부산의 해운대 등이 있습니다...\"\n",
    "- 현재 질문: \"부산은?\"\n",
    "- 재생성된 쿼리: \"부산의 관광지 추천해줘\"\n",
    "\n",
    "또 다른 예시로:\n",
    "\n",
    "- 이전 질문: \"어르신들을 위한 서울 여행지 추천해줘\"\n",
    "- 이전 응답: \"어르신들에게 적합한 서울 여행지로는...\"\n",
    "- 현재 질문: \"전주는?\"\n",
    "- 재생성된 쿼리: \"어르신들을 위한 전주 여행지 추천해줘\"\n",
    "\n",
    "이렇게 재생성된 쿼리는 벡터 검색 엔진에 전달되어, 단순히 \"부산은?\" 또는 \"전주는?\"이라는 모호한 질문으로 검색했을 때보다 훨씬 관련성 높은 문서를 검색할 수 있게 합니다.\n",
    "프롬프트의 마지막 부분은 LLM이 쿼리 문자열만 반환하도록 지시하여, 파싱이 쉽고 검색 엔진에 바로 사용할 수 있는 형태로 출력을 제한합니다. 이는 추가 후처리 없이 결과를 직접 활용할 수 있게 해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mamuaks_0a7Q",
    "outputId": "4a425214-ba5f-42e8-a204-eb14d2c9ce39"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7572\\3942955254.py:2: DeprecationWarning: Retrievers must implement abstract `_get_relevant_documents` method instead of `get_relevant_documents`\n",
      "  class ConversationalRetriever(BaseRetriever):\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7572\\3942955254.py:2: DeprecationWarning: Retrievers must implement abstract `_aget_relevant_documents` method instead of `aget_relevant_documents`\n",
      "  class ConversationalRetriever(BaseRetriever):\n"
     ]
    }
   ],
   "source": [
    "# 커스텀 검색기 정의\n",
    "class ConversationalRetriever(BaseRetriever):\n",
    "    vectorstore: Any = None\n",
    "    query_rewriter: Any = None\n",
    "    prev_query: str = \"\"\n",
    "    prev_response: str = \"\"\n",
    "\n",
    "    def __init__(self, vectorstore, query_rewriter, **kwargs):\n",
    "        super().__init__()\n",
    "        self.vectorstore = vectorstore\n",
    "        self.query_rewriter = query_rewriter\n",
    "        self.prev_query = kwargs.get(\"prev_query\", \"\")\n",
    "        self.prev_response = kwargs.get(\"prev_response\", \"\")\n",
    "\n",
    "    def update_conversation(self, query: str, response: str):\n",
    "        self.prev_query = query\n",
    "        self.prev_response = response\n",
    "\n",
    "    def get_relevant_documents(self, query: str, num_docs=4) -> List[Document]:\n",
    "        # 쿼리 재생성\n",
    "        # 이전 질문, 이전 답변, 현재 질문을 바탕으로 새 쿼리를 만드는 작업\n",
    "        rewritten_query = self.query_rewriter.rewrite_query(\n",
    "            self.prev_query,\n",
    "            self.prev_response,\n",
    "            query\n",
    "        )\n",
    "\n",
    "        print(f\"원래 쿼리: {query}\")\n",
    "        print(f\"재생성된 쿼리: {rewritten_query}\")\n",
    "\n",
    "        # 벡터 검색 수행\n",
    "        docs = self.vectorstore.similarity_search(rewritten_query, k=num_docs)\n",
    "        return docs\n",
    "\n",
    "    async def aget_relevant_documents(self, query: str) -> List[Document]:\n",
    "        raise NotImplementedError(\"Async retrieval not implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DvSU6snY_yj2"
   },
   "source": [
    "ConversationalRetriever 클래스는 LangChain의 BaseRetriever를 상속받아 대화 맥락을 고려한 문서 검색기를 구현합니다. 이 클래스는 이전 대화를 추적하고, QueryRewriter를 활용하여 쿼리를 재생성한 후 벡터 검색을 수행합니다.\n",
    "클래스 속성으로는 벡터 저장소(vectorstore), 쿼리 재생성기(query_rewriter), 이전 질문(prev_query), 이전 응답(prev_response)이 있습니다. 이 속성들은 대화 맥락을 유지하는 데 필요한 정보를 저장합니다.\n",
    "초기화 메서드(init)는 벡터 저장소와 쿼리 재생성기를 필수 인자로 받고, 선택적으로 이전 대화 정보를 받을 수 있습니다. 이전 대화 정보가 제공되지 않으면 빈 문자열로 초기화됩니다.\n",
    "- update_conversation 메서드는 가장 최근의 질문과 응답을 저장하여 대화 맥락을 업데이트합니다. 이 메서드는 대화가 진행될 때마다 호출되어 최신 맥락을 유지합니다.\n",
    "- get_relevant_documents 메서드는 BaseRetriever의 핵심 메서드를 구현한 것으로, 사용자 쿼리에 대해 관련 문서를 검색합니다. 이 메서드는 다음과 같은 과정으로 작동합니다:\n",
    "\n",
    "query_rewriter를 사용하여 이전 대화 맥락(prev_query, prev_response)과 현재 쿼리를 고려한 새로운 쿼리를 생성합니다.\n",
    "원래 쿼리와 재생성된 쿼리를 콘솔에 출력하여 디버깅을 용이하게 합니다.\n",
    "벡터 저장소의 similarity_search 메서드를 호출하여 재생성된 쿼리에 가장 관련성 높은 문서를 검색합니다.\n",
    "검색된 문서 목록을 반환합니다.\n",
    "\n",
    "예를 들어, 사용자가 이전에 \"한국의 전통음식 추천해줘\"라고 물은 후 \"서울에서는 뭐가 유명해?\"라고 질문했다면:\n",
    "\n",
    "- 원래 쿼리: \"서울에서는 뭐가 유명해?\"\n",
    "- 재생성된 쿼리: \"서울에서 유명한 한국 전통음식 추천해줘\"\n",
    "- 이 재생성된 쿼리로 벡터 DB를 검색하여 더 정확한 결과를 얻습니다.\n",
    "\n",
    "클래스는 또한 비동기 검색을 위한 aget_relevant_documents 메서드의 인터페이스를 제공하지만, 현재 구현되지 않았고 NotImplementedError를 발생시킵니다. 필요한 경우 추후에 비동기 검색 기능을 추가할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "KYG1c0FJ0dIC"
   },
   "outputs": [],
   "source": [
    "# 메인 RAG 클래스 정의\n",
    "class ConversationalRAG:\n",
    "    def __init__(self, vectorstore, model_name=\"gpt-4o\", temperature=0.2):\n",
    "        # 쿼리 재생성기 선언\n",
    "        self.query_rewriter = QueryRewriter(model_name=model_name)\n",
    "\n",
    "        # 쿼리 재생성을 포함하여 검색해주는 검색기\n",
    "        self.retriever = ConversationalRetriever(vectorstore=vectorstore, query_rewriter=self.query_rewriter)\n",
    "\n",
    "        # LLM 선언\n",
    "        self.llm = ChatOpenAI(temperature=temperature, model_name=model_name)\n",
    "\n",
    "        # 이 모든걸 연결해주는 Retrieval\n",
    "        self.qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=self.llm,\n",
    "            chain_type=\"stuff\", # RAG 프롬프트 디폴트값\n",
    "            retriever=self.retriever,\n",
    "            return_source_documents=True\n",
    "        )\n",
    "\n",
    "    def query(self, current_query: str) -> Dict:\n",
    "        # RAG 검색 및 응답 생성\n",
    "        result = self.qa_chain.invoke(current_query)\n",
    "\n",
    "        # 대화 기록 업데이트\n",
    "        self.retriever.update_conversation(current_query, result[\"result\"])\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cy1ze8OOAFMB"
   },
   "source": [
    "ConversationalRAG 클래스는 대화형 검색 기반 질의응답(RAG) 시스템의 핵심 구성 요소로, 모든 컴포넌트를 통합하고 사용자 질의에 대한 응답을 생성합니다.\n",
    "초기화 메서드(init)에서는 시스템의 주요 구성 요소들을 설정합니다:\n",
    "\n",
    "- QueryRewriter 인스턴스를 생성하여 대화 맥락을 고려한 쿼리 재생성을 담당하게 합니다.\n",
    "- ConversationalRetriever 인스턴스를 생성하고, 이전에 만든 query_rewriter와 함께 vectorstore(문서 벡터 데이터베이스)를 연결합니다.\n",
    "- ChatOpenAI 모델을 지정된 temperature로 초기화하여 최종 응답 생성에 사용합니다.\n",
    "- LangChain의 RetrievalQA 체인을 구성하여 검색과 응답 생성을 연결합니다. \"stuff\" 체인 타입은 모든 검색된 문서를 하나의 컨텍스트로 결합하는 가장 단순한 방식입니다.\n",
    "\n",
    "query 메서드는 사용자의 현재 질문을 받아 전체 RAG 과정을 실행합니다:\n",
    "\n",
    "1. qa_chain을 호출하여 현재 질문을 처리합니다. 이 과정에서 내부적으로 다음이 수행됩니다:\n",
    "\n",
    "  - retriever가 쿼리를 재생성하고 관련 문서를 검색\n",
    "  - 검색된 문서와 질문을 LLM에 전달하여 응답 생성\n",
    "\n",
    "\n",
    "2. 생성된 응답과 현재 질문을 retriever의 대화 기록에 업데이트합니다. 이는 다음 질문에서 대화 맥락을 유지하기 위한 중요한 단계입니다.\n",
    "3. 최종 결과를 반환합니다. 이 결과에는 생성된 응답(result[\"result\"])과 검색에 사용된 소스 문서(result[\"source_documents\"])가 포함됩니다.\n",
    "\n",
    "예를 들어, 사용자가 \"19년 말 평양시 소재 기업소에서 달마다 배급받은 음식\"이라고 질문한 후 \"그럼 일반 주민들은?\"이라고 물었을 때:\n",
    "\n",
    "1. 첫 번째 질문은 그대로 처리되어 관련 정보를 반환\n",
    "2. 두 번째 질문은 이전 맥락을 고려하여 \"19년 말 평양시 소재 일반 주민들이 달마다 배급받은 음식은 무엇인가?\"와 같은 형태로 재구성\n",
    "3. 재구성된 쿼리로 검색을 수행하고 응답 생성\n",
    "4. 새로운 질문과 응답을 대화 기록에 저장\n",
    "\n",
    "이 클래스는 RAG 파이프라인의 진입점 역할을 하며, 사용자는 단순히 query 메서드를 호출하는 것만으로 복잡한 대화형 검색 및 응답 생성 기능을 손쉽게 활용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N0HUTvx30fZZ",
    "outputId": "2b59f27d-791f-411e-976e-6cb7dbcd9a58"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7572\\1227733256.py:4: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  self.llm = ChatOpenAI(temperature=temperature, model_name=model_name, max_tokens=1000)\n"
     ]
    }
   ],
   "source": [
    "# 사용 예시\n",
    "conversational_rag = ConversationalRAG(vectordb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nlJs8Doi0iry",
    "outputId": "5cd063bd-e61a-4db8-cbbd-5752e22362d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원래 쿼리: 19년 말 평양시 소재 기업소에서 달마다 배급받은 음식\n",
      "재생성된 쿼리: 19년 말 평양시 소재 기업소에서 달마다 배급받은 음식\n",
      "\n",
      "질문: 19년 말 평양시 소재 기업소에서 달마다 배급받은 음식\n",
      "답변: 2019년 평양시에서 기업소 운전원으로 일하였던 노동자는 매월 쌀, 설탕, 기름, 야채, 돼지고기 등을 배급받았다고 합니다. 또한, 중앙당 산하의 기업소에서는 매월 쌀 6㎏, 기름 5ℓ, 설탕 2㎏, 맛내기 2봉지, 돼지고기 2㎏, 닭고기 1마리 정도를 받았다는 증언이 있습니다.\n"
     ]
    }
   ],
   "source": [
    "# 예시 1: 첫 번째 질문 (이전 대화 없음)\n",
    "query1 = \"19년 말 평양시 소재 기업소에서 달마다 배급받은 음식\"\n",
    "result1 = conversational_rag.query(query1)\n",
    "print(f\"\\n질문: {query1}\")\n",
    "print(f\"답변: {result1['result']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "31RwujSj0nio",
    "outputId": "ac29d71b-162b-45f6-c834-2e827180eb87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원래 쿼리: 그럼 일반 주민들은?\n",
      "재생성된 쿼리: 2019년 말 평양시 일반 주민들이 매월 배급받은 음식 종류와 양\n",
      "\n",
      "질문: 그럼 일반 주민들은?\n",
      "답변: 제공된 정보에 따르면, 일반 주민들은 식량 배급을 받는 데 있어서 기관이나 기업소에 따라 상당한 차이가 있는 것으로 보입니다. 일부 지역에서는 배급이 비교적 원활하게 이루어졌지만, 다른 지역에서는 규정에 미치지 못하는 적은 양의 식량을 받는 경우도 많았습니다. 예를 들어, 평양시에서는 기업소에 따라 매달 3~5일분 정도의 옥수수를 배급받았다는 진술이 있으며, 일부 지역에서는 1년에 한 번 배급이 이루어졌다는 사례도 있습니다. 따라서 일반 주민들은 식량 배급에 있어 불규칙성과 부족함을 경험하고 있는 것으로 보입니다.\n"
     ]
    }
   ],
   "source": [
    "# 예시 2: 후속 질문\n",
    "query2 = \"그럼 일반 주민들은?\"\n",
    "result2 = conversational_rag.query(query2)\n",
    "print(f\"\\n질문: {query2}\")\n",
    "print(f\"답변: {result2['result']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원래 쿼리: 그럼 고위 간부들은?\n",
      "재생성된 쿼리: 고위 간부들의 식량 배급 상황 및 일반 주민들과의 차이점\n",
      "\n",
      "질문: 그럼 고위 간부들은?\n",
      "답변: 죄송하지만, 제공된 정보에서는 북한의 고위 간부들에 대한 식량 배급이나 생활 조건에 대한 구체적인 내용이 언급되어 있지 않습니다.\n"
     ]
    }
   ],
   "source": [
    "# 예시 3: 후속 질문\n",
    "query3 = \"그럼 고위 간부들은?\"\n",
    "result3 = conversational_rag.query(query3)\n",
    "print(f\"\\n질문: {query3}\")\n",
    "print(f\"답변: {result3['result']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2andXEMd0rVI",
    "outputId": "ff48af9b-90b2-461e-f919-8d30d53e91a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원래 쿼리: 북한의 교육 시스템은 어떻게 되나요?\n",
      "재생성된 쿼리: 북한의 교육 시스템 구조와 특징\n",
      "\n",
      "질문: 북한의 교육 시스템은 어떻게 되나요?\n",
      "답변: 북한의 교육 시스템은 2012년에 개편되어 전반적 의무교육 체제를 갖추고 있습니다. 이 체제는 유치원 1년, 소학교 5년, 초급중학교 3년, 고급중학교 3년으로 구성되어 있습니다. 이전에는 초급중학교와 고급중학교를 통합하여 중학교 6년 과정을 운영하였습니다. 북한은 사회주의헌법에 따라 교육을 받을 권리를 보장하고 있으며, 교육법, 보통교육법, 고등교육법 등을 통해 이를 실현하고 있습니다. 그러나 일반교육보다 정치사상교육을 중시하며, 교과과정에 군사훈련을 포함하여 학생들이 의무적으로 참석하도록 하고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "# 예시 3: 주제 전환 질문\n",
    "query3 = \"북한의 교육 시스템은 어떻게 되나요?\"\n",
    "result3 = conversational_rag.query(query3)\n",
    "print(f\"\\n질문: {query3}\")\n",
    "print(f\"답변: {result3['result']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rBCEkBu51vNP"
   },
   "source": [
    "## 멀티턴 + 멀티쿼리까지 고려한 쿼리 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "BCqroT-I1xRr"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain import PromptTemplate\n",
    "from langchain.docstore.document import Document\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from textwrap import dedent\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain.chains import RetrievalQA\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZG23tU9v1z9N"
   },
   "outputs": [],
   "source": [
    "# OpenAI API 키 설정\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"여러분들의 Key 값\"\n",
    "\n",
    "# PDF 다운로드 및 로드\n",
    "urllib.request.urlretrieve(\"https://github.com/chatgpt-kr/openai-api-tutorial/raw/main/ch06/2023_%EB%B6%81%ED%95%9C%EC%9D%B8%EA%B6%8C%EB%B3%B4%EA%B3%A0%EC%84%9C.pdf\", filename=\"2023_북한인권보고서.pdf\")\n",
    "loader = PyPDFLoader('2023_북한인권보고서.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZXcvMZAX14pA"
   },
   "outputs": [],
   "source": [
    "# 텍스트 분할\n",
    "doc_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=100)\n",
    "docs = loader.load_and_split(doc_splitter)\n",
    "\n",
    "# 임베딩 설정\n",
    "embedding = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "# FAISS 벡터스토어 생성 및 저장\n",
    "faiss_store = FAISS.from_documents(docs, embedding)\n",
    "persist_directory = \"/content/DB\"\n",
    "faiss_store.save_local(persist_directory)\n",
    "\n",
    "# 벡터 DB 로드\n",
    "vectordb = FAISS.load_local(persist_directory, embeddings=embedding, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "hp3l7b_c1_ip"
   },
   "outputs": [],
   "source": [
    "# 멀티 쿼리 재생성 모델 정의\n",
    "class MultiQueryGenerator(BaseModel):\n",
    "    queries: List[str] = Field(description=\"사용자 질문에서 추출한 검색 쿼리 목록\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "yuhoPcXc24BP"
   },
   "outputs": [],
   "source": [
    "# 수정이 필요한 AdvancedQueryRewriter 클래스의 프롬프트 템플릿 부분만 수정\n",
    "\n",
    "class AdvancedQueryRewriter:\n",
    "    def __init__(self, model_name=\"gpt-4o\", temperature=0):\n",
    "        self.llm = ChatOpenAI(temperature=temperature, model_name=model_name, max_tokens=1000)\n",
    "        self.parser = JsonOutputParser(pydantic_object=MultiQueryGenerator)\n",
    "\n",
    "        # 프롬프트 템플릿 수정 - 명확한 형식 지침 추가\n",
    "        self.template = \"\"\"\n",
    "        당신은 사용자의 이전 대화와 현재 질문을 기반으로 검색 쿼리를 재생성하는 전문가입니다.\n",
    "\n",
    "        # 지침\n",
    "        1. 이전 대화 맥락과 현재 질문을 고려하여 더 정확한 검색 쿼리를 생성하세요.\n",
    "        2. 현재 질문이 간략하거나 이전 대화의 맥락을 참조하는 경우, 이전 대화를 고려하여 완전한 쿼리를 구성하세요.\n",
    "        3. 이전 대화가 없거나 관련이 없는 경우 현재 질문만 사용하세요.\n",
    "        4. 현재 질문에 여러 개의 질의가 포함되어 있다면, 각각을 별도의 쿼리로 분리하세요.\n",
    "        5. 각 쿼리는 독립적으로 벡터 DB에서 검색될 수 있도록 완전하고 명확해야 합니다.\n",
    "        6. 마크다운 형식으로 작성하지 마십시오.\n",
    "\n",
    "        # 입력\n",
    "        이전 대화 질문: {prev_query}\n",
    "        이전 대화 답변: {prev_response}\n",
    "        현재 질문: {current_query}\n",
    "\n",
    "        # 출력 형식\n",
    "        반드시 아래의 JSON 형식으로 출력하세요:\n",
    "        {{\n",
    "          \"queries\": [\"쿼리1\", \"쿼리2\", ...]\n",
    "        }}\n",
    "\n",
    "        예시:\n",
    "        현재 질문이 \"서울은? 그리고 맛집은?\"이고 이전 대화가 \"한국의 관광지 추천해줘\"라면,\n",
    "        {{\n",
    "          \"queries\": [\"서울의 관광지 추천해줘\", \"서울의 맛집 추천해줘\"]\n",
    "        }}\n",
    "\n",
    "        단일 쿼리인 경우:\n",
    "        {{\n",
    "          \"queries\": [\"한국의 관광지 추천해줘\"]\n",
    "        }}\n",
    "\n",
    "        {format_instructions}\n",
    "        \"\"\"\n",
    "\n",
    "        self.prompt = PromptTemplate(\n",
    "            input_variables=[\"prev_query\", \"prev_response\", \"current_query\", \"format_instructions\"],\n",
    "            template=self.template\n",
    "        )\n",
    "\n",
    "    # rewrite_query 메서드는 그대로 유지\n",
    "    def rewrite_query(self, prev_query: str, prev_response: str, current_query: str) -> List[str]:\n",
    "        # 이전 대화가 없는 경우 간단히 처리\n",
    "        if not prev_query or prev_query.strip() == \"\":\n",
    "            # 단일 질문인 경우\n",
    "            if \"?\" in current_query and current_query.count(\"?\") == 1:\n",
    "                return [current_query]\n",
    "            else:\n",
    "                # 복잡한 질문은 LLM으로 처리\n",
    "                pass\n",
    "\n",
    "        try:\n",
    "            # 프롬프트 준비\n",
    "            format_instructions = self.parser.get_format_instructions()\n",
    "\n",
    "            # 체인 실행\n",
    "            formatted_prompt = self.prompt.format(\n",
    "                prev_query=prev_query,\n",
    "                prev_response=prev_response,\n",
    "                current_query=current_query,\n",
    "                format_instructions=format_instructions\n",
    "            )\n",
    "\n",
    "            # 직접 LLM 호출\n",
    "            llm_response = self.llm.invoke(formatted_prompt)\n",
    "\n",
    "            # JSON 직접 파싱 (파서 사용 대신)\n",
    "            try:\n",
    "                import json\n",
    "                response_json = json.loads(llm_response.content)\n",
    "                if \"queries\" in response_json and isinstance(response_json[\"queries\"], list):\n",
    "                    return response_json[\"queries\"]\n",
    "                else:\n",
    "                    print(f\"응답에 'queries' 리스트가 없습니다: {response_json}\")\n",
    "                    return [current_query]\n",
    "            except json.JSONDecodeError as json_err:\n",
    "                print(f\"JSON 파싱 오류: {str(json_err)}\")\n",
    "                print(f\"LLM 원본 응답: {llm_response.content}\")\n",
    "                return [current_query]\n",
    "        except Exception as e:\n",
    "            print(f\"쿼리 재생성 중 오류 발생: {str(e)}\")\n",
    "            print(f\"LLM 원본 응답: {llm_response.content}\")\n",
    "            # 오류 발생 시 원래 쿼리 사용\n",
    "            return [current_query]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g78fvm9NAtMH"
   },
   "source": [
    "AdvancedQueryRewriter 클래스는 대화형 RAG(Retrieval-Augmented Generation) 시스템에서 멀티턴 대화 맥락과 멀티 쿼리 처리를 모두 지원하는 고급 쿼리 재생성 컴포넌트입니다.\n",
    "\n",
    "주요 기능과 작동 방식:\n",
    "\n",
    "1. 멀티턴 대화 처리:\n",
    "  - 이전 대화의 질문과 응답을 기억하여 현재 질문의 맥락을 파악합니다.\n",
    "  - 예: 이전 질문이 \"한국의 관광지 추천해줘\"이고 현재 질문이 \"서울은?\"이라면,\n",
    "    \"서울의 관광지 추천해줘\"로 확장합니다.\n",
    "\n",
    "2. 멀티 쿼리 생성:\n",
    "  - 하나의 사용자 질문에서 여러 개의 독립적인 질의를 식별하고 분리합니다.\n",
    "  - 예: \"전주는? 그리고 거기서 살 관광 상품도 있을까?\"라는 질문은\n",
    "    [\"전주의 관광지 추천해줘\", \"전주에서 살 수 있는 관광 상품 추천해줘\"]와 같은\n",
    "    두 개의 별도 쿼리로 분리됩니다.\n",
    "\n",
    "3. 구조화된 JSON 출력:\n",
    "  - 모든 생성된 쿼리를 {\"queries\": [\"쿼리1\", \"쿼리2\", ...]} 형식의 JSON으로 반환합니다.\n",
    "  - 이 형식은 후속 처리와 멀티 쿼리 검색을 용이하게 합니다.\n",
    "\n",
    "4. 강화된 오류 처리:\n",
    "  - JSON 파싱 오류, LLM 응답 오류 등 다양한 예외 상황에 대응합니다.\n",
    "  - 오류 발생 시 원래 사용자 쿼리를 단일 요소 리스트로 반환하여 시스템 안정성을 보장합니다.\n",
    "  - 디버깅을 위해 상세한 오류 메시지와 원본 LLM 응답을 출력합니다.\n",
    "\n",
    "구현 세부 사항:\n",
    "- ChatOpenAI 모델을 temperature=0으로 설정하여 일관된 결과를 보장합니다.\n",
    "- 프롬프트 템플릿은 LLM에게 정확한 지침과 출력 형식을 제공합니다.\n",
    "- 특히 '현재 질문에 여러 개의 질의가 포함되어 있다면, 각각을 별도의 쿼리로 분리하세요.'라는\n",
    " 지침은 멀티 쿼리 생성의 핵심입니다.\n",
    "- rewrite_query 메서드는 이전 버전과 달리 문자열 리스트를 반환합니다.\n",
    "\n",
    "사용 예시:\n",
    "1. 단일 질의 확장:\n",
    "  이전 질문: \"어르신들을 위한 서울 여행지 추천해줘\"\n",
    "  현재 질문: \"전주는?\"\n",
    "  결과: [\"어르신들을 위한 전주 여행지 추천해줘\"]\n",
    "\n",
    "2. 복합 질의 분리:\n",
    "  이전 질문: \"어르신들을 위한 서울 여행지 추천해줘\"\n",
    "  현재 질문: \"전주는? 그리고 거기서 살 위한 관광 상품도 있을까?\"\n",
    "  결과: [\"어르신들을 위한 전주 여행지 추천해줘\", \"전주 여행가서 살 어르신들을 위한 전주 관광 상품도 있을까?\"]\n",
    "\n",
    "이 클래스는 RAG 시스템의 검색 정확도를 크게 향상시키며, 특히 사용자가 복잡하거나 다중 질문을 할 때\n",
    "더 관련성 높은 정보를 제공할 수 있게 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "nAHVwrIn2Cqd"
   },
   "outputs": [],
   "source": [
    "# 멀티 쿼리 검색 클래스 정의\n",
    "class MultiQueryRetriever:\n",
    "    def __init__(self, vectorstore, query_rewriter, **kwargs):\n",
    "        self.vectorstore = vectorstore\n",
    "        self.query_rewriter = query_rewriter\n",
    "        self.prev_query = kwargs.get(\"prev_query\", \"\")\n",
    "        self.prev_response = kwargs.get(\"prev_response\", \"\")\n",
    "\n",
    "    def update_conversation(self, query: str, response: str):\n",
    "        self.prev_query = query\n",
    "        self.prev_response = response\n",
    "\n",
    "    def retrieve(self, query: str, num_docs=3) -> List[Document]:\n",
    "        # 쿼리 재생성 (여러 개의 쿼리로)\n",
    "        rewritten_queries = self.query_rewriter.rewrite_query(\n",
    "            self.prev_query,\n",
    "            self.prev_response,\n",
    "            query\n",
    "        )\n",
    "\n",
    "        print(f\"원래 쿼리: {query}\")\n",
    "        print(f\"재생성된 쿼리들: {rewritten_queries}\")\n",
    "\n",
    "        # 모든 쿼리에 대해 검색 수행 및 결과 병합\n",
    "        all_docs = []\n",
    "        seen_contents = set()  # 중복 제거를 위한 집합\n",
    "\n",
    "        # 다수의 쿼리가 주어졌을 때 1개씩 검색\n",
    "        for idx, rewritten_query in enumerate(rewritten_queries):\n",
    "            print(f\"쿼리 {idx+1}: {rewritten_query}\")\n",
    "\n",
    "            # 벡터 검색 수행\n",
    "            docs = self.vectorstore.similarity_search(rewritten_query, k=num_docs)\n",
    "\n",
    "            # 중복 제거하며 문서 추가\n",
    "            for doc in docs:\n",
    "                if doc.page_content not in seen_contents:\n",
    "                    seen_contents.add(doc.page_content)\n",
    "                    # 메타데이터에 쿼리 정보 추가\n",
    "                    if not hasattr(doc, 'metadata') or doc.metadata is None:\n",
    "                        doc.metadata = {}\n",
    "                    doc.metadata['query'] = rewritten_query\n",
    "                    all_docs.append(doc)\n",
    "\n",
    "        print(f\"총 {len(all_docs)}개의 고유 문서를 검색했습니다.\")\n",
    "        return all_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z3y0283ZA4g7"
   },
   "source": [
    "MultiQueryRetriever 클래스는 멀티턴 대화 맥락과 멀티 쿼리 처리를 모두 지원하는 고급 문서 검색 컴포넌트입니다. 이 클래스는 이전의 ConversationalRetriever를 확장하여 여러 개의 쿼리를 동시에 처리할 수 있는 기능을 추가했습니다.\n",
    "\n",
    "주요 특징과 동작 방식:\n",
    "\n",
    "1. 초기화 및 상태 관리:\n",
    "  - vectorstore: 문서 검색을 위한 벡터 데이터베이스를 참조합니다.\n",
    "  - query_rewriter: 대화 맥락과 멀티 쿼리를 처리하는 AdvancedQueryRewriter 인스턴스를 사용합니다.\n",
    "  - prev_query와 prev_response: 이전 대화의 질문과 응답을 저장하여 대화 맥락을 유지합니다.\n",
    "  - BaseRetriever를 상속하지 않고 독립적인 클래스로 구현되었습니다.\n",
    "\n",
    "2. 대화 맥락 업데이트:\n",
    "  - update_conversation 메서드는 현재 질문과 응답을 저장하여 다음 질문에서 참조할 수 있게 합니다.\n",
    "  - 이는 '그럼 서울은?'과 같은 맥락 의존적 질문을 처리할 때 필수적입니다.\n",
    "\n",
    "3. 멀티 쿼리 검색 프로세스:\n",
    "  - retrieve 메서드는 query_rewriter를 통해 단일 사용자 질문에서 여러 검색 쿼리를 생성합니다.\n",
    "  - 각 쿼리에 대해 별도로 벡터 검색을 수행하고 결과를 병합합니다.\n",
    "  - 예: \"전주는? 거기 맛집도 알려줘\"라는 질문은 [\"전주 관광지 추천\", \"전주 맛집 추천\"]과 같은\n",
    "    여러 쿼리로 확장되어 각각 검색됩니다.\n",
    "\n",
    "4. 검색 결과 최적화:\n",
    "  - 중복 제거: seen_contents 집합을 사용하여 여러 쿼리에서 중복되는 문서를 제거합니다.\n",
    "  - 이는 검색 결과의 다양성을 보장하고 중복 정보를 제거하여 사용자 경험을 향상시킵니다.\n",
    "  - 메타데이터 강화: 각 문서에 어떤 쿼리에서 검색되었는지 정보를 메타데이터로 추가합니다.\n",
    "    이 정보는 후속 처리나 디버깅에 유용할 수 있습니다.\n",
    "\n",
    "5. 로깅 및 디버깅:\n",
    "  - 검색 과정의 각 단계를 콘솔에 출력하여 쿼리 재생성과 검색 결과를 모니터링할 수 있습니다.\n",
    "  - 원래 쿼리, 재생성된 쿼리들, 각 개별 쿼리, 최종 검색된 문서 수 등이 출력됩니다.\n",
    "\n",
    "실제 동작 예시:\n",
    "- 사용자 질문: \"북한의 식량 상황은? 그리고 인권 문제는 어떻게 되나요?\"\n",
    "- 재생성된 쿼리들: [\"북한의 식량 상황\", \"북한의 인권 문제 현황\"]\n",
    "- 각 쿼리별로 벡터 DB 검색 수행 (각각 3개씩 문서 검색)\n",
    "- 중복 제거 후 총 5-6개 정도의 고유 문서 반환 (일부 문서는 두 쿼리에서 모두 검색될 수 있음)\n",
    "\n",
    "이 클래스는 복잡한 사용자 질문에 대해 더 포괄적이고 다양한 정보를 제공할 수 있게 하며,\n",
    "대화형 RAG 시스템의 검색 기능을 크게 향상시킵니다. 특히 여러 주제나 질문을 한 번에 물어보는\n",
    "사용자 상호작용 패턴에 효과적으로 대응할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "w0-Y68fn2E8w"
   },
   "outputs": [],
   "source": [
    "# RAG 응답 생성 클래스 정의\n",
    "class AdvancedConversationalRAG:\n",
    "    def __init__(self, vectorstore, model_name=\"gpt-4o\", temperature=0.2):\n",
    "        # 쿼리 재생성기\n",
    "        self.query_rewriter = AdvancedQueryRewriter(model_name=model_name)\n",
    "        # 재생성된 쿼리를 바탕으로 각각의 검색어를 따로 검색한 뒤에 검색 결과를 취합하는 멀티 쿼리 리트리버\n",
    "        self.retriever = MultiQueryRetriever(vectorstore=vectorstore, query_rewriter=self.query_rewriter)\n",
    "        # 답변을 해줄 답변 용도의 LLM 객체\n",
    "        self.llm = ChatOpenAI(temperature=temperature, model_name=model_name)\n",
    "\n",
    "        # 응답 생성을 위한 프롬프트 템플릿\n",
    "        self.response_template = \"\"\"\n",
    "        당신은 사용자 질문에 대한 정보를 제공하는 도우미입니다.\n",
    "\n",
    "        사용자 질문: {query}\n",
    "\n",
    "        다음 정보를 참고하여 사용자 질문에 답변하세요:\n",
    "        {context}\n",
    "\n",
    "        참고 사항:\n",
    "        1. 사용자 질문에 여러 개의 질의가 포함되어 있다면, 각각에 대해 명확하게 답변하세요.\n",
    "        2. 제공된 정보만을 사용하여 답변하세요. 정보가 부족하면 솔직히 모른다고 답변하세요.\n",
    "        3. 답변은 한국어로 제공하세요.\n",
    "        4. 제공된 정보의 원본 출처가 있다면 인용해주세요.\n",
    "\n",
    "        답변:\n",
    "        \"\"\"\n",
    "\n",
    "        self.response_prompt = PromptTemplate(\n",
    "            input_variables=[\"query\", \"context\"],\n",
    "            template=self.response_template\n",
    "        )\n",
    "\n",
    "    def query(self, current_query: str) -> Dict:\n",
    "        # 관련 문서 검색\n",
    "        docs = self.retriever.retrieve(current_query)\n",
    "\n",
    "        # 문서 내용을 컨텍스트로 변환\n",
    "        context = \"\\n\\n\".join([f\"문서 {i+1}:\\n{doc.page_content}\" for i, doc in enumerate(docs)])\n",
    "\n",
    "        # 응답 생성\n",
    "        formatted_prompt = self.response_prompt.format(\n",
    "            query=current_query,\n",
    "            context=context\n",
    "        )\n",
    "\n",
    "        # 직접 LLM 호출\n",
    "        result = self.llm.invoke(formatted_prompt)\n",
    "        response = result.content\n",
    "\n",
    "        # 대화 기록 업데이트\n",
    "        self.retriever.update_conversation(current_query, response)\n",
    "\n",
    "        # 결과 반환\n",
    "        return {\n",
    "            \"query\": current_query,\n",
    "            \"result\": response,\n",
    "            \"source_documents\": docs\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KpLWdKNoBIXW"
   },
   "source": [
    "AdvancedConversationalRAG 클래스는 멀티턴 대화와 멀티 쿼리를 모두 지원하는 완전한 RAG(Retrieval-Augmented Generation) 시스템을 구현합니다. 이 클래스는 모든 구성 요소를 통합하고 사용자 질의에 대한 최종 응답을 생성하는 핵심 컴포넌트입니다.\n",
    "\n",
    "주요 특징과 작동 방식:\n",
    "\n",
    "1. 구성 요소 초기화:\n",
    "  - AdvancedQueryRewriter: 멀티턴 맥락과 멀티 쿼리를 처리하는 고급 쿼리 재생성기를 생성합니다.\n",
    "  - MultiQueryRetriever: 여러 쿼리를 동시에 처리하고 결과를 병합하는 검색기를 생성합니다.\n",
    "  - ChatOpenAI: 최종 응답 생성에 사용되는 언어 모델을 초기화합니다.\n",
    "  - 이전 ConversationalRAG와 달리 RetrievalQA 체인을 사용하지 않고, 직접 프롬프트와 LLM을 제어합니다.\n",
    "\n",
    "2. 응답 생성 프롬프트:\n",
    "  - 사용자 질문과 검색된 컨텍스트를 포함하는 상세한 프롬프트 템플릿을 정의합니다.\n",
    "  - 특히 '사용자 질문에 여러 개의 질의가 포함되어 있다면, 각각에 대해 명확하게 답변하세요'라는\n",
    "    지침은 멀티 쿼리 처리를 위해 중요합니다.\n",
    "  - RAG 시스템의 핵심 원칙인 '제공된 정보만을 사용하여 답변하세요'를 강조합니다.\n",
    "\n",
    "3. 쿼리 처리 과정:\n",
    "  - query 메서드는 사용자의 현재 질문을 입력으로 받습니다.\n",
    "  - MultiQueryRetriever를 통해 멀티턴 맥락을 고려하고 필요시 여러 쿼리로 분리하여 관련 문서를 검색합니다.\n",
    "  - 예: \"북한의 식량 사정은? 인권 문제는 어떤가?\"라는 질문은 두 개의 쿼리로 나뉘어 각각 검색됩니다.\n",
    "  - 검색된 모든 문서를 번호가 매겨진 형식으로 컨텍스트에 통합합니다.\n",
    "\n",
    "4. 응답 생성 및 대화 업데이트:\n",
    "  - 사용자 질문과 검색된 컨텍스트를 포함한 프롬프트를 LLM에 전달합니다.\n",
    "  - LLM은 모든 하위 질의에 답변하는 통합된 응답을 생성합니다.\n",
    "  - 생성된 응답과 현재 질문을 retriever의 대화 기록에 저장하여 다음 턴에서 맥락을 유지합니다.\n",
    "\n",
    "5. 결과 반환:\n",
    "  - 원본 질의, 생성된 응답, 검색에 사용된 소스 문서를 포함하는 딕셔너리를 반환합니다.\n",
    "  - 이를 통해 클라이언트는 답변뿐만 아니라 해당 답변의 근거가 된 정보도 확인할 수 있습니다.\n",
    "\n",
    "실제 동작 예시:\n",
    "- 사용자 입력: \"서울은? 그리고 맛집은?\"\n",
    "- 이전 대화: \"한국의 관광지 추천해줘\"\n",
    "- 재생성된 쿼리: [\"서울의 관광지 추천해줘\", \"서울의 맛집 추천해줘\"]\n",
    "- 각 쿼리로 문서 검색 및 결과 병합\n",
    "- 통합된 응답 생성: \"서울의 주요 관광지로는... 서울에서 꼭 가봐야 할 맛집으로는...\"\n",
    "\n",
    "이 클래스는 대화형 RAG 시스템의 가장 상위 레벨 컴포넌트로, 사용자는 단순히 query 메서드를 호출하는\n",
    "것만으로 멀티턴 대화와 복잡한 질의를 처리할 수 있습니다. 사용자 경험 측면에서 볼 때, 여러 관련 질문을\n",
    "한 번에 물어볼 수 있고, 이전 대화 맥락이 자동으로 고려되므로 더 자연스럽고 효율적인 정보 검색이 가능합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "BrpXchwn2HXq"
   },
   "outputs": [],
   "source": [
    "conversational_rag = AdvancedConversationalRAG(vectordb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z641VjEa2Kd3",
    "outputId": "b5af532a-62b3-4ff9-8ca6-6fe4b3a252be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "질문 1: 19년 말 평양시 소재 기업소에서 달마다 배급받은 음식\n",
      "원래 쿼리: 19년 말 평양시 소재 기업소에서 달마다 배급받은 음식\n",
      "재생성된 쿼리들: ['2019년 말 평양시 소재 기업소에서 매달 배급받은 음식 종류']\n",
      "쿼리 1: 2019년 말 평양시 소재 기업소에서 매달 배급받은 음식 종류\n",
      "총 3개의 고유 문서를 검색했습니다.\n",
      "답변 1:\n",
      "2019년 말 평양시 소재 기업소에서 매월 배급받은 음식에 대한 정보는 다음과 같습니다:\n",
      "\n",
      "- 쌀: 약 6kg\n",
      "- 기름: 5ℓ\n",
      "- 설탕: 2kg\n",
      "- 맛내기: 2봉지\n",
      "- 돼지고기: 2kg\n",
      "- 닭고기: 1마리\n",
      "\n",
      "이 정보는 2019년 평양시에서 기업소 운전원으로 일하였던 노동자의 증언에 기반한 것입니다. 이 노동자는 매월 이러한 식량을 배급받아 식량이 부족하지 않았다고 증언하였습니다. (출처: 문서 1)\n"
     ]
    }
   ],
   "source": [
    "# 예시 1: 첫 번째 질문 (이전 대화 없음, 멀티 쿼리도 아님)\n",
    "query1 = \"19년 말 평양시 소재 기업소에서 달마다 배급받은 음식\"\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"질문 1: {query1}\")\n",
    "result1 = conversational_rag.query(query1)\n",
    "print(f\"답변 1:\\n{result1['result']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M1UrLYYZ3FXA",
    "outputId": "2a574fb1-97ce-4662-c594-9b2338547b74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원래 쿼리: 그럼 일반 주민들은?\n",
      "재생성된 쿼리들: ['2019년 말 평양시 일반 주민들이 매월 배급받은 음식']\n",
      "쿼리 1: 2019년 말 평양시 일반 주민들이 매월 배급받은 음식\n",
      "총 3개의 고유 문서를 검색했습니다.\n",
      "\n",
      "질문: 그럼 일반 주민들은?\n",
      "답변: 일반 주민들에 대한 식량 배급 상황은 기관이나 지역에 따라 다소 차이가 있는 것으로 보입니다. 평양시의 경우, 식량 배급이 비교적 원활하게 이루어지고 있으며, 대학생들에게는 하루에 쌀 500g이 15일에 한 번씩 배급된다는 진술이 있습니다. 또한, 양강도 보천군에서는 감자 수확철에 세대 당 인원을 기준으로 감자를 배급한 사례가 있습니다. 그러나 문서에서 일반 주민들 전체에 대한 구체적인 배급 상황은 명시되어 있지 않으므로, 더 자세한 정보는 제공되지 않았습니다.\n"
     ]
    }
   ],
   "source": [
    "# 예시 2: 후속 질문 (멀티턴 o, 멀티쿼리x)\n",
    "query2 = \"그럼 일반 주민들은?\"\n",
    "result2 = conversational_rag.query(query2)\n",
    "print(f\"\\n질문: {query2}\")\n",
    "print(f\"답변: {result2['result']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j3TRXU_h99BP",
    "outputId": "4ed278ac-a3af-461f-cdc5-59341a4352db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원래 쿼리: 혹시 교육 시스템은 어떻게 되나요? 그리고 주민들이 좋아하는 음식이 있나요?\n",
      "재생성된 쿼리들: ['북한의 교육 시스템', '북한 주민들이 좋아하는 음식']\n",
      "쿼리 1: 북한의 교육 시스템\n",
      "쿼리 2: 북한 주민들이 좋아하는 음식\n",
      "총 6개의 고유 문서를 검색했습니다.\n",
      "\n",
      "질문: 혹시 교육 시스템은 어떻게 되나요? 그리고 주민들이 좋아하는 음식이 있나요?\n",
      "답변: 북한의 교육 시스템에 대해 말씀드리겠습니다. 북한은 2012년에 전반적 의무교육 체제를 개편하여 유치원 1년, 소학교 5년, 초급중학교 3년, 고급중학교 3년으로 구성된 학제를 운영하고 있습니다. 이는 이전의 중학교 6년 과정에서 변경된 것입니다. 또한, 북한은 사회주의헌법에 따라 모든 공민에게 교육을 받을 권리를 보장하고 있으며, 이를 위해 다양한 교육 관련 법률을 제정하고 있습니다. (출처: 문서 1, 문서 3)\n",
      "\n",
      "북한 주민들이 좋아하는 음식에 대한 정보는 제공된 문서에서 명확히 언급되지 않았습니다. 다만, 명절에는 떡, 돼지고기, 수산물 등이 배급된 사례가 있으며, 군부대에서는 쌀밥이 급식되었다는 사례가 있습니다. (출처: 문서 4) 이러한 정보로 볼 때, 떡이나 돼지고기, 수산물 등이 북한 주민들이 선호하는 음식일 가능성이 있습니다. 그러나 주민들이 특별히 좋아하는 음식에 대한 구체적인 정보는 제공된 문서에 포함되어 있지 않습니다.\n"
     ]
    }
   ],
   "source": [
    "# 예시 3: 주제 전환 질문 (멀티턴o, 멀티쿼리o)\n",
    "query3 = \"혹시 교육 시스템은 어떻게 되나요? 그리고 주민들이 좋아하는 음식이 있나요?\"\n",
    "result3 = conversational_rag.query(query3)\n",
    "print(f\"\\n질문: {query3}\")\n",
    "print(f\"답변: {result3['result']}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
